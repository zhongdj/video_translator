"""
è¯­éŸ³å…‹éš†ç”¨ä¾‹ - æŒä¹…åŒ–ç¼“å­˜ç‰ˆ
ç¡®ä¿GPUç”Ÿæˆçš„éŸ³é¢‘æ•°æ®èƒ½å¤ŸæŒä¹…åŒ–ä¿å­˜ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

from pathlib import Path
from typing import Optional
import pickle
import hashlib
import array

from domain.entities import (
    Video, Subtitle, AudioTrack, VoiceProfile, AudioSample, TextSegment, LanguageCode
)
from domain.ports import TTSProvider, VideoProcessor, CacheRepository
from domain.services import calculate_cache_key


# ============== éŸ³é¢‘æ–‡ä»¶ç¼“å­˜ç®¡ç† ============== #

class AudioFileCache:
    """éŸ³é¢‘æ–‡ä»¶ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: Path = Path(".cache/audio")):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def get_audio_file_path(self, cache_key: str) -> Path:
        """è·å–éŸ³é¢‘æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.wav"

    def get_metadata_file_path(self, cache_key: str) -> Path:
        """è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.meta"

    def save_audio_data(self, cache_key: str, audio_sample: AudioSample, metadata: dict) -> bool:
        """ä¿å­˜éŸ³é¢‘æ•°æ®å’Œå…ƒæ•°æ®"""
        try:
            # ä¿å­˜éŸ³é¢‘æ•°æ®ä¸ºWAVæ–‡ä»¶
            audio_path = self.get_audio_file_path(cache_key)
            self._save_as_wav(audio_path, audio_sample)

            # ä¿å­˜å…ƒæ•°æ®
            meta_path = self.get_metadata_file_path(cache_key)
            with open(meta_path, 'wb') as f:
                pickle.dump(metadata, f)

            print(f"âœ… éŸ³é¢‘æ•°æ®å·²ä¿å­˜: {audio_path}")
            return True

        except Exception as e:
            print(f"âŒ ä¿å­˜éŸ³é¢‘æ•°æ®å¤±è´¥: {e}")
            return False

    def load_audio_data(self, cache_key: str) -> tuple[Optional[AudioSample], Optional[dict]]:
        """åŠ è½½éŸ³é¢‘æ•°æ®å’Œå…ƒæ•°æ®"""
        try:
            audio_path = self.get_audio_file_path(cache_key)
            meta_path = self.get_metadata_file_path(cache_key)

            if not audio_path.exists() or not meta_path.exists():
                return None, None

            # åŠ è½½å…ƒæ•°æ®
            with open(meta_path, 'rb') as f:
                metadata = pickle.load(f)

            # åŠ è½½éŸ³é¢‘æ•°æ®
            audio_sample = self._load_from_wav(audio_path)

            print(f"âœ… éŸ³é¢‘æ•°æ®å·²åŠ è½½: {audio_path}")
            return audio_sample, metadata

        except Exception as e:
            print(f"âŒ åŠ è½½éŸ³é¢‘æ•°æ®å¤±è´¥: {e}")
            return None, None

    def _save_as_wav(self, file_path: Path, audio_sample: AudioSample):
        """å°†éŸ³é¢‘æ•°æ®ä¿å­˜ä¸ºWAVæ–‡ä»¶"""
        import wave
        import struct

        # å°†æµ®ç‚¹æ•°è½¬æ¢ä¸º16ä½PCM
        samples_int16 = [int(sample * 32767) for sample in audio_sample.samples]

        with wave.open(str(file_path), 'w') as wav_file:
            wav_file.setnchannels(1)  # å•å£°é“
            wav_file.setsampwidth(2)  # 16ä½
            wav_file.setframerate(audio_sample.sample_rate)

            # å†™å…¥æ•°æ®
            for sample in samples_int16:
                wav_file.writeframes(struct.pack('<h', sample))

    def _load_from_wav(self, file_path: Path) -> AudioSample:
        """ä»WAVæ–‡ä»¶åŠ è½½éŸ³é¢‘æ•°æ®"""
        import wave
        import struct

        with wave.open(str(file_path), 'r') as wav_file:
            # è·å–å‚æ•°
            sample_rate = wav_file.getframerate()
            n_frames = wav_file.getnframes()

            # è¯»å–æ•°æ®
            frames = wav_file.readframes(n_frames)

            # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            samples = []
            for i in range(0, len(frames), 2):
                sample = struct.unpack('<h', frames[i:i+2])[0]
                samples.append(sample / 32767.0)  # è½¬æ¢ä¸º[-1, 1]èŒƒå›´çš„æµ®ç‚¹æ•°

            return AudioSample(
                samples=tuple(samples),
                sample_rate=sample_rate
            )


# ============== åˆ†æ®µåˆæˆçŠ¶æ€ç®¡ç† ============== #

class SegmentSynthesisState:
    """åˆ†æ®µåˆæˆçŠ¶æ€ç®¡ç†å™¨"""

    def __init__(self, cache_dir: Path = Path(".cache/segments")):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def get_segment_file_path(self, cache_key: str, segment_index: int) -> Path:
        """è·å–åˆ†æ®µéŸ³é¢‘æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}_seg_{segment_index:04d}.wav"

    def save_segment(self, cache_key: str, segment_index: int, audio_sample: AudioSample) -> bool:
        """ä¿å­˜åˆ†æ®µéŸ³é¢‘"""
        try:
            file_path = self.get_segment_file_path(cache_key, segment_index)

            import wave
            import struct

            # å°†æµ®ç‚¹æ•°è½¬æ¢ä¸º16ä½PCM
            samples_int16 = [int(sample * 32767) for sample in audio_sample.samples]

            with wave.open(str(file_path), 'w') as wav_file:
                wav_file.setnchannels(1)
                wav_file.setsampwidth(2)
                wav_file.setframerate(audio_sample.sample_rate)

                for sample in samples_int16:
                    wav_file.writeframes(struct.pack('<h', sample))

            return True

        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†æ®µ {segment_index} å¤±è´¥: {e}")
            return False

    def load_segment(self, cache_key: str, segment_index: int) -> Optional[AudioSample]:
        """åŠ è½½åˆ†æ®µéŸ³é¢‘"""
        try:
            file_path = self.get_segment_file_path(cache_key, segment_index)

            if not file_path.exists():
                return None

            import wave
            import struct

            with wave.open(str(file_path), 'r') as wav_file:
                sample_rate = wav_file.getframerate()
                n_frames = wav_file.getnframes()
                frames = wav_file.readframes(n_frames)

                samples = []
                for i in range(0, len(frames), 2):
                    sample = struct.unpack('<h', frames[i:i+2])[0]
                    samples.append(sample / 32767.0)

                return AudioSample(
                    samples=tuple(samples),
                    sample_rate=sample_rate
                )

        except Exception as e:
            print(f"âŒ åŠ è½½åˆ†æ®µ {segment_index} å¤±è´¥: {e}")
            return None

    def get_completed_segments(self, cache_key: str, total_segments: int) -> list[int]:
        """è·å–å·²å®Œæˆçš„æ®µè½ä¸‹æ ‡åˆ—è¡¨"""
        completed = []
        for i in range(total_segments):
            file_path = self.get_segment_file_path(cache_key, i)
            if file_path.exists():
                completed.append(i)
        return completed


# ============== ç¼“å­˜è¾…åŠ©å‡½æ•° ============== #

def _load_voice_cloning_from_cache(
        audio_cache: AudioFileCache,
        cache_key: str
) -> tuple[AudioTrack, VoiceProfile]:
    """ä»æ–‡ä»¶ç¼“å­˜åŠ è½½è¯­éŸ³å…‹éš†ç»“æœ"""
    audio_sample, metadata = audio_cache.load_audio_data(cache_key)

    if audio_sample is None or metadata is None:
        raise ValueError("ç¼“å­˜æ•°æ®åŠ è½½å¤±è´¥")

    voice_profile = VoiceProfile(
        reference_audio_path=Path(metadata["reference_audio"]),
        language=LanguageCode(metadata["language"]),
        duration=metadata["reference_duration"]
    )

    return AudioTrack(audio_sample, voice_profile.language), voice_profile


def _save_voice_cloning_to_cache(
        audio_cache: AudioFileCache,
        cache_key: str,
        audio: AudioSample,
        voice_profile: VoiceProfile
):
    """ä¿å­˜è¯­éŸ³å…‹éš†ç»“æœåˆ°æ–‡ä»¶ç¼“å­˜"""
    metadata = {
        "reference_audio": str(voice_profile.reference_audio_path),
        "language": voice_profile.language.value,
        "reference_duration": voice_profile.duration,
        "sample_rate": audio.sample_rate,
        "num_samples": len(audio.samples)
    }

    audio_cache.save_audio_data(cache_key, audio, metadata)


# ============== éŸ³é¢‘å¤„ç†è¾…åŠ©å‡½æ•° ============== #

def _prepare_reference_audio(
        video: Video,
        video_processor: VideoProcessor,
        reference_audio_path: Optional[Path],
        reference_duration: float
) -> Path:
    """å‡†å¤‡å‚è€ƒéŸ³é¢‘"""
    if reference_audio_path is not None:
        return reference_audio_path

    return video_processor.extract_reference_audio(video, reference_duration)


def _synthesize_single_segment(
        segment: TextSegment,
        tts_provider: TTSProvider,
        voice_profile: VoiceProfile
) -> tuple[AudioSample, TextSegment]:
    """åˆæˆå•ä¸ªéŸ³é¢‘ç‰‡æ®µ"""
    audio_sample = tts_provider.synthesize(
        text=segment.text,
        voice_profile=voice_profile,
        target_duration=segment.time_range.duration
    )
    return audio_sample, segment


def _synthesize_segments_with_checkpoint(
        segments: tuple[TextSegment, ...],
        tts_provider: TTSProvider,
        voice_profile: VoiceProfile,
        segment_state: SegmentSynthesisState,
        cache_key: str,
        progress_callback: Optional[callable]
) -> list[tuple[AudioSample, TextSegment]]:
    """å¸¦æ£€æŸ¥ç‚¹çš„åˆ†æ®µåˆæˆ - æ”¯æŒæ–­ç‚¹ç»­ä¼ """
    synthesized_segments = []
    total_segments = len(segments)

    # æ£€æŸ¥å·²å®Œæˆçš„ç‰‡æ®µ
    completed_indices = segment_state.get_completed_segments(cache_key, total_segments)
    print(f"ğŸ“Š æ–­ç‚¹ç»­ä¼ : å·²å®Œæˆ {len(completed_indices)}/{total_segments} ä¸ªç‰‡æ®µ")

    for idx, segment in enumerate(segments):
        # å¦‚æœè¿™ä¸ªç‰‡æ®µå·²ç»å®Œæˆï¼Œç›´æ¥åŠ è½½
        if idx in completed_indices:
            if progress_callback:
                progress = 0.2 + (idx / total_segments) * 0.7
                progress_callback(progress, f"åŠ è½½å·²åˆæˆç‰‡æ®µ {idx + 1}/{total_segments}")

            audio_sample = segment_state.load_segment(cache_key, idx)
            if audio_sample:
                synthesized_segments.append((audio_sample, segment))
                continue

        # åˆæˆæ–°ç‰‡æ®µ
        if progress_callback:
            progress = 0.2 + (idx / total_segments) * 0.7
            progress_callback(progress, f"åˆæˆè¯­éŸ³ {idx + 1}/{total_segments}")

        audio_sample, segment = _synthesize_single_segment(segment, tts_provider, voice_profile)

        # ç«‹å³ä¿å­˜ç‰‡æ®µåˆ°ç£ç›˜
        if segment_state.save_segment(cache_key, idx, audio_sample):
            print(f"âœ… ç‰‡æ®µ {idx} å·²ä¿å­˜åˆ°ç£ç›˜")

        synthesized_segments.append((audio_sample, segment))

    return synthesized_segments


def _create_empty_audio_buffer(video_duration: float, sample_rate: int) -> list[float]:
    """åˆ›å»ºç©ºéŸ³é¢‘ç¼“å†²åŒº"""
    total_samples = int(video_duration * sample_rate)
    return [0.0] * total_samples


def _fill_audio_buffer(
        buffer: list[float],
        synthesized_segments: list[tuple[AudioSample, TextSegment]]
):
    """å°†åˆæˆçš„éŸ³é¢‘ç‰‡æ®µå¡«å……åˆ°ç¼“å†²åŒº"""
    for audio_sample, segment in synthesized_segments:
        start_idx = int(segment.time_range.start_seconds * audio_sample.sample_rate)
        for i, sample in enumerate(audio_sample.samples):
            if start_idx + i < len(buffer):
                buffer[start_idx + i] = sample


def _merge_audio_segments(
        synthesized_segments: list[tuple[AudioSample, TextSegment]],
        video_duration: float
) -> AudioSample:
    """æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ"""
    if not synthesized_segments:
        raise ValueError("æ²¡æœ‰å¯åˆæˆçš„éŸ³é¢‘ç‰‡æ®µ")

    sample_rate = synthesized_segments[0][0].sample_rate
    buffer = _create_empty_audio_buffer(video_duration, sample_rate)
    _fill_audio_buffer(buffer, synthesized_segments)

    return AudioSample(
        samples=tuple(buffer),
        sample_rate=sample_rate
    )


# ============== ä¸»ç”¨ä¾‹å‡½æ•° ============== #

class VoiceCloningResult:
    """è¯­éŸ³å…‹éš†ç»“æœ"""
    def __init__(self, audio_track: AudioTrack, voice_profile: VoiceProfile,
                 total_segments: int, cache_hit: bool):
        self.audio_track = audio_track
        self.voice_profile = voice_profile
        self.total_segments = total_segments
        self.cache_hit = cache_hit


def clone_voice_use_case(
        video: Video,
        subtitle: Subtitle,
        tts_provider: TTSProvider,
        video_processor: VideoProcessor,
        cache_repo: CacheRepository,
        reference_audio_path: Optional[Path] = None,
        reference_duration: float = 10.0,
        progress: Optional[callable] = None
) -> VoiceCloningResult:
    """
    è¯­éŸ³å…‹éš†ç”¨ä¾‹ï¼ˆæŒä¹…åŒ–ç¼“å­˜ç‰ˆï¼‰
    æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œç¡®ä¿GPUè€—æ—¶ä»»åŠ¡çš„ç»“æœèƒ½å¤ŸæŒä¹…åŒ–ä¿å­˜
    """
    if progress:
        progress(0.0, "å¼€å§‹è¯­éŸ³å…‹éš†")

    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    audio_cache = AudioFileCache()
    segment_state = SegmentSynthesisState()

    # 1. è®¡ç®—ç¼“å­˜é”®
    cache_key = calculate_cache_key(
        video.path,
        "clone_voice",
        {
            "target_language": subtitle.language.value,
            "source_language": "auto",
            "reference_audio_hash": str(reference_audio_path) if reference_audio_path else "default"
        }
    )

    # 2. æ£€æŸ¥å®Œæ•´éŸ³é¢‘ç¼“å­˜
    try:
        audio_sample, metadata = audio_cache.load_audio_data(cache_key)
        if audio_sample is not None:
            voice_profile = VoiceProfile(
                reference_audio_path=Path(metadata["reference_audio"]),
                language=LanguageCode(metadata["language"]),
                duration=metadata["reference_duration"]
            )

            if progress:
                progress(1.0, "è¯­éŸ³å…‹éš†å®Œæ•´ç¼“å­˜å‘½ä¸­")

            print("âœ… ä½¿ç”¨å®Œæ•´éŸ³é¢‘ç¼“å­˜")
            return VoiceCloningResult(
                audio_track=AudioTrack(audio_sample, subtitle.language),
                voice_profile=voice_profile,
                total_segments=len(subtitle.segments),
                cache_hit=True
            )
    except Exception as e:
        print(f"âš ï¸ å®Œæ•´ç¼“å­˜åŠ è½½å¤±è´¥: {e}")

    # 3. å‡†å¤‡å‚è€ƒéŸ³é¢‘
    if progress:
        progress(0.1, "å‡†å¤‡å‚è€ƒéŸ³é¢‘")

    reference_audio = _prepare_reference_audio(
        video, video_processor, reference_audio_path, reference_duration
    )

    # 4. åˆ›å»ºå£°éŸ³é…ç½®
    voice_profile = VoiceProfile(
        reference_audio_path=reference_audio,
        language=subtitle.language,
        duration=reference_duration
    )

    # 5. å¸¦æ£€æŸ¥ç‚¹çš„åˆ†æ®µåˆæˆ
    if progress:
        progress(0.2, "åˆæˆè¯­éŸ³ç‰‡æ®µ")

    synthesized_segments = _synthesize_segments_with_checkpoint(
        subtitle.segments,
        tts_provider,
        voice_profile,
        segment_state,
        cache_key,
        progress
    )

    tts_provider.unload()

    # 6. æ‹¼æ¥éŸ³é¢‘
    if progress:
        progress(0.9, "æ‹¼æ¥éŸ³é¢‘")

    full_audio = _merge_audio_segments(synthesized_segments, video.duration)

    # 7. ä¿å­˜å®Œæ•´éŸ³é¢‘åˆ°ç¼“å­˜
    try:
        _save_voice_cloning_to_cache(
            audio_cache,
            cache_key,
            full_audio,
            voice_profile
        )
        print("âœ… å®Œæ•´éŸ³é¢‘å·²ä¿å­˜åˆ°ç¼“å­˜")
    except Exception as e:
        print(f"âš ï¸ å®Œæ•´éŸ³é¢‘ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    # 8. æ¸…ç†åˆ†æ®µç¼“å­˜ï¼ˆå¯é€‰ï¼Œä¸ºäº†èŠ‚çœç©ºé—´ï¼‰
    try:
        for i in range(len(subtitle.segments)):
            segment_file = segment_state.get_segment_file_path(cache_key, i)
            if segment_file.exists():
                segment_file.unlink()
        print("âœ… åˆ†æ®µç¼“å­˜å·²æ¸…ç†")
    except Exception as e:
        print(f"âš ï¸ åˆ†æ®µç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

    if progress:
        progress(1.0, "è¯­éŸ³å…‹éš†å®Œæˆ")

    return VoiceCloningResult(
        audio_track=AudioTrack(full_audio, subtitle.language),
        voice_profile=voice_profile,
        total_segments=len(subtitle.segments),
        cache_hit=False
    )