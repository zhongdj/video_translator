"""
Application Layer - å¢é‡è¯­éŸ³å…‹éš†ç”¨ä¾‹
æ”¯æŒåˆ†æ®µåˆæˆã€ç¼“å­˜å’Œå¢é‡æ›´æ–°
"""
import time
from pathlib import Path
from typing import Optional, Callable, Dict

from domain.entities import (
    Video, Subtitle, AudioSegment, IncrementalSynthesisResult,
    VoiceProfile, AudioSample, AudioTrack
)
from domain.ports import (
    TTSProvider, VideoProcessor,
    CacheRepository, AudioSegmentRepository
)


# ============== è¾…åŠ©å‡½æ•° ============== #

def _get_segment_cache_key(
        video_path: Path,
        segment_index: int,
        text: str
) -> str:
    """ç”Ÿæˆç‰‡æ®µç¼“å­˜é”®ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰"""
    import hashlib
    content = f"{video_path.name}_{segment_index}_{text}"
    return hashlib.md5(content.encode()).hexdigest()[:16]


def _check_cached_segments(
        subtitle: Subtitle,
        video: Video,
        audio_repo: AudioSegmentRepository
) -> Dict[int, AudioSegment]:
    """æ£€æŸ¥å“ªäº›ç‰‡æ®µå·²æœ‰ç¼“å­˜"""
    cached = {}

    for idx, seg in enumerate(subtitle.segments):
        if audio_repo.exists(idx, video.path):
            audio_seg = audio_repo.load_segment(idx, video.path, seg)
            if audio_seg:
                cached[idx] = audio_seg

    return cached


def _synthesize_missing_segments(
        subtitle: Subtitle,
        cached_segments: Dict[int, AudioSegment],
        tts_provider: TTSProvider,
        voice_profile: VoiceProfile,
        video: Video,
        audio_repo: AudioSegmentRepository,
        progress: Optional[Callable[[float, str, int, AudioSegment], None]] = None
) -> Dict[int, AudioSegment]:
    """
    åˆæˆç¼ºå¤±çš„ç‰‡æ®µ

    Args:
        progress: è¿›åº¦å›è°ƒ (ratio, message, segment_index, audio_segment)
    """
    all_segments = {}
    all_segments.update(cached_segments)

    total = len(subtitle.segments)
    missing_indices = [
        idx for idx in range(total)
        if idx not in cached_segments
    ]

    if not missing_indices:
        return all_segments

    print(f"  ğŸ¤ éœ€è¦åˆæˆ {len(missing_indices)} ä¸ªæ–°ç‰‡æ®µ")

    for i, idx in enumerate(missing_indices):
        text_seg = subtitle.segments[idx]

        if progress:
            ratio = i / len(missing_indices)
            progress(ratio, f"åˆæˆç‰‡æ®µ {idx + 1}/{total}", idx, None)

        # å•æ®µåˆæˆ
        audio_sample = tts_provider.synthesize(
            text=text_seg.text,
            voice_profile=voice_profile,
            target_duration=text_seg.time_range.duration
        )

        # åˆ›å»ºå®ä½“
        cache_key = _get_segment_cache_key(
            video.path, idx, text_seg.text
        )

        audio_seg = AudioSegment(
            segment_index=idx,
            audio=audio_sample,
            text_segment=text_seg,
            cache_key=cache_key
        )

        # ä¿å­˜åˆ°ä»“å‚¨
        file_path = audio_repo.save_segment(idx, audio_seg, video.path)
        audio_seg = audio_seg.with_file_path(file_path)

        all_segments[idx] = audio_seg

        # å®æ—¶å›è°ƒï¼ˆæºå¸¦éŸ³é¢‘ç‰‡æ®µï¼‰
        if progress:
            progress(
                (i + 1) / len(missing_indices),
                f"å®Œæˆç‰‡æ®µ {idx + 1}/{total}",
                idx,
                audio_seg
            )

        print(f"  âœ… ç‰‡æ®µ {idx} å·²åˆæˆå¹¶ç¼“å­˜")

    return all_segments


def _merge_segments_to_track(
        audio_segments: Dict[int, AudioSegment],
        video_duration: float,
        language
) -> AudioTrack:
    """å°†ç‰‡æ®µåˆå¹¶ä¸ºå®Œæ•´éŸ³è½¨"""
    if not audio_segments:
        raise ValueError("æ²¡æœ‰éŸ³é¢‘ç‰‡æ®µå¯åˆå¹¶")

    # æŒ‰ç´¢å¼•æ’åº
    sorted_segments = sorted(audio_segments.items())

    sample_rate = sorted_segments[0][1].audio.sample_rate
    total_samples = int(video_duration * sample_rate)
    buffer = [0.0] * total_samples

    for idx, audio_seg in sorted_segments:
        text_seg = audio_seg.text_segment
        start_idx = int(text_seg.time_range.start_seconds * sample_rate)

        for i, sample in enumerate(audio_seg.audio.samples):
            target_idx = start_idx + i
            if target_idx < total_samples:
                buffer[target_idx] = sample

    full_audio = AudioSample(
        samples=tuple(buffer),
        sample_rate=sample_rate
    )

    return AudioTrack(full_audio, language)


# ============== ä¸»ç”¨ä¾‹å‡½æ•° ============== #

def incremental_voice_cloning_use_case(
        video: Video,
        subtitle: Subtitle,
        tts_provider: TTSProvider,
        video_processor: VideoProcessor,
        audio_repo: AudioSegmentRepository,
        cache_repo: CacheRepository,
        reference_audio_path: Optional[Path] = None,
        reference_duration: float = 10.0,
        progress: Optional[Callable[[float, str, int, Optional[AudioSegment]], None]] = None
) -> IncrementalSynthesisResult:
    """
    å¢é‡è¯­éŸ³å…‹éš†ç”¨ä¾‹

    ç‰¹æ€§:
    1. é€ç‰‡æ®µåˆæˆ
    2. å®æ—¶ç¼“å­˜
    3. æ–­ç‚¹ç»­ä¼ 
    4. è¿›åº¦å›è°ƒæºå¸¦éŸ³é¢‘ç‰‡æ®µ

    Args:
        progress: å›è°ƒå‡½æ•° (ratio, message, segment_index, audio_segment)
            - ratio: è¿›åº¦æ¯”ä¾‹ 0.0-1.0
            - message: è¿›åº¦æè¿°
            - segment_index: å½“å‰ç‰‡æ®µç´¢å¼•
            - audio_segment: å½“å‰å®Œæˆçš„éŸ³é¢‘ç‰‡æ®µï¼ˆå¯é€‰ï¼‰
    """
    start_time = time.perf_counter()

    if progress:
        progress(0.0, "å¼€å§‹å¢é‡è¯­éŸ³å…‹éš†", -1, None)

    # 1. å‡†å¤‡å‚è€ƒéŸ³é¢‘
    if progress:
        progress(0.05, "å‡†å¤‡å‚è€ƒéŸ³é¢‘", -1, None)

    if reference_audio_path is None:
        reference_audio_path = video_processor.extract_reference_audio(
            video, reference_duration
        )

    voice_profile = VoiceProfile(
        reference_audio_path=reference_audio_path,
        language=subtitle.language,
        duration=reference_duration
    )

    # 2. æ£€æŸ¥å·²ç¼“å­˜ç‰‡æ®µ
    if progress:
        progress(0.1, "æ£€æŸ¥ç¼“å­˜", -1, None)

    cached_segments = _check_cached_segments(subtitle, video, audio_repo)
    print(f"  ğŸ’¾ ç¼“å­˜å‘½ä¸­: {len(cached_segments)}/{len(subtitle.segments)} ç‰‡æ®µ")

    # 3. åˆæˆç¼ºå¤±ç‰‡æ®µï¼ˆå¸¦å®æ—¶å›è°ƒï¼‰
    def synthesis_progress(ratio, msg, idx, audio_seg):
        overall_ratio = 0.1 + ratio * 0.8  # 10%-90%
        if progress:
            progress(overall_ratio, msg, idx, audio_seg)

    all_segments = _synthesize_missing_segments(
        subtitle=subtitle,
        cached_segments=cached_segments,
        tts_provider=tts_provider,
        voice_profile=voice_profile,
        video=video,
        audio_repo=audio_repo,
        progress=synthesis_progress
    )

    # 4. åˆå¹¶ä¸ºå®Œæ•´éŸ³è½¨
    if progress:
        progress(0.9, "åˆå¹¶éŸ³é¢‘ç‰‡æ®µ", -1, None)

    audio_track = _merge_segments_to_track(
        all_segments, video.duration, subtitle.language
    )

    # 5. ä¿å­˜å®Œæ•´éŸ³è½¨ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
    if progress:
        progress(0.95, "ä¿å­˜å®Œæ•´éŸ³è½¨", -1, None)

    synthesis_time = time.perf_counter() - start_time

    if progress:
        progress(1.0, "å¢é‡åˆæˆå®Œæˆ", -1, None)

    return IncrementalSynthesisResult(
        total_segments=len(subtitle.segments),
        cached_segments=len(cached_segments),
        regenerated_segments=len(subtitle.segments) - len(cached_segments),
        audio_segments=tuple(
            all_segments[i] for i in sorted(all_segments.keys())
        ),
        synthesis_time=synthesis_time
    )


def regenerate_modified_segments_use_case(
        video: Video,
        original_subtitle: Subtitle,
        modified_subtitle: Subtitle,
        modified_indices: set[int],
        tts_provider: TTSProvider,
        video_processor: VideoProcessor,
        audio_repo: AudioSegmentRepository,
        reference_audio_path: Path,
        progress: Optional[Callable[[float, str, int, Optional[AudioSegment]], None]] = None
) -> IncrementalSynthesisResult:
    """
    é‡æ–°ç”Ÿæˆä¿®æ”¹è¿‡çš„ç‰‡æ®µ

    Args:
        modified_indices: è¢«ä¿®æ”¹çš„ç‰‡æ®µç´¢å¼•é›†åˆ
    """
    start_time = time.perf_counter()

    if not modified_indices:
        print("  â„¹ï¸  æ²¡æœ‰ä¿®æ”¹çš„ç‰‡æ®µï¼Œè·³è¿‡é‡æ–°ç”Ÿæˆ")
        # åŠ è½½æ‰€æœ‰ç°æœ‰ç‰‡æ®µ
        all_segments = {}
        for idx, seg in enumerate(modified_subtitle.segments):
            audio_seg = audio_repo.load_segment(idx, video.path, seg)
            if audio_seg:
                all_segments[idx] = audio_seg

        return IncrementalSynthesisResult(
            total_segments=len(modified_subtitle.segments),
            cached_segments=len(all_segments),
            regenerated_segments=0,
            audio_segments=tuple(
                all_segments[i] for i in sorted(all_segments.keys())
            ),
            synthesis_time=0.0
        )

    print(f"  ğŸ”„ éœ€è¦é‡æ–°ç”Ÿæˆ {len(modified_indices)} ä¸ªç‰‡æ®µ")

    voice_profile = VoiceProfile(
        reference_audio_path=reference_audio_path,
        language=modified_subtitle.language,
        duration=10.0
    )

    # é‡æ–°ç”Ÿæˆä¿®æ”¹çš„ç‰‡æ®µ
    regenerated = {}
    total = len(modified_indices)

    for i, idx in enumerate(sorted(modified_indices)):
        text_seg = modified_subtitle.segments[idx]

        if progress:
            progress(
                i / total,
                f"é‡æ–°ç”Ÿæˆç‰‡æ®µ {idx + 1}",
                idx,
                None
            )

        # åˆ é™¤æ—§ç¼“å­˜
        audio_repo.delete_segment(idx, video.path)

        # åˆæˆæ–°éŸ³é¢‘
        audio_sample = tts_provider.synthesize(
            text=text_seg.text,
            voice_profile=voice_profile,
            target_duration=text_seg.time_range.duration
        )

        cache_key = _get_segment_cache_key(
            video.path, idx, text_seg.text
        )

        audio_seg = AudioSegment(
            segment_index=idx,
            audio=audio_sample,
            text_segment=text_seg,
            cache_key=cache_key
        )

        # ä¿å­˜
        file_path = audio_repo.save_segment(idx, audio_seg, video.path)
        audio_seg = audio_seg.with_file_path(file_path)

        regenerated[idx] = audio_seg

        if progress:
            progress(
                (i + 1) / total,
                f"å®Œæˆç‰‡æ®µ {idx + 1}",
                idx,
                audio_seg
            )

        print(f"  âœ… ç‰‡æ®µ {idx} å·²é‡æ–°ç”Ÿæˆ")

    # åŠ è½½æœªä¿®æ”¹çš„ç‰‡æ®µ
    all_segments = dict(regenerated)
    for idx in range(len(modified_subtitle.segments)):
        if idx not in modified_indices:
            audio_seg = audio_repo.load_segment(
                idx, video.path, modified_subtitle.segments[idx]
            )
            if audio_seg:
                all_segments[idx] = audio_seg

    synthesis_time = time.perf_counter() - start_time

    return IncrementalSynthesisResult(
        total_segments=len(modified_subtitle.segments),
        cached_segments=len(all_segments) - len(regenerated),
        regenerated_segments=len(regenerated),
        audio_segments=tuple(
            all_segments[i] for i in sorted(all_segments.keys())
        ),
        synthesis_time=synthesis_time
    )