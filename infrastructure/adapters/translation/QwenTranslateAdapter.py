import torch

# å¯¼å…¥é¢†åŸŸå±‚
from domain.entities import (
    # Entities
    TextSegment,
    # Value Objects
    LanguageCode, )


class QwenTranslationAdapter:
    """Qwen ç¿»è¯‘é€‚é…å™¨"""

    def __init__(self, model_name: str = "Qwen/Qwen2.5-7B", device: str = "cuda"):
        self.model_name = model_name
        self.device = device
        self._model = None
        self._tokenizer = None

    def _load_model(self):
        """æ‡’åŠ è½½æ¨¡å‹"""
        if self._model is None:
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

            print(f"ğŸ”„ åŠ è½½ Qwen ç¿»è¯‘æ¨¡å‹: {self.model_name}")

            # 8bit é‡åŒ–é…ç½®
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True
            )

            self._tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                padding_side='left'
            )

            self._model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )

        return self._model, self._tokenizer

    def translate(
            self,
            segments: tuple[TextSegment, ...],
            source_lang: LanguageCode,
            target_lang: LanguageCode
    ) -> tuple[TextSegment, ...]:
        """å®ç° TranslationProvider æ¥å£"""
        model, tokenizer = self._load_model()

        # æ‰¹é‡ç¿»è¯‘
        batch_size = len(segments)
        translated_segments = []

        print(f" Translating {len(segments)} segments in {batch_size} batch")
        for i in range(0, len(segments), batch_size):
            batch = segments[i:i + batch_size]

            # æ„é€  prompt
            prompts = [
                f"Translate the following {source_lang.value} text into {target_lang.value}. "
                f"Output only the translation.\n{source_lang.value}: {seg.text}\n{target_lang.value}:"
                for seg in batch
            ]

            # ç¼–ç 
            inputs = tokenizer(
                prompts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=256
            ).to(model.device)

            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=128,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )

            # è§£ç 
            for seg, output_ids in zip(batch, outputs):
                # åªå–æ–°ç”Ÿæˆçš„ token
                new_tokens = output_ids[inputs.input_ids.shape[1]:]
                translated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
                translated_text = translated_text.strip().split('\n')[0]  # å–ç¬¬ä¸€è¡Œ

                translated_segment = TextSegment(
                    text=translated_text,
                    time_range=seg.time_range,
                    language=target_lang
                )
                translated_segments.append(translated_segment)

            # æ¸…ç†ä¸­é—´å¼ é‡
            del inputs, outputs
            torch.cuda.empty_cache()

        return tuple(translated_segments)

    def unload(self):
        """å¸è½½æ¨¡å‹"""
        if self._model is not None:
            del self._model
            del self._tokenizer
            self._model = None
            self._tokenizer = None
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print("ğŸ§¹ Qwen ç¿»è¯‘æ¨¡å‹å·²å¸è½½")