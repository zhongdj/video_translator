# infrastructure/adapters/tts/indextts_adapter.py
import os
import sys
import time
from pathlib import Path
from typing import Optional, Tuple
import numpy as np
import torch
import torchaudio

from domain.ports import TTSProvider
from domain.entities import *


class IndexTTSAdapter(TTSProvider):
    """IndexTTS2 é€‚é…å™¨ - ä¿®å¤ç‰ˆ"""

    def __init__(self, model_path: Optional[str] = None, device: str = "cuda"):
        self.enable_auto_recovery = True
        self.default_batch_size = 16
        self._is_loaded = None
        self.device = device
        self._model = None

        # è®¾ç½® IndexTTS2 è·¯å¾„
        current_dir = Path(__file__).parent
        self.indextts2_path = current_dir.parent.parent.parent.parent / "indextts2"

        # åŸºç¡€é…ç½®å‚æ•°
        self.temperature = 0.8
        self.top_p = 0.8
        self.speed = 1.0

        print(f"ğŸ” æ£€æµ‹ IndexTTS2 è·¯å¾„:")
        print(f"   IndexTTS2 ä¸»ç›®å½•: {self.indextts2_path}")
        print(f"   ä¸»ç›®å½•å­˜åœ¨: {self.indextts2_path.exists()}")

        # æ£€æŸ¥å…³é”®ç›®å½•
        self.checkpoints_path = self.indextts2_path / "checkpoints"
        print(f"   checkpoints ç›®å½•: {self.checkpoints_path.exists()}")

        if self.checkpoints_path.exists():
            print(f"   checkpoints å†…å®¹: {list(self.checkpoints_path.glob('*'))}")

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None

        # æ·»åŠ WebUIå‚æ•°
        self.gpt_config = {
            'do_sample': True,
            'temperature': 0.8,
            'top_p': 0.8,
            'top_k': 30,
            'num_beams': 3,
            'repetition_penalty': 10,
            'length_penalty': 0,
        }
        self.max_mel_tokens = 1500
        self.split_max_tokens = 120

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("temp_output")
        self.output_dir.mkdir(exist_ok=True)
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_texts": 0,
            "total_batches": 0,
            "total_time": 0.0,
            "oom_count": 0,
            "peak_memory_gb": 0.0
        }


    def load(self):
        """åŠ è½½ IndexTTS 2.0 æ¨¡å‹"""
        if not self.indextts2_path.exists():
            raise ImportError(f"âŒ IndexTTS2 ç›®å½•ä¸å­˜åœ¨: {self.indextts2_path}")

        if self.model is not None:
            return self.model

        print(f"ğŸ”„ åŠ è½½ IndexTTS 2.0 æ¨¡å‹...")

        try:
            # æ·»åŠ è·¯å¾„åˆ° sys.path
            if str(self.indextts2_path) not in sys.path:
                sys.path.insert(0, str(self.indextts2_path))

            # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼
            IndexTTS2 = None
            try:
                from indextts.infer_v2 import IndexTTS2
                print("âœ… ä» indextts.infer_v2 å¯¼å…¥ IndexTTS2 æˆåŠŸ")
            except ImportError as e:
                print(f"âŒ ç¬¬ä¸€ç§å¯¼å…¥æ–¹å¼å¤±è´¥: {e}")
                try:
                    from infer_v2 import IndexTTS2
                    print("âœ… ä» infer_v2 å¯¼å…¥ IndexTTS2 æˆåŠŸ")
                except ImportError as e:
                    print(f"âŒ ç¬¬äºŒç§å¯¼å…¥æ–¹å¼å¤±è´¥: {e}")
                    try:
                        sys.path.append(str(self.indextts2_path))
                        from indextts2.infer_v2 import IndexTTS2
                        print("âœ… ä» indextts2.infer_v2 å¯¼å…¥ IndexTTS2 æˆåŠŸ")
                    except ImportError as e:
                        print(f"âŒ æ‰€æœ‰å¯¼å…¥æ–¹å¼éƒ½å¤±è´¥: {e}")
                        return self._create_dummy_model()

            # åˆå§‹åŒ–æ¨¡å‹
            config_path = self.checkpoints_path / "config.yaml"
            if not config_path.exists():
                # å°è¯•å¯»æ‰¾å…¶ä»–é…ç½®æ–‡ä»¶
                config_files = list(self.checkpoints_path.glob("*.yaml")) + list(self.checkpoints_path.glob("*.yml"))
                if config_files:
                    config_path = config_files[0]
                    print(f"ğŸ”§ ä½¿ç”¨å¤‡ç”¨é…ç½®æ–‡ä»¶: {config_path}")
                else:
                    raise FileNotFoundError(f"æ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ in {self.checkpoints_path}")

            print(f"ğŸ”§ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
            print(f"ğŸ”§ æ¨¡å‹ç›®å½•: {self.checkpoints_path}")

            self.model = IndexTTS2(
                model_dir=str(self.checkpoints_path),
                cfg_path=str(config_path),
                device=self.device,
                use_fp16=(self.device != "cpu")
            )
            self._is_loaded = True

            print("âœ… IndexTTS 2.0 æ¨¡å‹åŠ è½½å®Œæˆ")
            return self.model

        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return self._create_dummy_model()

    def _create_dummy_model(self):
        """åˆ›å»ºè™šæ‹Ÿæ¨¡å‹ç”¨äºæµ‹è¯•"""

        class DummyModel:
            def infer(self, text, reference_audio, speed=1.0):
                print(f"ğŸ”Š è™šæ‹Ÿåˆæˆ: '{text}' (å‚è€ƒéŸ³é¢‘: {reference_audio}, è¯­é€Ÿ: {speed})")
                # ç”ŸæˆåŸºäºæ–‡æœ¬é•¿åº¦çš„æµ‹è¯•éŸ³é¢‘
                duration = max(1.0, len(text) * 0.1)
                samples = int(duration * 22050)
                t = np.linspace(0, duration * 2 * np.pi, samples)
                audio_data = 0.3 * np.sin(440 * t) * np.exp(-0.001 * t)
                return (audio_data, 22050)  # è¿”å›å…ƒç»„

        return DummyModel()


    def unload(self) -> None:
        """å¸è½½æ¨¡å‹"""
        if not self._is_loaded:
            return

        if self.model is not None:
            del self.model
            self.model = None

        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self._is_loaded = False
        print(f"âœ… IndexTTS2 æ¨¡å‹å·²å¸è½½")

    def synthesize(
            self,
            text: str,
            voice_profile: VoiceProfile,
            target_duration: Optional[float] = None
    ) -> AudioSample:
        """å•å¥åˆæˆï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        if not self._is_loaded:
            self.load()

        # è°ƒç”¨æ‰¹é‡æ¥å£ï¼ˆbatch_size=1ï¼‰
        results = self.batch_synthesize(
            texts=[text],
            reference_audio_path=voice_profile.reference_audio_path,
            language=voice_profile.language
        )

        return results[0]

    def suggest_batch_size(self, texts: list[str]) -> int:
        """
        æ ¹æ®æ–‡æœ¬ç‰¹å¾å»ºè®® batch_size

        è€ƒè™‘å› ç´ ï¼š
        1. å¹³å‡æ–‡æœ¬é•¿åº¦
        2. æœ€é•¿æ–‡æœ¬é•¿åº¦
        3. æ–‡æœ¬æ•°é‡
        """
        if not texts:
            return self.default_batch_size

        avg_len = sum(len(t) for t in texts) / len(texts)
        max_len = max(len(t) for t in texts)

        # å¯å‘å¼è§„åˆ™
        if max_len > 200 or avg_len > 100:
            # é•¿æ–‡æœ¬ï¼šå°æ‰¹é‡
            suggested = 8
        elif max_len > 100 or avg_len > 50:
            # ä¸­ç­‰æ–‡æœ¬ï¼šä¸­æ‰¹é‡
            suggested = 16
        else:
            # çŸ­æ–‡æœ¬ï¼šå¤§æ‰¹é‡
            suggested = 24

        # è€ƒè™‘æ€»æ•°é‡
        if len(texts) < suggested:
            suggested = len(texts)

        return int(suggested / 2)

    def batch_synthesize(
            self,
            texts: list[str],
            reference_audio_path: Path,
            language: LanguageCode,
            batch_size: Optional[int] = None
    ) -> tuple[AudioSample, ...]:
        """
        æ‰¹é‡åˆæˆï¼ˆè‡ªé€‚åº” batch_sizeï¼‰

        Args:
            texts: å¾…åˆæˆæ–‡æœ¬åˆ—è¡¨
            reference_audio_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
            language: ç›®æ ‡è¯­è¨€
            batch_size: æ‰¹é‡å¤§å°ï¼ˆNone åˆ™è‡ªåŠ¨å»ºè®®ï¼‰
        """
        if not self._is_loaded:
            self.load()

        # è‡ªåŠ¨å»ºè®® batch_size
        if batch_size is None:
            batch_size = self.suggest_batch_size(texts)
            print(f"  ğŸ’¡ è‡ªåŠ¨å»ºè®® batch_size={batch_size} (åŸºäºæ–‡æœ¬é•¿åº¦åˆ†æ)")

        total_texts = len(texts)
        print(f"  ğŸ“ æ‰¹é‡åˆæˆ: {total_texts} ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œbatch_size={batch_size}")

        # å°è¯•æ‰¹é‡åˆæˆï¼Œå¤±è´¥åˆ™è‡ªåŠ¨é™çº§
        try:
            return self._batch_synthesize_with_recovery(
                texts=texts,
                reference_audio_path=reference_audio_path,
                language=language,
                batch_size=batch_size
            )
        except Exception as e:
            print(f"âŒ æ‰¹é‡åˆæˆå¤±è´¥: {e}")
            raise

    def _batch_synthesize_with_recovery(
            self,
            texts: list[str],
            reference_audio_path: Path,
            language: LanguageCode,
            batch_size: int
    ) -> tuple[AudioSample, ...]:
        """å¸¦ OOM è‡ªåŠ¨æ¢å¤çš„æ‰¹é‡åˆæˆ"""

        try:
            return self._do_batch_synthesize(
                texts=texts,
                reference_audio_path=reference_audio_path,
                language=language,
                batch_size=batch_size
            )
        except RuntimeError as e:
            if "out of memory" in str(e).lower() and self.enable_auto_recovery:
                # OOM å‘ç”Ÿï¼Œå°è¯•æ¢å¤
                self.stats["oom_count"] += 1
                new_batch_size = max(1, batch_size // 2)

                print(f"  âš ï¸  GPU OOM! è‡ªåŠ¨é™çº§: batch_size {batch_size} â†’ {new_batch_size}")

                # æ¸…ç†å†…å­˜
                torch.cuda.empty_cache()

                # é€’å½’é‡è¯•ï¼ˆæ›´å°çš„ batch_sizeï¼‰
                return self._batch_synthesize_with_recovery(
                    texts=texts,
                    reference_audio_path=reference_audio_path,
                    language=language,
                    batch_size=new_batch_size
                )
            else:
                raise

    def _do_batch_synthesize(
            self,
            texts: list[str],
            reference_audio_path: Path,
            language: LanguageCode,
            batch_size: int
    ) -> tuple[AudioSample, ...]:
        """å®é™…æ‰§è¡Œæ‰¹é‡åˆæˆ"""

        total_texts = len(texts)
        all_audio_samples = []
        num_batches = (total_texts + batch_size - 1) // batch_size

        batch_start_time = time.perf_counter()

        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_texts)
            batch_texts = texts[start_idx:end_idx]

            print(f"    æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}: å¤„ç† {len(batch_texts)} ä¸ªç‰‡æ®µ [{start_idx}:{end_idx}]",
                  end="")

            iter_start = time.perf_counter()

            # è°ƒç”¨æ‰¹é‡æ¨ç†
            batch_results = self.model.batch_infer_same_speaker(
                texts=batch_texts,
                spk_audio_prompt=str(reference_audio_path),
                output_paths=None,
                emo_audio_prompt=None,
                emo_alpha=1.0,
                interval_silence=0,
                verbose=False,
                max_text_tokens_per_segment=120,
                do_sample=True,
                top_p=0.8,
                top_k=30,
                temperature=0.8,
                length_penalty=0.0,
                num_beams=3,
                repetition_penalty=10.0,
                max_mel_tokens=1500
            )

            iter_time = time.perf_counter() - iter_start

            # è®°å½•å†…å­˜å³°å€¼
            if torch.cuda.is_available():
                peak_memory = torch.cuda.max_memory_allocated() / 1e9
                self.stats["peak_memory_gb"] = max(self.stats["peak_memory_gb"], peak_memory)
                print(f" âœ“ {iter_time:.2f}ç§’ (GPU: {peak_memory:.1f}GB)")
            else:
                print(f" âœ“ {iter_time:.2f}ç§’")

            # è½¬æ¢ä¸º AudioSample
            for sampling_rate, wav_data in batch_results:
                if wav_data.ndim == 2:
                    wav_data = wav_data[:, 0]

                audio_sample = AudioSample(
                    samples=tuple(float(s) for s in wav_data.flatten()),
                    sample_rate=sampling_rate
                )
                all_audio_samples.append(audio_sample)

        # æ›´æ–°ç»Ÿè®¡
        total_time = time.perf_counter() - batch_start_time
        self.stats["total_texts"] += total_texts
        self.stats["total_batches"] += num_batches
        self.stats["total_time"] += total_time

        avg_time = total_time / total_texts
        print(f"  âœ… å®Œæˆ: {total_texts} ä¸ªç‰‡æ®µ, æ€»è€—æ—¶ {total_time:.2f}ç§’ (å¹³å‡ {avg_time:.3f}ç§’/ç‰‡æ®µ)")

        return tuple(all_audio_samples)

    def update_config(self, temperature: float = None, top_p: float = None, speed: float = None):
        """æ›´æ–°é…ç½®å‚æ•°"""
        if temperature is not None:
            self.temperature = temperature
            self.gpt_config['temperature'] = temperature
        if top_p is not None:
            self.top_p = top_p
            self.gpt_config['top_p'] = top_p
        if speed is not None:
            self.speed = speed
