# infrastructure/adapters/tts/indextts_adapter.py
import sys
import time

import numpy as np
import torch

from domain.entities import *
from domain.ports import TTSProvider


class IndexTTSAdapter(TTSProvider):
    """IndexTTS2 é€‚é…å™¨ - ä¿®å¤ç‰ˆ"""



    def __init__(self, model_path: Optional[str] = None, device: str = "cuda"):
        self.enable_auto_recovery = True
        self.default_batch_size = 16
        self._is_loaded = None
        self.device = device
        self._model = None

        # è®¾ç½® IndexTTS2 è·¯å¾„
        current_dir = Path(__file__).parent
        self.indextts2_path = current_dir.parent.parent.parent.parent / "indextts2"

        # åŸºç¡€é…ç½®å‚æ•°
        self.temperature = 0.8
        self.top_p = 0.8
        self.speed = 1.0

        print(f"ğŸ” æ£€æµ‹ IndexTTS2 è·¯å¾„:")
        print(f"   IndexTTS2 ä¸»ç›®å½•: {self.indextts2_path}")
        print(f"   ä¸»ç›®å½•å­˜åœ¨: {self.indextts2_path.exists()}")

        # æ£€æŸ¥å…³é”®ç›®å½•
        self.checkpoints_path = self.indextts2_path / "checkpoints"
        print(f"   checkpoints ç›®å½•: {self.checkpoints_path.exists()}")

        if self.checkpoints_path.exists():
            print(f"   checkpoints å†…å®¹: {list(self.checkpoints_path.glob('*'))}")

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None

        # æ·»åŠ WebUIå‚æ•°
        self.gpt_config = {
            'do_sample': True,
            'temperature': 0.8,
            'top_p': 0.8,
            'top_k': 30,
            'num_beams': 3,
            'repetition_penalty': 10,
            'length_penalty': 0,
        }
        self.max_mel_tokens = 1500
        self.split_max_tokens = 120

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("temp_output")
        self.output_dir.mkdir(exist_ok=True)
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_texts": 0,
            "total_batches": 0,
            "total_time": 0.0,
            "oom_count": 0,
            "peak_memory_gb": 0.0
        }
        # é€Ÿåº¦é…ç½®
        self.speed = 1.0  # é»˜è®¤é€Ÿåº¦
        self.speed_index = 0  # IndexTTS2 ä½¿ç”¨æ•´æ•°ç´¢å¼•æ§åˆ¶é€Ÿåº¦

        # é€Ÿåº¦æ˜ å°„è¡¨ (æ ¹æ® IndexTTS2 çš„å®é™…å®ç°)
        # é€šå¸¸: 0=æ­£å¸¸, 1=ç¨å¿«, 2=å¿«, -1=ç¨æ…¢, -2=æ…¢
        self.speed_mapping = {
            0.5: -2,  # 0.5x -> å¾ˆæ…¢
            0.75: -1,  # 0.75x -> ç¨æ…¢
            1.0: 0,  # 1.0x -> æ­£å¸¸
            1.25: 1,  # 1.25x -> ç¨å¿«
            1.5: 2,  # 1.5x -> å¿«
        }


    def load(self):
        """åŠ è½½ IndexTTS 2.0 æ¨¡å‹"""
        if not self.indextts2_path.exists():
            raise ImportError(f"âŒ IndexTTS2 ç›®å½•ä¸å­˜åœ¨: {self.indextts2_path}")

        if self.model is not None:
            return self.model

        print(f"ğŸ”„ åŠ è½½ IndexTTS 2.0 æ¨¡å‹...")

        try:
            # æ·»åŠ è·¯å¾„åˆ° sys.path
            if str(self.indextts2_path) not in sys.path:
                sys.path.insert(0, str(self.indextts2_path))

            # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼
            IndexTTS2 = None
            try:
                from indextts.infer_v2 import IndexTTS2
                print("âœ… ä» indextts.infer_v2 å¯¼å…¥ IndexTTS2 æˆåŠŸ")
            except ImportError as e:
                print(f"âŒ ç¬¬ä¸€ç§å¯¼å…¥æ–¹å¼å¤±è´¥: {e}")
                try:
                    from infer_v2 import IndexTTS2
                    print("âœ… ä» infer_v2 å¯¼å…¥ IndexTTS2 æˆåŠŸ")
                except ImportError as e:
                    print(f"âŒ ç¬¬äºŒç§å¯¼å…¥æ–¹å¼å¤±è´¥: {e}")
                    try:
                        sys.path.append(str(self.indextts2_path))
                        from indextts2.infer_v2 import IndexTTS2
                        print("âœ… ä» indextts2.infer_v2 å¯¼å…¥ IndexTTS2 æˆåŠŸ")
                    except ImportError as e:
                        print(f"âŒ æ‰€æœ‰å¯¼å…¥æ–¹å¼éƒ½å¤±è´¥: {e}")
                        return self._create_dummy_model()

            # åˆå§‹åŒ–æ¨¡å‹
            config_path = self.checkpoints_path / "config.yaml"
            if not config_path.exists():
                # å°è¯•å¯»æ‰¾å…¶ä»–é…ç½®æ–‡ä»¶
                config_files = list(self.checkpoints_path.glob("*.yaml")) + list(self.checkpoints_path.glob("*.yml"))
                if config_files:
                    config_path = config_files[0]
                    print(f"ğŸ”§ ä½¿ç”¨å¤‡ç”¨é…ç½®æ–‡ä»¶: {config_path}")
                else:
                    raise FileNotFoundError(f"æ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ in {self.checkpoints_path}")

            print(f"ğŸ”§ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
            print(f"ğŸ”§ æ¨¡å‹ç›®å½•: {self.checkpoints_path}")

            self.model = IndexTTS2(
                model_dir=str(self.checkpoints_path),
                cfg_path=str(config_path),
                device=self.device,
                use_fp16=(self.device != "cpu")
            )
            self._is_loaded = True

            print("âœ… IndexTTS 2.0 æ¨¡å‹åŠ è½½å®Œæˆ")
            return self.model

        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return self._create_dummy_model()

    def _create_dummy_model(self):
        """åˆ›å»ºè™šæ‹Ÿæ¨¡å‹ç”¨äºæµ‹è¯•"""

        class DummyModel:
            def infer(self, text, reference_audio, speed=1.0):
                print(f"ğŸ”Š è™šæ‹Ÿåˆæˆ: '{text}' (å‚è€ƒéŸ³é¢‘: {reference_audio}, è¯­é€Ÿ: {speed})")
                # ç”ŸæˆåŸºäºæ–‡æœ¬é•¿åº¦çš„æµ‹è¯•éŸ³é¢‘
                duration = max(1.0, len(text) * 0.1)
                samples = int(duration * 22050)
                t = np.linspace(0, duration * 2 * np.pi, samples)
                audio_data = 0.3 * np.sin(440 * t) * np.exp(-0.001 * t)
                return (audio_data, 22050)  # è¿”å›å…ƒç»„

        return DummyModel()

    def unload(self) -> None:
        """å¸è½½æ¨¡å‹"""
        if not self._is_loaded:
            return

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if self.stats["total_texts"] > 0:
            avg_time = self.stats["total_time"] / self.stats["total_texts"]
            print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            print(f"   æ€»æ–‡æœ¬æ•°: {self.stats['total_texts']}")
            print(f"   æ€»æ‰¹æ¬¡æ•°: {self.stats['total_batches']}")
            print(f"   æ€»è€—æ—¶: {self.stats['total_time']:.2f}ç§’")
            print(f"   å¹³å‡: {avg_time:.3f}ç§’/æ–‡æœ¬")
            print(f"   å³°å€¼å†…å­˜: {self.stats['peak_memory_gb']:.2f}GB")
            if self.stats['oom_count'] > 0:
                print(f"   âš ï¸ OOMæ¬¡æ•°: {self.stats['oom_count']}")

        if self.model is not None:
            del self.model
            self.model = None

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self._is_loaded = False
        print(f"âœ… IndexTTS2 æ¨¡å‹å·²å¸è½½")

    def _speed_to_index(self, speed: float) -> int:
        """å°†é€Ÿåº¦å› å­è½¬æ¢ä¸º IndexTTS2 çš„é€Ÿåº¦ç´¢å¼•"""
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„é¢„å®šä¹‰é€Ÿåº¦
        closest_speed = min(self.speed_mapping.keys(),
                            key=lambda x: abs(x - speed))
        return self.speed_mapping[closest_speed]


    def synthesize(
            self,
            text: str,
            voice_profile: VoiceProfile,
            target_duration: Optional[float] = None
    ) -> AudioSample:
        """å•å¥åˆæˆ(å…¼å®¹æ—§æ¥å£)"""
        if not self._is_loaded:
            self.load()

        # å¦‚æœæŒ‡å®šäº† target_duration,å…ˆè¯•åˆæˆä¸€æ¬¡ä¼°ç®—æ—¶é•¿
        if target_duration is not None:
            # ç¬¬ä¸€æ¬¡åˆæˆ(ä½¿ç”¨é»˜è®¤é€Ÿåº¦)
            results = self.batch_synthesize(
                texts=[text],
                reference_audio_path=voice_profile.reference_audio_path,
                language=voice_profile.language,
                batch_size=8,
                speed_factor=1.0,  # æ˜ç¡®ä¼ é€’é€Ÿåº¦
                target_durations=[target_duration],
            )
            audio = results[0]
            actual_duration = len(audio.samples) / audio.sample_rate

            # å¦‚æœè¶…æ—¶,è°ƒæ•´è¯­é€Ÿé‡æ–°åˆæˆ
            if actual_duration > target_duration:
                speed_factor = actual_duration / (0.95 * target_duration)
                adjusted_speed = min(speed_factor, 2.0)  # æœ€å¤§2å€é€Ÿ

                print(f"  âš¡ éŸ³é¢‘è¿‡é•¿ ({actual_duration:.2f}s > {target_duration:.2f}s)")
                print(f"     è°ƒæ•´è¯­é€Ÿè‡³ {adjusted_speed:.2f}x é‡æ–°åˆæˆ")

                # é‡æ–°åˆæˆ(ä½¿ç”¨è°ƒæ•´åçš„é€Ÿåº¦)
                results = self.batch_synthesize(
                    texts=[text],
                    reference_audio_path=voice_profile.reference_audio_path,
                    language=voice_profile.language,
                    batch_size=8,
                    speed_factor=adjusted_speed,
                    target_durations=[target_duration],
                )

            return results[0]

        # æ²¡æœ‰ target_duration,ç›´æ¥åˆæˆ
        results = self.batch_synthesize(
            texts=[text],
            reference_audio_path=voice_profile.reference_audio_path,
            language=voice_profile.language,
            batch_size=8,
            speed_factor=1.0  # é»˜è®¤é€Ÿåº¦
        )
        return results[0]


    def suggest_batch_size(self, texts: list[str]) -> int:
        """
        æ ¹æ®æ–‡æœ¬ç‰¹å¾å»ºè®® batch_size

        è€ƒè™‘å› ç´ ï¼š
        1. å¹³å‡æ–‡æœ¬é•¿åº¦
        2. æœ€é•¿æ–‡æœ¬é•¿åº¦
        3. æ–‡æœ¬æ•°é‡
        """
        if not texts:
            return self.default_batch_size

        avg_len = sum(len(t) for t in texts) / len(texts)
        max_len = max(len(t) for t in texts)

        # å¯å‘å¼è§„åˆ™
        if max_len > 200 or avg_len > 100:
            # é•¿æ–‡æœ¬ï¼šå°æ‰¹é‡
            suggested = 8
        elif max_len > 100 or avg_len > 50:
            # ä¸­ç­‰æ–‡æœ¬ï¼šä¸­æ‰¹é‡
            suggested = 16
        else:
            # çŸ­æ–‡æœ¬ï¼šå¤§æ‰¹é‡
            suggested = 24

        # è€ƒè™‘æ€»æ•°é‡
        if len(texts) < suggested:
            suggested = len(texts)

        return max(1, int(suggested/4))

    def batch_synthesize(
            self,
            texts: list[str],
            reference_audio_path: Path,
            language: LanguageCode,
            batch_size: Optional[int] = None,
            speed_factor: float = 1.0,  # ğŸ”¥ æ–°å¢é€Ÿåº¦å‚æ•°
            target_durations: Optional[list[float]] = None  # âœ… æ–°å¢
    ) -> tuple[AudioSample, ...]:
        """
        æ‰¹é‡åˆæˆ(è‡ªé€‚åº” batch_size)

        Args:
            speed_factor: è¯­é€Ÿå› å­ (1.0=æ­£å¸¸, >1.0=åŠ å¿«, <1.0=å‡æ…¢)
        """
        if not self._is_loaded:
            self.load()

        total_texts = len(texts)
        print(f"  ğŸ“ æ‰¹é‡åˆæˆ: {total_texts} ä¸ªæ–‡æœ¬ç‰‡æ®µ, speed={speed_factor}x")

        try:
            return self._batch_synthesize_with_recovery(
                texts=texts,
                reference_audio_path=reference_audio_path,
                speed_factor=speed_factor,  # ğŸ”¥ ä¼ é€’é€Ÿåº¦å‚æ•°
                target_durations=target_durations
            )
        except Exception as e:
            print(f"âŒ æ‰¹é‡åˆæˆå¤±è´¥: {e}")
            raise

    def _batch_synthesize_with_recovery(
            self,
            texts: list[str],
            reference_audio_path: Path,
            speed_factor: float = 1.0,  # ğŸ”¥ æ–°å¢å‚æ•°
            target_durations: Optional[list[float]] = None  # âœ…
    ) -> tuple[AudioSample, ...]:
        """å¸¦ OOM è‡ªåŠ¨æ¢å¤çš„æ‰¹é‡åˆæˆ"""
        try:
            return self._do_batch_synthesize(
                texts=texts,
                reference_audio_path=reference_audio_path,
                speed_factor=speed_factor,  # ğŸ”¥ ä¼ é€’å‚æ•°
                target_durations=target_durations  # âœ…
            )
        except RuntimeError as e:
            if "out of memory" in str(e).lower() and self.enable_auto_recovery:
                self.stats["oom_count"] += 1
            raise
        finally:
            torch.cuda.empty_cache()

    def _do_batch_synthesize(
            self,
            texts: list[str],
            reference_audio_path: Path,
            speed_factor: float = 1.0,
            target_durations: Optional[list[float]] = None  # (æ¯æ¡ text å¯¹åº”ç›®æ ‡ç§’æ•°)
    ) -> tuple[AudioSample, ...]:
        """å®é™…æ‰§è¡Œæ‰¹é‡åˆæˆï¼ˆæŒ‰æ¡è®¡ç®—æ¯æ¡çš„ max_text_tokens_per_segment & max_mel_tokensï¼‰"""
        print(f"  âš ï¸  reference audio path: {str(reference_audio_path)}")
        print(f"  âš¡ speed factor: {speed_factor}x")

        total_texts = len(texts)
        all_audio_samples = []
        batch_start_time = time.perf_counter()

        # è½¬æ¢é€Ÿåº¦å› å­ä¸º IndexTTS2 çš„é€Ÿåº¦ç´¢å¼•
        speed_index = self._speed_to_index(speed_factor)
        print(f"  ğŸ“Š speed_index={speed_index} (mapped from {speed_factor}x)")

        # ä¼°ç®—å‚æ•°çš„åŸºå‡†å€¼ï¼ˆå¯è°ƒæ•´æˆ–æ”¹æˆä» token_calculator å¯¼å…¥ï¼‰
        token_per_sec = 12.8
        mel_per_sec = 55
        margin = 1.05  # ç¨å¾®æ”¾å®½ä¸€ç‚¹ï¼Œé¿å…æˆªæ–­

        def estimate_text_tokens(sec):
            if sec is None:
                return min(self.split_max_tokens, 120)
            val = int(sec * token_per_sec * margin)
            val = max(20, val)
            return min(val, int(self.split_max_tokens))

        def estimate_mel_tokens(sec):
            if sec is None:
                return int(self.max_mel_tokens)
            val = int(sec * mel_per_sec * margin)
            val = max(50, val)
            return min(val, int(self.max_mel_tokens))

        # å¦‚æœç»™äº† target_durations é•¿åº¦æ ¡éªŒ
        if target_durations and len(target_durations) != len(texts):
            raise ValueError("target_durations length must equal texts length")

        # é€æ¡åˆæˆï¼ˆå¦‚æœåº•å±‚æ”¯æŒæ‰¹é‡ä¼ å…¥ per-item å‚æ•°ï¼Œå¯æ”¹ä¸ºä¸€æ¬¡æ€§ä¼ å…¥ listï¼‰
        for idx, text in enumerate(texts):
            tgt_sec = None
            if target_durations:
                tgt_sec = target_durations[idx]

            max_text_tokens_per_segment = 120 #estimate_text_tokens(tgt_sec)
            max_mel_tokens = 1500 #estimate_mel_tokens(tgt_sec)

            print(
                f"  â–¶ [{idx}] tgt_sec={tgt_sec} -> max_text_tokens_per_segment={max_text_tokens_per_segment}, max_mel_tokens={max_mel_tokens}")

            # è°ƒç”¨åº•å±‚ï¼ˆå•æ¡æˆ–å°æ‰¹é‡æ¨¡å¼ï¼‰
            results = self.model.batch_infer_same_speaker(
                texts=[text],
                spk_audio_prompt=str(reference_audio_path),
                output_paths=None,
                emo_audio_prompt=None,
                emo_alpha=1.0,
                interval_silence=0,
                verbose=True,
                max_text_tokens_per_segment=max_text_tokens_per_segment,
                speed_index=speed_index,
                # generation kwargs
                do_sample=self.gpt_config.get('do_sample', True),
                top_p=self.top_p,
                top_k=self.gpt_config.get('top_k', 30),
                temperature=self.temperature,
                length_penalty=self.gpt_config.get('length_penalty', 0.0),
                num_beams=self.gpt_config.get('num_beams', 3),
                repetition_penalty=10.0,
                max_mel_tokens=max_mel_tokens
            )

            # å¤„ç†è¿”å›å€¼ï¼ˆbatch_infer_same_speaker è¿”å›çš„å¯èƒ½æ˜¯ list/tupleï¼‰
            if not results:
                print(f"âš ï¸ [{idx}] æ— è¿”å›ç»“æœ")
                continue

            # å–ç¬¬ä¸€ä¸ªç»“æœï¼ˆå› ä¸ºæˆ‘ä»¬ä¼ å…¥å•æ¡ textï¼‰
            sampling_rate, wav_data = results[0]
            if wav_data.ndim == 2:
                wav = wav_data[:, 0]
            else:
                wav = wav_data

            wav_float = wav.astype(np.float32) / 32767.0
            audio_sample = AudioSample(samples=tuple(float(s) for s in wav_float), sample_rate=sampling_rate)

            # æ‰“å°å®é™…æ—¶é•¿ç”¨äºè°ƒè¯•ä¸äºŒæ¬¡è°ƒæ•´å‚è€ƒ
            actual_dur = len(audio_sample.samples) / audio_sample.sample_rate
            if tgt_sec is not None:
                diff = actual_dur - tgt_sec
                print(f"    â± å®é™…æ—¶é•¿: {actual_dur:.3f}s (ç›®æ ‡ {tgt_sec:.3f}s, åå·® {diff:+.3f}s)")
            else:
                print(f"    â± å®é™…æ—¶é•¿: {actual_dur:.3f}s")

            all_audio_samples.append(audio_sample)

        # ç»Ÿè®¡ä¸ç¼“å­˜æ¸…ç†
        iter_time = time.perf_counter() - batch_start_time
        if torch.cuda.is_available():
            peak_memory = torch.cuda.max_memory_allocated() / 1e9
            self.stats["peak_memory_gb"] = max(self.stats["peak_memory_gb"], peak_memory)
            print(f" âœ“ {iter_time:.2f}s (GPU: {peak_memory:.1f}GB)")
        else:
            print(f" âœ“ {iter_time:.2f}s")

        # æ›´æ–°ç»Ÿè®¡
        total_time = time.perf_counter() - batch_start_time
        self.stats["total_texts"] += total_texts
        self.stats["total_batches"] += 1
        self.stats["total_time"] += total_time

        avg_time = total_time / max(1, total_texts)
        print(f"  âœ… å®Œæˆ: {total_texts} ä¸ªç‰‡æ®µ, æ€»è€—æ—¶ {total_time:.2f}s (å¹³å‡ {avg_time:.3f}s/ç‰‡æ®µ)")

        return tuple(all_audio_samples)

    def update_config(self, temperature: float = None, top_p: float = None, speed: float = None, length_penalty: float = None) -> None :
        """æ›´æ–°é…ç½®å‚æ•°"""
        if temperature is not None and temperature > 0:
            self.temperature = temperature
            self.gpt_config['temperature'] = temperature
        if top_p is not None and 0 < top_p <= 1:
            self.top_p = top_p
            self.gpt_config['top_p'] = top_p
        if speed is not None and speed > 0:
            self.speed = speed
        if length_penalty is not None:
            self.gpt_config['length_penalty'] = length_penalty

