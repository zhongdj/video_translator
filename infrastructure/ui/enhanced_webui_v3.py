"""
Infrastructure Layer - å¢å¼º WebUI V3

æ–°å¢åŠŸèƒ½:
1. âœ… å¤šè¯´è¯äººè¯­éŸ³åˆæˆæ”¯æŒ
2. âœ… ä»…å­—å¹•æ¨¡å¼ï¼ˆä¸ç”Ÿæˆè¯­éŸ³ï¼‰
"""

from pathlib import Path
from typing import Optional, Dict, List

import gradio as gr

from application.use_cases.incremental_voice_cloning import (
    incremental_voice_cloning_use_case,
    regenerate_modified_segments_use_case
)
from application.use_cases.multi_speaker_voice_cloning import (
    multi_speaker_voice_cloning_use_case
)
from application.use_cases.subtitle_only_synthesis import (
    subtitle_only_synthesis_use_case
)
from domain.entities import (
    Video, Subtitle, LanguageCode,
    AudioSegment, SegmentReviewStatus
)
from domain.multi_speaker import (
    SpeakerId, MultiSpeakerVoiceProfile,
    SegmentSpeakerAssignment, MultiSpeakerConfig
)
from infrastructure.config.dependency_injection import container

# åˆå§‹åŒ–ä»“å‚¨
audio_segment_repo = container.audio_segment_repo
audio_file_repo = container.audio_file_repo
cache_service = container.cache_service


# ============== ä¼šè¯çŠ¶æ€ ============== #

class TranslationSessionV3:
    """å¢å¼ºç¿»è¯‘ä¼šè¯çŠ¶æ€"""

    def __init__(self):
        # åŸæœ‰å­—æ®µ
        self.translation_context = None
        self.video: Optional[Video] = None
        self.original_subtitle: Optional[Subtitle] = None
        self.translated_subtitle: Optional[Subtitle] = None
        self.english_subtitle: Optional[Subtitle] = None
        self.detected_language: Optional[LanguageCode] = None
        self.source_language: Optional[LanguageCode] = None
        self.quality_report = None
        self.audio_segments: Dict[int, AudioSegment] = {}
        self.segment_review_status: Dict[int, SegmentReviewStatus] = {}
        self.edited_segments: Dict[int, str] = {}
        self.modified_indices: set[int] = set()
        self.reference_audio_path: Optional[Path] = None
        self.approved = False
        self.length_penalty: float = 0.0
        self.duration_stats: Dict[int, dict] = {}

        # âœ… æ–°å¢ï¼šå¤šè¯´è¯äººæ”¯æŒ
        self.synthesis_mode: str = "single_speaker"  # "single_speaker" | "multi_speaker" | "subtitle_only"
        self.speaker_profiles: Dict[str, MultiSpeakerVoiceProfile] = {}  # speaker_id -> profile
        self.segment_speaker_map: Dict[int, str] = {}  # segment_index -> speaker_id
        self.default_speaker_id: Optional[str] = None


current_session = TranslationSessionV3()


# ============== å¯¼å…¥åŸæœ‰è¾…åŠ©å‡½æ•° ============== #

def _source_language_cache_format(source_language: str) -> Optional[LanguageCode]:
    """è½¬æ¢æºè¯­è¨€æ ¼å¼"""
    return LanguageCode(source_language) if source_language != "auto" else None


def _apply_edits_to_subtitle_v2():
    """åº”ç”¨ç¼–è¾‘åˆ°å­—å¹•å¯¹è±¡"""
    if not current_session.edited_segments:
        return

    from domain.entities import TextSegment

    new_segments = []
    for idx, seg in enumerate(current_session.translated_subtitle.segments):
        if idx in current_session.edited_segments:
            new_seg = TextSegment(
                text=current_session.edited_segments[idx],
                time_range=seg.time_range,
                language=seg.language
            )
            new_segments.append(new_seg)
        else:
            new_segments.append(seg)

    current_session.translated_subtitle = Subtitle(
        segments=tuple(new_segments),
        language=current_session.translated_subtitle.language
    )


def get_video_duration(video_path: Path) -> float:
    """è·å–è§†é¢‘æ—¶é•¿"""
    import subprocess
    result = subprocess.run([
        'ffprobe', '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        str(video_path)
    ], capture_output=True, text=True)
    return float(result.stdout.strip())


def _prepare_review_data_v3(filter_over_limit: bool = False):
    """
    å‡†å¤‡å®¡æ ¸æ•°æ®ï¼ˆV3å¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šè¯´è¯äººï¼‰
    """
    if not current_session.translated_subtitle:
        return None

    data = []
    for idx, (orig_seg, trans_seg) in enumerate(
            zip(current_session.original_subtitle.segments,
                current_session.translated_subtitle.segments)
    ):
        # è·å–è‹±æ–‡å­—å¹•
        en_text = (
            current_session.english_subtitle.segments[idx].text
            if current_session.english_subtitle
               and idx < len(current_session.english_subtitle.segments)
            else orig_seg.text
        )

        # è·å–è¯´è¯äººä¿¡æ¯
        speaker_info = "é»˜è®¤"
        if current_session.synthesis_mode == "multi_speaker":
            speaker_id = current_session.segment_speaker_map.get(
                idx,
                current_session.default_speaker_id
            )
            if speaker_id and speaker_id in current_session.speaker_profiles:
                speaker_info = current_session.speaker_profiles[speaker_id].speaker_id.name

        # è·å–éŸ³é¢‘ä¿¡æ¯
        audio_seg = current_session.audio_segments.get(idx)
        target_duration = trans_seg.time_range.duration

        if audio_seg:
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            duration_error = actual_duration - target_duration
            duration_ratio = (actual_duration / target_duration * 100) if target_duration > 0 else 0

            audio_status = "âœ… å·²ç”Ÿæˆ"
            duration_str = f"{actual_duration:.2f}s"

            if duration_error > 0.5:
                duration_status = f"âš ï¸ è¶…æ—¶ {duration_error:.2f}s ({duration_ratio:.0f}%)"
            elif duration_error > 0.1:
                duration_status = f"âš¡ ç•¥è¶… {duration_error:.2f}s ({duration_ratio:.0f}%)"
            elif duration_error < -0.5:
                duration_status = f"ğŸ“‰ è¿‡çŸ­ {duration_error:.2f}s ({duration_ratio:.0f}%)"
            else:
                duration_status = f"âœ… æ­£å¸¸ ({duration_ratio:.0f}%)"

            if filter_over_limit and duration_error <= 0.1:
                continue
        else:
            audio_status = "æœªç”Ÿæˆ" if current_session.synthesis_mode != "subtitle_only" else "N/A"
            duration_str = "-"
            duration_status = "â³ å¾…ç”Ÿæˆ" if current_session.synthesis_mode != "subtitle_only" else "N/A"

            if filter_over_limit:
                continue

        data.append([
            idx,
            f"{trans_seg.time_range.start_seconds:.2f}s",
            speaker_info,  # âœ… æ–°å¢åˆ—ï¼šè¯´è¯äºº
            en_text,
            trans_seg.text,
            f"{target_duration:.2f}s",
            duration_str,
            duration_status,
            audio_status,
            "â³ å¾…å®¡æ ¸"
        ])

    return data


# ============== æ­¥éª¤1: ç”Ÿæˆå­—å¹•ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰============== #

def step1_generate_and_check_v3(
        video_file,
        whisper_model: str,
        translation_model: str,
        translation_context_name: str,
        source_language: str,
        progress=gr.Progress()
):
    """æ­¥éª¤1: ç”Ÿæˆå­—å¹•ï¼ˆå¤ç”¨V2é€»è¾‘ï¼‰"""
    if not video_file:
        return None, "âŒ è¯·ä¸Šä¼ è§†é¢‘", gr.update(visible=False)

    try:
        global current_session
        current_session = TranslationSessionV3()

        video_path = Path(video_file.name)
        current_session.video = Video(
            path=video_path,
            duration=get_video_duration(video_path),
            has_audio=True
        )

        translation_context = container.translator_context_repo.load(
            translation_context_name
        )

        src_lang = _source_language_cache_format(source_language)

        progress(0.1, "æ£€æŸ¥ç¼“å­˜...")
        cached_result = cache_service.load_subtitle_cache(
            video_path=video_path,
            source_language=src_lang,
            context_domain=translation_context.domain if translation_context else None
        )

        if cached_result:
            current_session.original_subtitle = cached_result["original_subtitle"]
            current_session.translated_subtitle = cached_result["chinese_subtitle"]
            current_session.english_subtitle = cached_result["english_subtitle"]
            current_session.detected_language = cached_result["detected_language"]
            current_session.source_language = src_lang
            current_session.translation_context = translation_context

            status_report = f"""
âœ… å­—å¹•ç¼“å­˜å‘½ä¸­

ğŸ“Š åŸºæœ¬ä¿¡æ¯:
   è§†é¢‘: {video_path.name}
   æ£€æµ‹è¯­è¨€: {cached_result['detected_language'].value}
   æ€»ç‰‡æ®µæ•°: {len(cached_result['chinese_subtitle'].segments)}
"""
            review_data = _prepare_review_data_v3()
            return review_data, status_report, gr.update(visible=True)

        progress(0.2, "ç”Ÿæˆå­—å¹•...")

        from application.use_cases.improved_generate_subtitles import improved_generate_subtitles_use_case

        result = improved_generate_subtitles_use_case(
            video=current_session.video,
            asr_provider=container.get_asr(whisper_model),
            translation_provider=container.get_translator(),
            video_processor=container.video_processor,
            cache_repo=container.cache_repo,
            translation_context=translation_context,
            target_language=LanguageCode.CHINESE,
            source_language=src_lang,
            enable_quality_check=True,
            progress=lambda p, d: progress(p, d)
        )

        container.get_translator().unload()

        current_session.original_subtitle = result.original_subtitle
        current_session.translated_subtitle = result.translated_subtitle
        current_session.detected_language = result.detected_language
        current_session.translation_context = translation_context
        current_session.source_language = src_lang

        cached_result = cache_service.load_subtitle_cache(
            video_path=video_path,
            source_language=src_lang,
            context_domain=translation_context.domain if translation_context else None
        )

        if cached_result and cached_result["english_subtitle"]:
            current_session.english_subtitle = cached_result["english_subtitle"]

        status_report = f"""
âœ… å­—å¹•ç”Ÿæˆå®Œæˆ

ğŸ“Š åŸºæœ¬ä¿¡æ¯:
   è§†é¢‘: {video_path.name}
   æ£€æµ‹è¯­è¨€: {result.detected_language.value}
   æ€»ç‰‡æ®µæ•°: {len(result.translated_subtitle.segments)}
"""

        review_data = _prepare_review_data_v3()
        return review_data, status_report, gr.update(visible=True)

    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, error_msg, gr.update(visible=False)


# ============== âœ… æ–°å¢ï¼šå¤šè¯´è¯äººç®¡ç† ============== #

def add_speaker_profile(
        speaker_name: str,
        reference_audio_file,
        ref_duration: float
):
    """æ·»åŠ è¯´è¯äººé…ç½®"""
    global current_session

    if not speaker_name or not speaker_name.strip():
        return "âŒ è¯·è¾“å…¥è¯´è¯äººåç§°", gr.update()

    if not reference_audio_file:
        return "âŒ è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘", gr.update()

    try:
        # ç”Ÿæˆå”¯ä¸€ID
        speaker_id_str = f"speaker_{len(current_session.speaker_profiles) + 1}"
        speaker_id = SpeakerId(id=speaker_id_str, name=speaker_name)

        # æŒä¹…åŒ–å‚è€ƒéŸ³é¢‘
        ref_audio_path = audio_file_repo.save_reference_audio(
            video_path=current_session.video.path,
            source_audio_path=Path(reference_audio_file.name)
        )

        # åˆ›å»ºé…ç½®
        profile = MultiSpeakerVoiceProfile(
            speaker_id=speaker_id,
            reference_audio_path=ref_audio_path,
            language=LanguageCode.CHINESE,
            duration=ref_duration
        )

        current_session.speaker_profiles[speaker_id_str] = profile

        # è®¾ç½®ç¬¬ä¸€ä¸ªä¸ºé»˜è®¤è¯´è¯äºº
        if not current_session.default_speaker_id:
            current_session.default_speaker_id = speaker_id_str

        # æ›´æ–°è¯´è¯äººåˆ—è¡¨æ˜¾ç¤º
        speaker_list = "\n".join([
            f"- {p.speaker_id.name} ({p.speaker_id.id})" +
            (" [é»˜è®¤]" if sid == current_session.default_speaker_id else "")
            for sid, p in current_session.speaker_profiles.items()
        ])

        return (
            f"âœ… å·²æ·»åŠ è¯´è¯äºº: {speaker_name}\n\nå½“å‰è¯´è¯äººåˆ—è¡¨:\n{speaker_list}",
            gr.update(
                choices=list(current_session.speaker_profiles.keys()),
                value=speaker_id_str
            )
        )

    except Exception as e:
        import traceback
        return f"âŒ æ·»åŠ å¤±è´¥: {str(e)}\n{traceback.format_exc()}", gr.update()


def assign_speaker_to_segments(
        segment_indices_str: str,
        speaker_id: str
):
    """ä¸ºç‰‡æ®µåˆ†é…è¯´è¯äºº"""
    global current_session

    if not segment_indices_str or not speaker_id:
        return "âŒ è¯·è¾“å…¥ç‰‡æ®µç´¢å¼•å’Œé€‰æ‹©è¯´è¯äºº"

    try:
        # è§£æç‰‡æ®µç´¢å¼•ï¼ˆæ”¯æŒ "1,2,3" æˆ– "1-5"ï¼‰
        indices = []
        for part in segment_indices_str.split(','):
            part = part.strip()
            if '-' in part:
                start, end = map(int, part.split('-'))
                indices.extend(range(start, end + 1))
            else:
                indices.append(int(part))

        # åˆ†é…è¯´è¯äºº
        for idx in indices:
            if 0 <= idx < len(current_session.translated_subtitle.segments):
                current_session.segment_speaker_map[idx] = speaker_id

        # æ›´æ–°å®¡æ ¸è¡¨æ ¼
        updated_data = _prepare_review_data_v3()

        speaker_name = current_session.speaker_profiles[speaker_id].speaker_id.name

        return (
            f"âœ… å·²ä¸º {len(indices)} ä¸ªç‰‡æ®µåˆ†é…è¯´è¯äºº: {speaker_name}",
            gr.update(value=updated_data)
        )

    except Exception as e:
        return f"âŒ åˆ†é…å¤±è´¥: {str(e)}", gr.update()


# ============== æ­¥éª¤2: è¯­éŸ³åˆæˆï¼ˆæ”¯æŒå¤šæ¨¡å¼ï¼‰============== #

def step2_voice_synthesis_multi_mode(
        synthesis_mode: str,
        reference_audio_file,
        ref_audio_duration: float,
        ref_audio_start_offset: float,
        length_penalty: float,
        progress=gr.Progress()
):
    """
    æ­¥éª¤2: è¯­éŸ³åˆæˆï¼ˆå¤šæ¨¡å¼æ”¯æŒï¼‰

    æ¨¡å¼:
    - single_speaker: å•è¯´è¯äºº
    - multi_speaker: å¤šè¯´è¯äºº
    - subtitle_only: ä»…å­—å¹•ï¼ˆè·³è¿‡è¯­éŸ³åˆæˆï¼‰
    """
    global current_session

    if not current_session.video or not current_session.translated_subtitle:
        return "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±", gr.update(), ""

    current_session.synthesis_mode = synthesis_mode

    # âœ… æ¨¡å¼1: ä»…å­—å¹•æ¨¡å¼ï¼ˆè·³è¿‡è¯­éŸ³åˆæˆï¼‰
    if synthesis_mode == "subtitle_only":
        progress(1.0, "ä»…å­—å¹•æ¨¡å¼ï¼šè·³è¿‡è¯­éŸ³åˆæˆ")

        status = """
âœ… ä»…å­—å¹•æ¨¡å¼

ğŸ“‹ æç¤º:
   - å·²è·³è¿‡è¯­éŸ³åˆæˆæ­¥éª¤
   - å¯ç›´æ¥è¿›å…¥æ­¥éª¤3ç”Ÿæˆå­—å¹•æ–‡ä»¶
   - ä¸ä¼šç”Ÿæˆé…éŸ³è§†é¢‘
"""

        updated_data = _prepare_review_data_v3()
        return status, gr.update(value=updated_data), ""

    # âœ… æ¨¡å¼2: å•è¯´è¯äººæ¨¡å¼
    if synthesis_mode == "single_speaker":
        try:
            # å‡†å¤‡å‚è€ƒéŸ³é¢‘
            if reference_audio_file:
                ref_audio_path = audio_file_repo.save_reference_audio(
                    video_path=current_session.video.path,
                    source_audio_path=Path(reference_audio_file.name)
                )
                current_session.reference_audio_path = ref_audio_path
            else:
                existing_ref_audio = audio_file_repo.load_reference_audio(
                    current_session.video.path
                )
                if existing_ref_audio and existing_ref_audio.exists():
                    ref_audio_path = existing_ref_audio
                else:
                    temp_ref_audio = container.video_processor.extract_reference_audio(
                        video=current_session.video,
                        duration=ref_audio_duration,
                        start_offset=ref_audio_start_offset
                    )
                    ref_audio_path = audio_file_repo.save_reference_audio(
                        video_path=current_session.video.path,
                        source_audio_path=temp_ref_audio
                    )
                    if temp_ref_audio.exists():
                        temp_ref_audio.unlink()

                current_session.reference_audio_path = ref_audio_path

            # æ›´æ–°TTSé…ç½®
            tts = container.get_tts()
            if hasattr(tts, 'update_config'):
                tts.update_config(length_penalty=length_penalty)

            # æ‰§è¡Œå•è¯´è¯äººåˆæˆ
            def segment_progress(ratio, msg, idx, audio_seg):
                progress(ratio, msg)
                if audio_seg:
                    current_session.audio_segments[idx] = audio_seg

            result = incremental_voice_cloning_use_case(
                video=current_session.video,
                subtitle=current_session.translated_subtitle,
                tts_provider=container.get_tts(),
                video_processor=container.video_processor,
                audio_repo=audio_segment_repo,
                cache_repo=container.cache_repo,
                reference_audio_path=ref_audio_path,
                progress=segment_progress
            )

            for audio_seg in result.audio_segments:
                current_session.audio_segments[audio_seg.segment_index] = audio_seg

            status = f"""
âœ… å•è¯´è¯äººè¯­éŸ³å…‹éš†å®Œæˆ!

ğŸ“Š ç»Ÿè®¡:
   æ€»ç‰‡æ®µ: {result.total_segments}
   ç¼“å­˜å‘½ä¸­: {result.cached_segments}
   æ–°ç”Ÿæˆ: {result.regenerated_segments}
   è€—æ—¶: {result.synthesis_time:.1f}ç§’

âš™ï¸ é…ç½®:
   length_penalty: {length_penalty}
   å‚è€ƒéŸ³é¢‘: {ref_audio_path.name}
"""

            updated_data = _prepare_review_data_v3()
            return status, gr.update(value=updated_data), ""

        except Exception as e:
            import traceback
            return f"âŒ åˆæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}", gr.update(), ""

    # âœ… æ¨¡å¼3: å¤šè¯´è¯äººæ¨¡å¼
    if synthesis_mode == "multi_speaker":
        if not current_session.speaker_profiles:
            return "âŒ è¯·å…ˆæ·»åŠ è‡³å°‘ä¸€ä¸ªè¯´è¯äººé…ç½®", gr.update(), ""

        if not current_session.default_speaker_id:
            return "âŒ è¯·è®¾ç½®é»˜è®¤è¯´è¯äºº", gr.update(), ""

        try:
            # æ„å»ºå¤šè¯´è¯äººé…ç½®
            voice_profiles = tuple(current_session.speaker_profiles.values())

            assignments = tuple(
                SegmentSpeakerAssignment(
                    segment_index=idx,
                    speaker_id=profile.speaker_id
                )
                for idx, speaker_id_str in current_session.segment_speaker_map.items()
                for profile in current_session.speaker_profiles.values()
                if profile.speaker_id.id == speaker_id_str
            )

            default_speaker = current_session.speaker_profiles[
                current_session.default_speaker_id
            ].speaker_id

            multi_speaker_config = MultiSpeakerConfig(
                voice_profiles=voice_profiles,
                segment_assignments=assignments,
                default_speaker_id=default_speaker
            )

            # æ›´æ–°TTSé…ç½®
            tts = container.get_tts()
            if hasattr(tts, 'update_config'):
                tts.update_config(length_penalty=length_penalty)

            # æ‰§è¡Œå¤šè¯´è¯äººåˆæˆ
            def segment_progress(ratio, msg, idx, audio_seg):
                progress(ratio, msg)
                if audio_seg:
                    current_session.audio_segments[idx] = audio_seg

            result = multi_speaker_voice_cloning_use_case(
                video=current_session.video,
                subtitle=current_session.translated_subtitle,
                multi_speaker_config=multi_speaker_config,
                tts_provider=container.get_tts(),
                video_processor=container.video_processor,
                audio_repo=audio_segment_repo,
                cache_repo=container.cache_repo,
                progress=segment_progress
            )

            for audio_seg in result.audio_segments:
                current_session.audio_segments[audio_seg.segment_index] = audio_seg

            status = f"""
âœ… å¤šè¯´è¯äººè¯­éŸ³å…‹éš†å®Œæˆ!

ğŸ“Š ç»Ÿè®¡:
   æ€»ç‰‡æ®µ: {result.total_segments}
   è¯´è¯äººæ•°: {len(current_session.speaker_profiles)}
   ç¼“å­˜å‘½ä¸­: {result.cached_segments}
   æ–°ç”Ÿæˆ: {result.regenerated_segments}
   è€—æ—¶: {result.synthesis_time:.1f}ç§’

âš™ï¸ é…ç½®:
   length_penalty: {length_penalty}
"""

            updated_data = _prepare_review_data_v3()
            return status, gr.update(value=updated_data), ""

        except Exception as e:
            import traceback
            return f"âŒ åˆæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}", gr.update(), ""


# ============== æ­¥éª¤3: æœ€ç»ˆåˆæˆï¼ˆæ”¯æŒä»…å­—å¹•æ¨¡å¼ï¼‰============== #

def step3_final_synthesis_v3(
        enable_bilingual: bool,
        burn_subtitles: bool,
        progress=gr.Progress()
):
    """æ­¥éª¤3: æœ€ç»ˆåˆæˆï¼ˆæ”¯æŒä»…å­—å¹•æ¨¡å¼ï¼‰"""
    global current_session

    if not current_session.video:
        return None, None, None, "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±"

    try:
        output_dir = current_session.video.path.parent / "output"
        output_dir.mkdir(exist_ok=True)

        # âœ… ä»…å­—å¹•æ¨¡å¼
        if current_session.synthesis_mode == "subtitle_only":
            progress(0.5, "ä»…å­—å¹•æ¨¡å¼ï¼šç”Ÿæˆå­—å¹•æ–‡ä»¶...")

            output_paths, status = subtitle_only_synthesis_use_case(
                video=current_session.video,
                target_subtitle=current_session.translated_subtitle,
                secondary_subtitle=current_session.english_subtitle,
                video_processor=container.video_processor,
                subtitle_writer=container.subtitle_writer,
                output_dir=output_dir,
                enable_bilingual=enable_bilingual,
                burn_subtitles=burn_subtitles,
                formats=("srt", "ass"),
                progress=lambda p, d: progress(p, d)
            )

            # æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶
            zh_srt = next((str(p) for p in output_paths if 'zh.srt' in p.name), None)
            zh_en_ass = next((str(p) for p in output_paths if 'zh_en' in p.name), None) if enable_bilingual else None
            subtitled_video = next((str(p) for p in output_paths if p.suffix == '.mp4'),
                                   None) if burn_subtitles else None

            return zh_srt, zh_en_ass, subtitled_video, status

        # âœ… è¯­éŸ³åˆæˆæ¨¡å¼ï¼ˆå•è¯´è¯äºº/å¤šè¯´è¯äººï¼‰
        total_segments = len(current_session.translated_subtitle.segments)
        audio_ready = len(current_session.audio_segments)

        if audio_ready < total_segments * 0.7:
            return None, None, None, f"âš ï¸ éŸ³é¢‘ç‰‡æ®µä¸è¶³ï¼ˆ{audio_ready}/{total_segments}ï¼‰ï¼Œè¯·å…ˆå®Œæˆæ­¥éª¤2"

        progress(0.2, "åˆå¹¶éŸ³é¢‘ç‰‡æ®µ...")

        from domain.entities import AudioTrack
        from domain.value_objects import AudioSample

        sample_rate = list(current_session.audio_segments.values())[0].audio.sample_rate
        total_samples = int(current_session.video.duration * sample_rate)
        buffer = [0.0] * total_samples

        for idx, audio_seg in current_session.audio_segments.items():
            text_seg = audio_seg.text_segment
            start_idx = int(text_seg.time_range.start_seconds * sample_rate)

            for i, sample in enumerate(audio_seg.audio.samples):
                target_idx = start_idx + i
                if target_idx < total_samples:
                    buffer[target_idx] = sample

        full_audio = AudioSample(
            samples=tuple(buffer),
            sample_rate=sample_rate
        )

        audio_track = AudioTrack(full_audio, current_session.translated_subtitle.language)

        progress(0.5, "åˆæˆè§†é¢‘...")

        from application.use_cases.synthesize_video_use_case import synthesize_video_use_case
        from domain.services import merge_bilingual_subtitles

        if enable_bilingual and current_session.english_subtitle:
            zh_en_subtitle = merge_bilingual_subtitles(
                current_session.translated_subtitle,
                current_session.english_subtitle
            )
            subtitles_tuple = (
                current_session.translated_subtitle,
                current_session.english_subtitle,
                zh_en_subtitle
            )
            subtitle_mode = "åŒè¯­"
        else:
            subtitles_tuple = (current_session.translated_subtitle,)
            subtitle_mode = "å•è¯­"

        synthesis_result = synthesize_video_use_case(
            video=current_session.video,
            subtitles=subtitles_tuple,
            audio_track=audio_track,
            video_processor=container.video_processor,
            subtitle_writer=container.subtitle_writer,
            output_dir=output_dir,
            formats=("srt", "ass"),
            burn_subtitles=burn_subtitles,
            progress=lambda p, d: progress(0.5 + p * 0.5, d)
        )

        def find_file(patterns: list[str], suffix: str = None) -> Optional[str]:
            for pattern in patterns:
                matches = [
                    p for p in synthesis_result.output_paths
                    if pattern in p.name and (suffix is None or p.suffix == suffix)
                ]
                if matches:
                    return str(matches[0])
            return None

        zh_srt = find_file(['zh.srt'], '.srt')
        zh_en_ass = find_file(['zh_en'], '.ass') if enable_bilingual else None
        voiced_video = find_file(['_voiced_subtitled.mp4']) if burn_subtitles else find_file(['_voiced.mp4'])

        synthesis_mode_name = "å¤šè¯´è¯äºº" if current_session.synthesis_mode == "multi_speaker" else "å•è¯´è¯äºº"

        status = f"""
âœ… æœ€ç»ˆåˆæˆå®Œæˆ!

ğŸ“¦ è¾“å‡ºæ–‡ä»¶:
   - ä¸­æ–‡å­—å¹•: {zh_srt.split('/')[-1] if zh_srt else 'âŒ'}
   - åŒè¯­å­—å¹•: {zh_en_ass.split('/')[-1] if zh_en_ass else 'æœªå¯ç”¨'}
   - é…éŸ³è§†é¢‘: {voiced_video.split('/')[-1] if voiced_video else 'âŒ'}

âš™ï¸ é…ç½®:
   åˆæˆæ¨¡å¼: {synthesis_mode_name}
   å­—å¹•æ¨¡å¼: {subtitle_mode}
   çƒ§å½•å­—å¹•: {'æ˜¯' if burn_subtitles else 'å¦'}
   å¤„ç†æ—¶é—´: {synthesis_result.processing_time:.1f}ç§’

ğŸ“Š ç»Ÿè®¡:
   æ€»ç‰‡æ®µ: {len(current_session.audio_segments)}
   ä½¿ç”¨ç¼“å­˜: {audio_ready - len(current_session.modified_indices)}
   é‡æ–°ç”Ÿæˆ: {len(current_session.modified_indices) if hasattr(current_session, 'modified_indices') else 0}
"""

        return zh_srt, zh_en_ass, voiced_video, status

    except Exception as e:
        import traceback
        error_msg = f"âŒ åˆæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, None, None, error_msg


# ============== ç‰‡æ®µé¢„è§ˆï¼ˆå¤ç”¨V2é€»è¾‘ï¼‰============== #

def preview_segment(evt: gr.SelectData):
    """é¢„è§ˆé€‰ä¸­çš„ç‰‡æ®µ"""
    global current_session

    if evt is None or not current_session.video or not current_session.translated_subtitle:
        return None, "âš ï¸ æ— æ•ˆçš„ä¼šè¯çŠ¶æ€", "", ""

    try:
        if isinstance(evt.index, (tuple, list)):
            selected_row_index = int(evt.index[0])
        else:
            selected_row_index = int(evt.index)

        total_segments = len(current_session.translated_subtitle.segments)
        if selected_row_index < 0 or selected_row_index >= total_segments:
            return None, f"âŒ æ— æ•ˆçš„ç‰‡æ®µç´¢å¼•: {selected_row_index}", "", ""

        idx = selected_row_index
        text_seg = current_session.translated_subtitle.segments[idx]
        max_duration = text_seg.time_range.duration

        audio_seg = current_session.audio_segments.get(idx)
        if not audio_seg and current_session.synthesis_mode != "subtitle_only":
            audio_seg = audio_segment_repo.load_segment(
                segment_index=idx,
                video_path=current_session.video.path,
                text_segment=text_seg
            )
            if audio_seg:
                current_session.audio_segments[idx] = audio_seg

        # è·å–è¯´è¯äººä¿¡æ¯
        speaker_info = ""
        if current_session.synthesis_mode == "multi_speaker":
            speaker_id = current_session.segment_speaker_map.get(
                idx,
                current_session.default_speaker_id
            )
            if speaker_id and speaker_id in current_session.speaker_profiles:
                speaker_name = current_session.speaker_profiles[speaker_id].speaker_id.name
                speaker_info = f"\nğŸ¤ è¯´è¯äºº: {speaker_name}"

        actual_duration = None
        if audio_seg and audio_seg.file_path and audio_seg.file_path.exists():
            audio_path = str(audio_seg.file_path)
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            duration_diff = actual_duration - max_duration
            diff_sign = "+" if duration_diff > 0 else ""
            audio_status = f"âœ… éŸ³é¢‘å·²ç”Ÿæˆ ({(actual_duration / max_duration * 100):.1f}%)"
        else:
            audio_path = None
            if current_session.synthesis_mode == "subtitle_only":
                audio_status = "N/A (ä»…å­—å¹•æ¨¡å¼)"
            else:
                audio_status = "âš ï¸ éŸ³é¢‘æœªç”Ÿæˆ"

        if actual_duration:
            duration_diff = actual_duration - max_duration
            diff_sign = "+" if duration_diff > 0 else ""
            text_info = f"""
ç‰‡æ®µ #{idx}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  æ—¶é—´è½´: {text_seg.time_range.start_seconds:.2f}s - {text_seg.time_range.end_seconds:.2f}s{speaker_info}

ğŸ“ æ—¶é•¿ä¿¡æ¯:
   â€¢ æœ€å¤§å…è®¸: {max_duration:.2f}s
   â€¢ å®é™…ç”Ÿæˆ: {actual_duration:.2f}s
   â€¢ å·®å¼‚: {diff_sign}{duration_diff:.2f}s ({diff_sign}{(duration_diff / max_duration * 100):.1f}%)

ğŸ“Š çŠ¶æ€: {'âœ… æ­£å¸¸' if abs(duration_diff) < 0.5 else 'âš ï¸ åå·®è¾ƒå¤§'}
"""
        else:
            text_info = f"""
ç‰‡æ®µ #{idx}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  æ—¶é—´è½´: {text_seg.time_range.start_seconds:.2f}s - {text_seg.time_range.end_seconds:.2f}s{speaker_info}

ğŸ“ æ—¶é•¿ä¿¡æ¯:
   â€¢ æœ€å¤§å…è®¸: {max_duration:.2f}s
   â€¢ å®é™…ç”Ÿæˆ: {'æœªç”Ÿæˆ' if current_session.synthesis_mode != 'subtitle_only' else 'N/A'}
"""

        subtitle_text = text_seg.text
        return audio_path, audio_status, text_info, subtitle_text

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ é¢„è§ˆå¤±è´¥: {e}", "", ""


# ============== UI æ„å»º ============== #

def build_ui_v3():
    """æ„å»ºå¢å¼º UI V3"""

    with gr.Blocks(
            title="è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V3",
            css="""
        .gradio-container {max-width: 1900px !important}
        .segment-preview {border: 1px solid #ddd; padding: 10px; border-radius: 5px;}
        .speaker-config {background: #f5f5f5; padding: 15px; border-radius: 8px; margin: 10px 0;}
        """
    ) as demo:
        gr.Markdown("""
        # ğŸ¬ è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V3 - å¤šè¯´è¯äºº & ä»…å­—å¹•æ”¯æŒ

        ## âœ¨ V3 æ–°å¢åŠŸèƒ½
        - ğŸ­ **å¤šè¯´è¯äººè¯­éŸ³åˆæˆ**: æ”¯æŒä¸ºä¸åŒç‰‡æ®µæŒ‡å®šä¸åŒè¯´è¯äºº
        - ğŸ“ **ä»…å­—å¹•æ¨¡å¼**: åªç”Ÿæˆå­—å¹•æ–‡ä»¶ï¼Œè·³è¿‡è¯­éŸ³åˆæˆ
        - ğŸ›ï¸ **çµæ´»çš„åˆæˆæ¨¡å¼**: å•è¯´è¯äºº / å¤šè¯´è¯äºº / ä»…å­—å¹• ä¸‰ç§æ¨¡å¼å¯é€‰

        ## ğŸ“‹ å·¥ä½œæµç¨‹
        1. **ç”Ÿæˆå­—å¹•** â†’ 2. **é€‰æ‹©åˆæˆæ¨¡å¼** â†’ 3. **é…ç½®å‚æ•°** â†’ 4. **æœ€ç»ˆåˆæˆ**
        """)

        with gr.Tab("ğŸ¬ å•è§†é¢‘å¤„ç† V3"):
            # ========== æ­¥éª¤1: ç”Ÿæˆå­—å¹• ========== #
            with gr.Accordion("ğŸ” æ­¥éª¤1: ç”Ÿæˆå­—å¹•", open=True):
                with gr.Row():
                    with gr.Column(scale=1):
                        video_input = gr.File(
                            label="ğŸ“¹ ä¸Šä¼ è§†é¢‘",
                            file_types=[".mp4", ".avi", ".mov", ".mkv"]
                        )

                        whisper_model = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large", "large-v3"],
                            value="medium",
                            label="ğŸ™ï¸ Whisper æ¨¡å‹"
                        )

                        translation_model = gr.Dropdown(
                            choices=["Qwen/Qwen2.5-7B"],
                            value="Qwen/Qwen2.5-7B",
                            label="ğŸŒ ç¿»è¯‘æ¨¡å‹"
                        )

                        translation_context = gr.Dropdown(
                            choices=container.translator_context_repo.list_contexts(),
                            value="general",
                            label="ğŸ“š ç¿»è¯‘ä¸Šä¸‹æ–‡"
                        )

                        source_lang = gr.Dropdown(
                            choices=["auto", "en", "zh", "pt", "ja"],
                            value="auto",
                            label="ğŸ—£ï¸ æºè¯­è¨€"
                        )

                        step1_btn = gr.Button("â–¶ï¸ ç”Ÿæˆå­—å¹•", variant="primary")

                    with gr.Column(scale=1):
                        step1_status = gr.Textbox(
                            label="ğŸ“Š ç”ŸæˆçŠ¶æ€",
                            lines=12
                        )

            # ========== æ­¥éª¤2: åˆæˆæ¨¡å¼é€‰æ‹© ========== #
            with gr.Accordion("ğŸ›ï¸ æ­¥éª¤2: é€‰æ‹©åˆæˆæ¨¡å¼", open=False) as step2_accordion:
                gr.Markdown("""
                ### é€‰æ‹©åˆæˆæ¨¡å¼

                - **å•è¯´è¯äºº**: æ‰€æœ‰ç‰‡æ®µä½¿ç”¨åŒä¸€ä¸ªå‚è€ƒéŸ³é¢‘
                - **å¤šè¯´è¯äºº**: ä¸ºä¸åŒç‰‡æ®µåˆ†é…ä¸åŒè¯´è¯äººï¼ˆå¦‚å¯¹è¯åœºæ™¯ï¼‰
                - **ä»…å­—å¹•**: åªç”Ÿæˆå­—å¹•æ–‡ä»¶ï¼Œä¸åˆæˆè¯­éŸ³
                """)

                synthesis_mode = gr.Radio(
                    choices=[
                        ("å•è¯´è¯äºº", "single_speaker"),
                        ("å¤šè¯´è¯äºº", "multi_speaker"),
                        ("ä»…å­—å¹•ï¼ˆä¸ç”Ÿæˆè¯­éŸ³ï¼‰", "subtitle_only")
                    ],
                    value="single_speaker",
                    label="åˆæˆæ¨¡å¼"
                )

                # âœ… å•è¯´è¯äººé…ç½®
                with gr.Group(visible=True) as single_speaker_config:
                    gr.Markdown("### å•è¯´è¯äººé…ç½®")

                    reference_audio = gr.File(
                        label="ğŸµ å‚è€ƒéŸ³é¢‘(å¯é€‰)",
                        file_types=[".wav", ".mp3"]
                    )

                    with gr.Row():
                        ref_duration_slider = gr.Slider(
                            minimum=5, maximum=60, value=10, step=5,
                            label="â±ï¸ å‚è€ƒéŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"
                        )
                        ref_offset_slider = gr.Slider(
                            minimum=0, maximum=120, value=0, step=5,
                            label="ğŸ“ èµ·å§‹åç§»ï¼ˆç§’ï¼‰"
                        )

                    length_penalty_slider = gr.Slider(
                        minimum=-2.0, maximum=2.0, value=0.0, step=0.1,
                        label="âš™ï¸ length_penalty"
                    )

                # âœ… å¤šè¯´è¯äººé…ç½®
                with gr.Group(visible=False) as multi_speaker_config:
                    gr.Markdown("### å¤šè¯´è¯äººé…ç½®")

                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("#### 1. æ·»åŠ è¯´è¯äºº")

                            speaker_name_input = gr.Textbox(
                                label="è¯´è¯äººåç§°",
                                placeholder="ä¾‹å¦‚: ä¸»è®²äºº, æ—ç™½, è§’è‰²A"
                            )

                            speaker_audio_input = gr.File(
                                label="å‚è€ƒéŸ³é¢‘",
                                file_types=[".wav", ".mp3"]
                            )

                            speaker_duration_input = gr.Slider(
                                minimum=5, maximum=60, value=10, step=5,
                                label="å‚è€ƒéŸ³é¢‘æ—¶é•¿"
                            )

                            add_speaker_btn = gr.Button("â• æ·»åŠ è¯´è¯äºº", variant="secondary")

                            speaker_status = gr.Textbox(
                                label="è¯´è¯äººåˆ—è¡¨",
                                lines=8,
                                placeholder="å°šæœªæ·»åŠ è¯´è¯äºº"
                            )

                        with gr.Column(scale=1):
                            gr.Markdown("#### 2. åˆ†é…ç‰‡æ®µ")

                            segment_indices_input = gr.Textbox(
                                label="ç‰‡æ®µç´¢å¼•",
                                placeholder="ä¾‹å¦‚: 0,1,2 æˆ– 0-5",
                                info="é€—å·åˆ†éš”æˆ–ä½¿ç”¨è¿å­—ç¬¦è¡¨ç¤ºèŒƒå›´"
                            )

                            speaker_selector = gr.Dropdown(
                                label="é€‰æ‹©è¯´è¯äºº",
                                choices=[],
                                interactive=True
                            )

                            assign_speaker_btn = gr.Button("âœ… åˆ†é…è¯´è¯äºº", variant="secondary")

                            assign_status = gr.Textbox(
                                label="åˆ†é…çŠ¶æ€",
                                lines=3
                            )

                            gr.Markdown("""
                            **æç¤º**:
                            - æœªåˆ†é…çš„ç‰‡æ®µå°†ä½¿ç”¨ç¬¬ä¸€ä¸ªæ·»åŠ çš„è¯´è¯äºº
                            - å¯ä»¥å¤šæ¬¡åˆ†é…ï¼Œè¦†ç›–ä¹‹å‰çš„è®¾ç½®
                            """)

                            multi_length_penalty = gr.Slider(
                                minimum=-2.0, maximum=2.0, value=0.0, step=0.1,
                                label="âš™ï¸ length_penalty"
                            )

                # âœ… ä»…å­—å¹•æç¤º
                with gr.Group(visible=False) as subtitle_only_info:
                    gr.Markdown("""
                    ### ä»…å­—å¹•æ¨¡å¼

                    âœ… å°†è·³è¿‡è¯­éŸ³åˆæˆæ­¥éª¤

                    **è¾“å‡ºå†…å®¹**:
                    - ä¸­æ–‡å­—å¹•æ–‡ä»¶ (.srt, .ass)
                    - è‹±æ–‡å­—å¹•æ–‡ä»¶ (.srt, .ass)
                    - åŒè¯­å­—å¹•æ–‡ä»¶ (.srt, .ass)
                    - çƒ§å½•å­—å¹•çš„è§†é¢‘ï¼ˆå¯é€‰ï¼‰

                    **ä¼˜åŠ¿**:
                    - å¤„ç†é€Ÿåº¦å¿«
                    - ä¸éœ€è¦GPU
                    - ä¿ç•™åŸå§‹éŸ³é¢‘
                    """)

                # å¼€å§‹åˆæˆæŒ‰é’®
                start_synthesis_btn = gr.Button(
                    "â–¶ï¸ å¼€å§‹åˆæˆ",
                    variant="primary",
                    size="lg"
                )

                synthesis_status = gr.Textbox(
                    label="åˆæˆçŠ¶æ€",
                    lines=12
                )

            # ========== å®¡æ ¸è¡¨æ ¼ ========== #
            with gr.Accordion("ğŸ“‹ æ­¥éª¤3: å®¡æ ¸å’Œé¢„è§ˆ", open=False):
                review_dataframe = gr.Dataframe(
                    headers=[
                        "ç´¢å¼•", "æ—¶é—´", "è¯´è¯äºº", "åŸæ–‡", "ç¿»è¯‘",
                        "ç›®æ ‡é•¿åº¦", "å®é™…é•¿åº¦", "æ—¶é•¿çŠ¶æ€", "éŸ³é¢‘", "å®¡æ ¸"
                    ],
                    datatype=[
                        "number", "str", "str", "str", "str",
                        "str", "str", "str", "str", "str"
                    ],
                    col_count=(10, "fixed"),
                    row_count=(10, "dynamic"),
                    interactive=True,
                    wrap=True,
                    label="å­—å¹•å®¡æ ¸è¡¨æ ¼ (ç‚¹å‡»è¡Œé¢„è§ˆ)"
                )

                # ç‰‡æ®µé¢„è§ˆåŒº
                with gr.Group():
                    gr.Markdown("### ğŸ‘‚ ç‰‡æ®µé¢„è§ˆ")

                    with gr.Row():
                        with gr.Column(scale=1):
                            preview_audio = gr.Audio(
                                label="ğŸ”Š éŸ³é¢‘æ’­æ”¾",
                                type="filepath"
                            )
                            preview_status = gr.Textbox(label="çŠ¶æ€", lines=1)

                        with gr.Column(scale=1):
                            preview_info = gr.Textbox(label="ç‰‡æ®µä¿¡æ¯", lines=5)
                            preview_text = gr.Textbox(label="å­—å¹•æ–‡æœ¬", lines=4)

            # ========== æ­¥éª¤4: æœ€ç»ˆåˆæˆ ========== #
            with gr.Accordion("ğŸ¬ æ­¥éª¤4: æœ€ç»ˆåˆæˆ", open=False):
                gr.Markdown("""
                ### è¾“å‡ºé…ç½®
                """)

                with gr.Row():
                    enable_bilingual_checkbox = gr.Checkbox(
                        label="ğŸ“ ç”ŸæˆåŒè¯­å­—å¹•",
                        value=True,
                        info="ä¸­æ–‡+è‹±æ–‡åŒè¯­å­—å¹•"
                    )

                    burn_subtitles_checkbox = gr.Checkbox(
                        label="ğŸ”¥ çƒ§å½•å­—å¹•åˆ°è§†é¢‘",
                        value=True,
                        info="å°†å­—å¹•ç¡¬ç¼–ç åˆ°è§†é¢‘ä¸­"
                    )

                final_btn = gr.Button("â–¶ï¸ ç”Ÿæˆæœ€ç»ˆè¾“å‡º", variant="primary", size="lg")
                final_status = gr.Textbox(label="åˆæˆçŠ¶æ€", lines=12)

                with gr.Row():
                    zh_srt_output = gr.File(label="ä¸­æ–‡å­—å¹•")
                    zh_en_ass_output = gr.File(label="åŒè¯­å­—å¹•")
                    final_video_output = gr.File(label="æœ€ç»ˆè§†é¢‘")

            # ========== äº‹ä»¶ç»‘å®š ========== #

            # æ­¥éª¤1: ç”Ÿæˆå­—å¹•
            step1_btn.click(
                step1_generate_and_check_v3,
                inputs=[
                    video_input, whisper_model, translation_model,
                    translation_context, source_lang
                ],
                outputs=[review_dataframe, step1_status, step2_accordion]
            ).then(
                lambda: gr.update(open=True),
                outputs=[step2_accordion]
            )

            # æ¨¡å¼åˆ‡æ¢
            def toggle_synthesis_mode(mode):
                return (
                    gr.update(visible=(mode == "single_speaker")),
                    gr.update(visible=(mode == "multi_speaker")),
                    gr.update(visible=(mode == "subtitle_only"))
                )

            synthesis_mode.change(
                toggle_synthesis_mode,
                inputs=[synthesis_mode],
                outputs=[single_speaker_config, multi_speaker_config, subtitle_only_info]
            )

            # å¤šè¯´è¯äººï¼šæ·»åŠ è¯´è¯äºº
            add_speaker_btn.click(
                add_speaker_profile,
                inputs=[speaker_name_input, speaker_audio_input, speaker_duration_input],
                outputs=[speaker_status, speaker_selector]
            )

            # å¤šè¯´è¯äººï¼šåˆ†é…ç‰‡æ®µ
            assign_speaker_btn.click(
                assign_speaker_to_segments,
                inputs=[segment_indices_input, speaker_selector],
                outputs=[assign_status, review_dataframe]
            )

            # å¼€å§‹åˆæˆï¼ˆæ ¹æ®æ¨¡å¼é€‰æ‹©å‚æ•°ï¼‰
            def dispatch_synthesis(mode, ref_audio, ref_dur, ref_offset, lp, mlp, progress=gr.Progress()):
                # æ ¹æ®æ¨¡å¼é€‰æ‹©æ­£ç¡®çš„ length_penalty
                final_lp = lp if mode != "multi_speaker" else mlp

                return step2_voice_synthesis_multi_mode(
                    synthesis_mode=mode,
                    reference_audio_file=ref_audio,
                    ref_audio_duration=ref_dur,
                    ref_audio_start_offset=ref_offset,
                    length_penalty=final_lp,
                    progress=progress
                )

            start_synthesis_btn.click(
                dispatch_synthesis,
                inputs=[
                    synthesis_mode,
                    reference_audio,
                    ref_duration_slider,
                    ref_offset_slider,
                    length_penalty_slider,
                    multi_length_penalty
                ],
                outputs=[synthesis_status, review_dataframe, synthesis_status]
            )

            # è¡¨æ ¼é€‰æ‹©äº‹ä»¶
            review_dataframe.select(
                preview_segment,
                outputs=[preview_audio, preview_status, preview_info, preview_text]
            )

            # æ­¥éª¤4: æœ€ç»ˆåˆæˆ
            final_btn.click(
                step3_final_synthesis_v3,
                inputs=[enable_bilingual_checkbox, burn_subtitles_checkbox],
                outputs=[zh_srt_output, zh_en_ass_output, final_video_output, final_status]
            )

    return demo


def main():
    """å¯åŠ¨ WebUI V3"""
    demo = build_ui_v3()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        inbrowser=True,
        share=False
    )


if __name__ == "__main__":
    main()