"""
Infrastructure Layer - é‡æ„åçš„WebUI V2ï¼ˆå®Œæ•´ç‰ˆï¼‰

âœ… éœ€æ±‚1: æ”¯æŒå¯é…ç½®çš„å‚è€ƒéŸ³é¢‘èµ·å§‹åç§»
âœ… éœ€æ±‚2: æ”¯æŒåŒè¯­å­—å¹•å¯é€‰
âœ… éœ€æ±‚3: ä½¿ç”¨AudioFileRepositoryç®¡ç†å‚è€ƒéŸ³é¢‘
"""

from pathlib import Path
from typing import Optional, Dict

import gradio as gr

from application.use_cases.incremental_voice_cloning import (
    incremental_voice_cloning_use_case,
    regenerate_modified_segments_use_case
)
from domain.entities import (
    Video, Subtitle, LanguageCode,
    AudioSegment, SegmentReviewStatus
)
from infrastructure.config.dependency_injection import container

# åˆå§‹åŒ–ä»“å‚¨
audio_segment_repo = container.audio_segment_repo
audio_file_repo = container.audio_file_repo  # âœ… ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶ä»“å‚¨
cache_service = container.cache_service


# ============== ä¼šè¯çŠ¶æ€ ============== #

class TranslationSessionV2:
    """ç¿»è¯‘ä¼šè¯çŠ¶æ€"""

    def __init__(self):
        self.translation_context = None
        self.video: Optional[Video] = None
        self.original_subtitle: Optional[Subtitle] = None
        self.translated_subtitle: Optional[Subtitle] = None
        self.english_subtitle: Optional[Subtitle] = None
        self.detected_language: Optional[LanguageCode] = None
        self.source_language: Optional[LanguageCode] = None
        self.quality_report = None
        self.audio_segments: Dict[int, AudioSegment] = {}
        self.segment_review_status: Dict[int, SegmentReviewStatus] = {}
        self.edited_segments: Dict[int, str] = {}
        self.modified_indices: set[int] = set()
        self.reference_audio_path: Optional[Path] = None
        self.approved = False


current_session = TranslationSessionV2()


# ============== è¾…åŠ©å‡½æ•° ============== #

def _load_cached_audio_segments(video: Video, subtitle: Subtitle) -> Dict[int, AudioSegment]:
    """ä»ç£ç›˜åŠ è½½å·²ç¼“å­˜çš„éŸ³é¢‘ç‰‡æ®µ"""
    cached_segments = {}

    for idx, text_seg in enumerate(subtitle.segments):
        try:
            audio_seg = audio_segment_repo.load_segment(
                segment_index=idx,
                video_path=video.path,
                text_segment=text_seg
            )
            if audio_seg:
                cached_segments[idx] = audio_seg
        except Exception:
            continue

    return cached_segments


def _source_language_cache_format(source_language: str) -> Optional[LanguageCode]:
    """è½¬æ¢æºè¯­è¨€æ ¼å¼"""
    return LanguageCode(source_language) if source_language != "auto" else None


def _prepare_review_data_v2():
    """å‡†å¤‡å®¡æ ¸æ•°æ®"""
    if not current_session.translated_subtitle:
        return None

    data = []
    for idx, (orig_seg, trans_seg) in enumerate(
        zip(current_session.original_subtitle.segments,
            current_session.translated_subtitle.segments)
    ):
        en_text = (
            current_session.english_subtitle.segments[idx].text
            if current_session.english_subtitle
               and idx < len(current_session.english_subtitle.segments)
            else orig_seg.text
        )

        audio_seg = current_session.audio_segments.get(idx)
        if audio_seg:
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            audio_status = "âœ… å·²ç¼“å­˜"
            duration_str = f"{actual_duration:.2f}s"
        else:
            audio_status = "æœªç”Ÿæˆ"
            duration_str = "-"

        data.append([
            idx,
            f"{trans_seg.time_range.start_seconds:.2f}s",
            en_text,
            trans_seg.text,
            f"{trans_seg.time_range.duration:.2f}s",
            duration_str,
            audio_status,
            "",
            "â³ å¾…å®¡æ ¸"
        ])

    return data


def _apply_edits_to_subtitle_v2():
    """åº”ç”¨ç¼–è¾‘åˆ°å­—å¹•å¯¹è±¡"""
    if not current_session.edited_segments:
        return

    from domain.entities import TextSegment

    new_segments = []
    for idx, seg in enumerate(current_session.translated_subtitle.segments):
        if idx in current_session.edited_segments:
            new_seg = TextSegment(
                text=current_session.edited_segments[idx],
                time_range=seg.time_range,
                language=seg.language
            )
            new_segments.append(new_seg)
        else:
            new_segments.append(seg)

    current_session.translated_subtitle = Subtitle(
        segments=tuple(new_segments),
        language=current_session.translated_subtitle.language
    )


def get_video_duration(video_path: Path) -> float:
    """è·å–è§†é¢‘æ—¶é•¿"""
    import subprocess
    result = subprocess.run([
        'ffprobe', '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        str(video_path)
    ], capture_output=True, text=True)
    return float(result.stdout.strip())


# ============== æ­¥éª¤1: ç”Ÿæˆå­—å¹• ============== #

def step1_generate_and_check_v2(
    video_file,
    whisper_model: str,
    translation_model: str,
    translation_context_name: str,
    source_language: str,
    progress=gr.Progress()
):
    """æ­¥éª¤1: ç”Ÿæˆå­—å¹•"""
    if not video_file:
        return None, "âŒ è¯·ä¸Šä¼ è§†é¢‘", gr.update(visible=False)

    try:
        global current_session
        current_session = TranslationSessionV2()

        video_path = Path(video_file.name)
        current_session.video = Video(
            path=video_path,
            duration=get_video_duration(video_path),
            has_audio=True
        )

        translation_context = container.translator_context_repo.load(
            translation_context_name
        )

        src_lang = _source_language_cache_format(source_language)

        progress(0.1, "æ£€æŸ¥ç¼“å­˜...")
        cached_result = cache_service.load_subtitle_cache(
            video_path=video_path,
            source_language=src_lang,
            context_domain=translation_context.domain if translation_context else None
        )

        if cached_result:
            current_session.original_subtitle = cached_result["original_subtitle"]
            current_session.translated_subtitle = cached_result["chinese_subtitle"]
            current_session.english_subtitle = cached_result["english_subtitle"]
            current_session.detected_language = cached_result["detected_language"]
            current_session.source_language = src_lang
            current_session.translation_context = translation_context

            status_report = f"""
âœ… å­—å¹•ç¼“å­˜å‘½ä¸­

ğŸ“Š åŸºæœ¬ä¿¡æ¯:
   è§†é¢‘: {video_path.name}
   æ£€æµ‹è¯­è¨€: {cached_result['detected_language'].value}
   æ€»ç‰‡æ®µæ•°: {len(cached_result['chinese_subtitle'].segments)}
"""
            review_data = _prepare_review_data_v2()
            return review_data, status_report, gr.update(visible=True)

        progress(0.2, "ç”Ÿæˆå­—å¹•...")

        from application.use_cases.improved_generate_subtitles import improved_generate_subtitles_use_case

        result = improved_generate_subtitles_use_case(
            video=current_session.video,
            asr_provider=container.get_asr(whisper_model),
            translation_provider=container.get_translator(),
            video_processor=container.video_processor,
            cache_repo=container.cache_repo,
            translation_context=translation_context,
            target_language=LanguageCode.CHINESE,
            source_language=src_lang,
            enable_quality_check=True,
            progress=lambda p, d: progress(p, d)
        )

        container.get_translator().unload()

        current_session.original_subtitle = result.original_subtitle
        current_session.translated_subtitle = result.translated_subtitle
        current_session.detected_language = result.detected_language
        current_session.translation_context = translation_context
        current_session.source_language = src_lang

        cached_result = cache_service.load_subtitle_cache(
            video_path=video_path,
            source_language=src_lang,
            context_domain=translation_context.domain if translation_context else None
        )

        if cached_result and cached_result["english_subtitle"]:
            current_session.english_subtitle = cached_result["english_subtitle"]

        status_report = f"""
âœ… å­—å¹•ç”Ÿæˆå®Œæˆ

ğŸ“Š åŸºæœ¬ä¿¡æ¯:
   è§†é¢‘: {video_path.name}
   æ£€æµ‹è¯­è¨€: {result.detected_language.value}
   æ€»ç‰‡æ®µæ•°: {len(result.translated_subtitle.segments)}
"""

        review_data = _prepare_review_data_v2()
        return review_data, status_report, gr.update(visible=True)

    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, error_msg, gr.update(visible=False)


# ============== æ­¥éª¤2A: å¢é‡è¯­éŸ³å…‹éš† ============== #

def step2_incremental_voice_cloning(
    reference_audio_file,
    ref_audio_duration: float,  # âœ… æ–°å¢å‚æ•°
    ref_audio_start_offset: float,  # âœ… æ–°å¢å‚æ•°
    progress=gr.Progress()
):
    """æ­¥éª¤2A: å¢é‡è¯­éŸ³å…‹éš†ï¼ˆé‡æ„ç‰ˆï¼‰"""
    global current_session

    if not current_session.video or not current_session.translated_subtitle:
        return "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±", gr.update()

    try:
        # âœ… ä¿®å¤: å‡†å¤‡å‚è€ƒéŸ³é¢‘ï¼ˆä½¿ç”¨AudioFileRepositoryï¼‰
        if reference_audio_file:
            # ç”¨æˆ·ä¸Šä¼ äº†å‚è€ƒéŸ³é¢‘
            progress(0.05, "ä¿å­˜å‚è€ƒéŸ³é¢‘...")

            # ä½¿ç”¨AudioFileRepositoryæŒä¹…åŒ–Gradioä¸´æ—¶æ–‡ä»¶
            ref_audio_path = audio_file_repo.save_reference_audio(
                video_path=current_session.video.path,
                source_audio_path=Path(reference_audio_file.name)
            )
            current_session.reference_audio_path = ref_audio_path
            print(f"ğŸ“ ä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„å‚è€ƒéŸ³é¢‘: {ref_audio_path}")

        else:
            # å…ˆå°è¯•åŠ è½½å·²å­˜åœ¨çš„å‚è€ƒéŸ³é¢‘
            existing_ref_audio = audio_file_repo.load_reference_audio(
                current_session.video.path
            )

            if existing_ref_audio and existing_ref_audio.exists():
                ref_audio_path = existing_ref_audio
                current_session.reference_audio_path = ref_audio_path
                print(f"ğŸ“ å¤ç”¨å·²æœ‰å‚è€ƒéŸ³é¢‘: {ref_audio_path}")
            else:
                # ä»è§†é¢‘æå–å‚è€ƒéŸ³é¢‘
                progress(0.05, f"ä»è§†é¢‘æå–å‚è€ƒéŸ³é¢‘ï¼ˆåç§»: {ref_audio_start_offset}s, æ—¶é•¿: {ref_audio_duration}sï¼‰...")

                temp_ref_audio = container.video_processor.extract_reference_audio(
                    video=current_session.video,
                    duration=ref_audio_duration,
                    start_offset=ref_audio_start_offset  # âœ… ä½¿ç”¨å¯é…ç½®çš„åç§»
                )

                # æŒä¹…åŒ–æå–çš„éŸ³é¢‘
                ref_audio_path = audio_file_repo.save_reference_audio(
                    video_path=current_session.video.path,
                    source_audio_path=temp_ref_audio
                )
                current_session.reference_audio_path = ref_audio_path

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_ref_audio.exists():
                    temp_ref_audio.unlink()

                print(f"ğŸ“ ä»è§†é¢‘æå–å‚è€ƒéŸ³é¢‘: {ref_audio_path}")

        # å®æ—¶è¿›åº¦å›è°ƒ
        def segment_progress(ratio, msg, idx, audio_seg):
            progress(ratio, msg)
            if audio_seg:
                current_session.audio_segments[idx] = audio_seg
                status = current_session.segment_review_status.get(idx)
                if status and not status.subtitle_modified:
                    current_session.segment_review_status[idx] = SegmentReviewStatus(
                        segment_index=idx,
                        subtitle_approved=False,
                        audio_approved=True,
                        subtitle_modified=False,
                        needs_regeneration=False
                    )

        # æ‰§è¡Œå¢é‡åˆæˆ
        result = incremental_voice_cloning_use_case(
            video=current_session.video,
            subtitle=current_session.translated_subtitle,
            tts_provider=container.get_tts(),
            video_processor=container.video_processor,
            audio_repo=audio_segment_repo,
            cache_repo=container.cache_repo,
            reference_audio_path=ref_audio_path,
            progress=segment_progress
        )

        # æ›´æ–°æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µåˆ°ä¼šè¯
        for audio_seg in result.audio_segments:
            current_session.audio_segments[audio_seg.segment_index] = audio_seg

        status = f"""
âœ… å¢é‡è¯­éŸ³å…‹éš†å®Œæˆ!

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   æ€»ç‰‡æ®µæ•°: {result.total_segments}
   ç¼“å­˜å‘½ä¸­: {result.cached_segments}
   æ–°ç”Ÿæˆ: {result.regenerated_segments}
   è€—æ—¶: {result.synthesis_time:.1f} ç§’

ğŸ“ å‚è€ƒéŸ³é¢‘: {ref_audio_path.name}
   èµ·å§‹åç§»: {ref_audio_start_offset}s
   æ—¶é•¿: {ref_audio_duration}s

ğŸ’¡ æç¤º: 
   - å‚è€ƒéŸ³é¢‘å·²æŒä¹…åŒ–ï¼Œä¿®æ”¹å­—å¹•åå¯å®‰å…¨é‡æ–°ç”Ÿæˆ
   - ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡ŒæŸ¥çœ‹å’Œæ’­æ”¾éŸ³é¢‘
"""

        updated_data = _prepare_review_data_v2()
        return status, gr.update(value=updated_data)

    except Exception as e:
        import traceback
        error_msg = f"âŒ è¯­éŸ³å…‹éš†å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return error_msg, gr.update()


# ============== æ­¥éª¤2B: ä¿å­˜ç¼–è¾‘ ============== #

def step2_save_edits_and_regenerate(review_dataframe):
    """ä¿å­˜ç¼–è¾‘å¹¶æ ‡è®°éœ€è¦é‡æ–°ç”Ÿæˆçš„ç‰‡æ®µ"""
    global current_session

    if hasattr(review_dataframe, "values"):
        review_dataframe = review_dataframe.values.tolist()

    if not review_dataframe:
        return "âš ï¸ æ²¡æœ‰å¯ä¿å­˜çš„ä¿®æ”¹", gr.update()

    if review_dataframe and isinstance(review_dataframe[0][0], str):
        review_dataframe = review_dataframe[1:]

    edited_count = 0

    for row in review_dataframe:
        try:
            idx = int(row[0])
        except (ValueError, IndexError):
            continue

        if idx >= len(current_session.translated_subtitle.segments):
            continue

        original_text = current_session.translated_subtitle.segments[idx].text
        edited_text = row[3]

        if edited_text != original_text:
            current_session.edited_segments[idx] = edited_text
            current_session.modified_indices.add(idx)
            edited_count += 1

    if edited_count:
        _apply_edits_to_subtitle_v2()

        cache_service.update_chinese_subtitle(
            video_path=current_session.video.path,
            updated_subtitle=current_session.translated_subtitle,
            source_language=current_session.source_language,
            context_domain=current_session.translation_context.domain
            if current_session.translation_context else None
        )

        cache_service.invalidate_downstream_caches(
            video_path=current_session.video.path,
            detected_language=current_session.detected_language
        )

        updated_data = _prepare_review_data_v2()

        return (
            f"âœ… å·²ä¿å­˜ {edited_count} å¤„ä¿®æ”¹ï¼ˆå·²åŒæ­¥åˆ°ç¼“å­˜ï¼‰\n"
            f"âš ï¸ éœ€è¦é‡æ–°ç”Ÿæˆ {len(current_session.modified_indices)} ä¸ªéŸ³é¢‘ç‰‡æ®µ",
            gr.update(value=updated_data)
        )
    else:
        return "â„¹ï¸ æœªæ£€æµ‹åˆ°ä¿®æ”¹", gr.update()


# ============== æ­¥éª¤2C: é‡æ–°ç”Ÿæˆ ============== #

def step2_regenerate_modified():
    """é‡æ–°ç”Ÿæˆä¿®æ”¹è¿‡çš„ç‰‡æ®µï¼ˆä¿®å¤ç‰ˆï¼‰"""
    global current_session

    if not current_session.modified_indices:
        return "â„¹ï¸ æ²¡æœ‰éœ€è¦é‡æ–°ç”Ÿæˆçš„ç‰‡æ®µ", gr.update()

    # âœ… ä¿®å¤: æ™ºèƒ½è·å–å‚è€ƒéŸ³é¢‘
    ref_audio_path = None

    # 1. ä¼˜å…ˆä½¿ç”¨ä¼šè¯ä¸­çš„è·¯å¾„
    if current_session.reference_audio_path and current_session.reference_audio_path.exists():
        ref_audio_path = current_session.reference_audio_path
        print(f"ğŸ“ ä½¿ç”¨ä¼šè¯ä¸­çš„å‚è€ƒéŸ³é¢‘: {ref_audio_path}")

    # 2. å°è¯•ä»ä»“å‚¨åŠ è½½
    else:
        ref_audio_path = audio_file_repo.load_reference_audio(current_session.video.path)
        if ref_audio_path and ref_audio_path.exists():
            current_session.reference_audio_path = ref_audio_path
            print(f"ğŸ“ ä»ä»“å‚¨åŠ è½½å‚è€ƒéŸ³é¢‘: {ref_audio_path}")

    # 3. éƒ½å¤±è´¥äº†ï¼Œæç¤ºç”¨æˆ·
    if not ref_audio_path:
        return (
            "âŒ é”™è¯¯: ç¼ºå°‘å‚è€ƒéŸ³é¢‘\n\n"
            "ğŸ’¡ è§£å†³æ–¹æ¡ˆ:\n"
            "   1. é‡æ–°æ‰§è¡Œæ­¥éª¤2A\n"
            "   2. ä¸Šä¼ å‚è€ƒéŸ³é¢‘æˆ–è®©ç³»ç»Ÿä»è§†é¢‘æå–",
            gr.update()
        )

    try:
        print(f"\nğŸ”„ é‡æ–°ç”Ÿæˆä¿®æ”¹ç‰‡æ®µ:")
        print(f"   ä¿®æ”¹ç‰‡æ®µæ•°: {len(current_session.modified_indices)}")
        print(f"   å‚è€ƒéŸ³é¢‘: {ref_audio_path}")

        result = regenerate_modified_segments_use_case(
            video=current_session.video,
            original_subtitle=current_session.original_subtitle,
            modified_subtitle=current_session.translated_subtitle,
            modified_indices=current_session.modified_indices,
            tts_provider=container.get_tts(),
            video_processor=container.video_processor,
            audio_repo=audio_segment_repo,
            reference_audio_path=ref_audio_path,
            progress=None
        )

        for audio_seg in result.audio_segments:
            current_session.audio_segments[audio_seg.segment_index] = audio_seg

        current_session.modified_indices.clear()

        updated_data = _prepare_review_data_v2()

        return (
            f"âœ… é‡æ–°ç”Ÿæˆå®Œæˆ!\n"
            f"   é‡æ–°ç”Ÿæˆ: {result.regenerated_segments} ä¸ªç‰‡æ®µ\n"
            f"   è€—æ—¶: {result.synthesis_time:.1f} ç§’",
            gr.update(value=updated_data)
        )

    except Exception as e:
        import traceback
        return f"âŒ é‡æ–°ç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}", gr.update()


# ============== ç‰‡æ®µé¢„è§ˆ ============== #

def preview_segment(evt: gr.SelectData):
    """é¢„è§ˆé€‰ä¸­çš„ç‰‡æ®µ"""
    global current_session

    if evt is None or not current_session.video or not current_session.translated_subtitle:
        return None, "âš ï¸ æ— æ•ˆçš„ä¼šè¯çŠ¶æ€", "", ""

    try:
        if isinstance(evt.index, (tuple, list)):
            selected_row_index = int(evt.index[0])
        else:
            selected_row_index = int(evt.index)

        total_segments = len(current_session.translated_subtitle.segments)
        if selected_row_index < 0 or selected_row_index >= total_segments:
            return None, f"âŒ æ— æ•ˆçš„ç‰‡æ®µç´¢å¼•: {selected_row_index}", "", ""

        idx = selected_row_index
        text_seg = current_session.translated_subtitle.segments[idx]
        max_duration = text_seg.time_range.duration

        audio_seg = current_session.audio_segments.get(idx)
        if not audio_seg:
            audio_seg = audio_segment_repo.load_segment(
                segment_index=idx,
                video_path=current_session.video.path,
                text_segment=text_seg
            )
            if audio_seg:
                current_session.audio_segments[idx] = audio_seg

        actual_duration = None
        if audio_seg and audio_seg.file_path and audio_seg.file_path.exists():
            audio_path = str(audio_seg.file_path)
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            duration_diff = actual_duration - max_duration
            diff_sign = "+" if duration_diff > 0 else ""
            audio_status = f"âœ… éŸ³é¢‘å·²ç”Ÿæˆ ({(actual_duration / max_duration * 100):.1f}%)"
        else:
            audio_path = None
            audio_status = "âš ï¸ éŸ³é¢‘æœªç”Ÿæˆ"

        if actual_duration:
            duration_diff = actual_duration - max_duration
            diff_sign = "+" if duration_diff > 0 else ""
            text_info = f"""
ç‰‡æ®µ #{idx}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  æ—¶é—´è½´: {text_seg.time_range.start_seconds:.2f}s - {text_seg.time_range.end_seconds:.2f}s

ğŸ“ æ—¶é•¿ä¿¡æ¯:
   â€¢ æœ€å¤§å…è®¸: {max_duration:.2f}s
   â€¢ å®é™…ç”Ÿæˆ: {actual_duration:.2f}s
   â€¢ å·®å¼‚: {diff_sign}{duration_diff:.2f}s ({diff_sign}{(duration_diff / max_duration * 100):.1f}%)

ğŸ“Š çŠ¶æ€: {'âœ… æ­£å¸¸' if abs(duration_diff) < 0.5 else 'âš ï¸ åå·®è¾ƒå¤§'}
"""
        else:
            text_info = f"""
ç‰‡æ®µ #{idx}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  æ—¶é—´è½´: {text_seg.time_range.start_seconds:.2f}s - {text_seg.time_range.end_seconds:.2f}s

ğŸ“ æ—¶é•¿ä¿¡æ¯:
   â€¢ æœ€å¤§å…è®¸: {max_duration:.2f}s
   â€¢ å®é™…ç”Ÿæˆ: æœªç”Ÿæˆ
"""

        subtitle_text = text_seg.text
        return audio_path, audio_status, text_info, subtitle_text

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ é¢„è§ˆå¤±è´¥: {e}", "", ""


# ============== æ­¥éª¤3: æœ€ç»ˆåˆæˆ ============== #

def step3_final_synthesis(
    enable_bilingual: bool,  # âœ… æ–°å¢å‚æ•°ï¼šæ˜¯å¦å¯ç”¨åŒè¯­å­—å¹•
    progress=gr.Progress()
):
    """æ­¥éª¤3: æœ€ç»ˆè§†é¢‘åˆæˆï¼ˆæ”¯æŒåŒè¯­å­—å¹•å¯é€‰ï¼‰"""
    global current_session
    container.get_tts().unload()

    if not current_session.video:
        return None, None, None, "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±"

    total_segments = len(current_session.translated_subtitle.segments)
    audio_ready = len(current_session.audio_segments)

    print(f"\nğŸ” æœ€ç»ˆåˆæˆå‰æ£€æŸ¥:")
    print(f"   æ€»ç‰‡æ®µæ•°: {total_segments}")
    print(f"   éŸ³é¢‘å·²ç”Ÿæˆ: {audio_ready}")
    print(f"   ç¼ºå¤±ç‰‡æ®µ: {total_segments - audio_ready}")
    print(f"   åŒè¯­å­—å¹•: {'å¯ç”¨' if enable_bilingual else 'ç¦ç”¨'}")

    unreviewed = [
        idx for idx in range(total_segments)
        if idx not in current_session.audio_segments
    ]

    if unreviewed and len(unreviewed) > total_segments * 0.3:
        return None, None, None, f"âš ï¸ è¿˜æœ‰ {len(unreviewed)} ä¸ªç‰‡æ®µæœªå®ŒæˆéŸ³é¢‘ç”Ÿæˆ,è¯·å…ˆå®Œæˆæ­¥éª¤2"

    try:
        progress(0.1, "å‡†å¤‡åˆæˆ...")

        output_dir = current_session.video.path.parent / "output"
        output_dir.mkdir(exist_ok=True)

        # åˆå¹¶éŸ³é¢‘ç‰‡æ®µ
        progress(0.2, "åˆå¹¶éŸ³é¢‘ç‰‡æ®µ...")

        from domain.entities import AudioTrack
        from domain.value_objects import AudioSample

        sample_rate = list(current_session.audio_segments.values())[0].audio.sample_rate
        total_samples = int(current_session.video.duration * sample_rate)
        buffer = [0.0] * total_samples

        for idx, audio_seg in current_session.audio_segments.items():
            text_seg = audio_seg.text_segment
            start_idx = int(text_seg.time_range.start_seconds * sample_rate)

            for i, sample in enumerate(audio_seg.audio.samples):
                target_idx = start_idx + i
                if target_idx < total_samples:
                    buffer[target_idx] = sample

        full_audio = AudioSample(
            samples=tuple(buffer),
            sample_rate=sample_rate
        )

        audio_track = AudioTrack(full_audio, current_session.translated_subtitle.language)

        # è§†é¢‘åˆæˆ
        progress(0.5, "åˆæˆè§†é¢‘...")

        from application.use_cases.synthesize_video_use_case import synthesize_video_use_case
        from domain.services import merge_bilingual_subtitles

        # âœ… éœ€æ±‚2: æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šå­—å¹•æ–¹æ¡ˆ
        if enable_bilingual and current_session.english_subtitle:
            # å¯ç”¨åŒè¯­å­—å¹•
            zh_en_subtitle = merge_bilingual_subtitles(
                current_session.translated_subtitle,
                current_session.english_subtitle
            )
            subtitles_tuple = (
                current_session.translated_subtitle,
                current_session.english_subtitle,
                zh_en_subtitle
            )
            subtitle_mode = "åŒè¯­ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰"
        else:
            # åªç”¨ä¸­æ–‡å­—å¹•
            subtitles_tuple = (current_session.translated_subtitle,)
            subtitle_mode = "å•è¯­ï¼ˆä»…ä¸­æ–‡ï¼‰"

        synthesis_result = synthesize_video_use_case(
            video=current_session.video,
            subtitles=subtitles_tuple,
            audio_track=audio_track,
            video_processor=container.video_processor,
            subtitle_writer=container.subtitle_writer,
            output_dir=output_dir,
            formats=("srt", "ass"),
            burn_subtitles=True,
            progress=lambda p, d: progress(0.5 + p * 0.5, d)
        )

        # æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶
        def find_file(patterns: list[str], suffix: str = None) -> Optional[str]:
            for pattern in patterns:
                matches = [
                    p for p in synthesis_result.output_paths
                    if pattern in p.name and (suffix is None or p.suffix == suffix)
                ]
                if matches:
                    return str(matches[0])
            return None

        zh_srt = find_file(['zh.srt'], '.srt')
        zh_en_ass = find_file(['zh_en'], '.ass') if enable_bilingual else None
        voiced_video = find_file(['_voiced_subtitled.mp4'])

        status = f"""
âœ… æœ€ç»ˆåˆæˆå®Œæˆ!

ğŸ“¦ è¾“å‡ºæ–‡ä»¶:
   - ä¸­æ–‡å­—å¹•: {zh_srt.split('/')[-1] if zh_srt else 'âŒ'}
   - åŒè¯­å­—å¹•: {zh_en_ass.split('/')[-1] if zh_en_ass else 'æœªå¯ç”¨'}
   - é…éŸ³è§†é¢‘: {voiced_video.split('/')[-1] if voiced_video else 'âŒ'}

âš™ï¸  å­—å¹•æ¨¡å¼: {subtitle_mode}
â±ï¸  å¤„ç†æ—¶é—´: {synthesis_result.processing_time:.1f} ç§’

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   æ€»ç‰‡æ®µæ•°: {len(current_session.audio_segments)}
   ä½¿ç”¨ç¼“å­˜: {audio_ready - len(current_session.modified_indices)}
   é‡æ–°ç”Ÿæˆ: {len(current_session.modified_indices) if hasattr(current_session, 'modified_indices') else 0}
"""

        return zh_srt, zh_en_ass, voiced_video, status

    except Exception as e:
        import traceback
        error_msg = f"âŒ åˆæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, None, None, error_msg


# ============== UI æ„å»º ============== #

def build_ui_v2():
    """æ„å»ºå¢å¼º UI V2ï¼ˆå®Œæ•´é‡æ„ç‰ˆï¼‰"""

    with gr.Blocks(
        title="è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V2",
        css="""
        .gradio-container {max-width: 1800px !important}
        .segment-preview {border: 1px solid #ddd; padding: 10px; border-radius: 5px;}
        """
    ) as demo:
        gr.Markdown("""
        # ğŸ¬ è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V2 - å®Œæ•´é‡æ„ç‰ˆ

        ## âœ¨ æ–°å¢ç‰¹æ€§
        - ğŸµ **å¯é…ç½®å‚è€ƒéŸ³é¢‘**: æ”¯æŒè‡ªå®šä¹‰èµ·å§‹åç§»å’Œæ—¶é•¿
        - ğŸ“ **åŒè¯­å­—å¹•å¯é€‰**: åˆæˆè§†é¢‘å‰é€‰æ‹©æ˜¯å¦çƒ§å½•åŒè¯­å­—å¹•
        - ğŸ—ï¸ **æ¶æ„ä¼˜åŒ–**: å‚è€ƒéŸ³é¢‘ç®¡ç†ä¸‹æ²‰åˆ°Infrastructureå±‚
        
        ## ğŸ“‹ å·¥ä½œæµç¨‹
        1. **ç”Ÿæˆå­—å¹•** â†’ 2A. **å¢é‡è¯­éŸ³å…‹éš†** â†’ 2B. **å®¡æ ¸ä¿®æ”¹** â†’ 2C. **é‡æ–°ç”Ÿæˆ** â†’ 3. **æœ€ç»ˆåˆæˆ**
        """)

        with gr.Tab("ğŸ¬ å•è§†é¢‘å¤„ç† V2"):
            # ========== æ­¥éª¤1 ========== #
            with gr.Accordion("ğŸ” æ­¥éª¤1: ç”Ÿæˆå­—å¹•", open=True):
                with gr.Row():
                    with gr.Column(scale=1):
                        video_input = gr.File(
                            label="ğŸ“¹ ä¸Šä¼ è§†é¢‘",
                            file_types=[".mp4", ".avi", ".mov", ".mkv"]
                        )

                        whisper_model = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large", "large-v3"],
                            value="medium",
                            label="ğŸ™ï¸ Whisper æ¨¡å‹"
                        )

                        translation_model = gr.Dropdown(
                            choices=["Qwen/Qwen2.5-7B"],
                            value="Qwen/Qwen2.5-7B",
                            label="ğŸŒ ç¿»è¯‘æ¨¡å‹"
                        )

                        translation_context = gr.Dropdown(
                            choices=container.translator_context_repo.list_contexts(),
                            value="general",
                            label="ğŸ“š ç¿»è¯‘ä¸Šä¸‹æ–‡"
                        )

                        source_lang = gr.Dropdown(
                            choices=["auto", "en", "zh", "pt", "ja"],
                            value="auto",
                            label="ğŸ—£ï¸ æºè¯­è¨€"
                        )

                        step1_btn = gr.Button("â–¶ï¸ ç”Ÿæˆå­—å¹•", variant="primary")

                    with gr.Column(scale=1):
                        step1_status = gr.Textbox(
                            label="ğŸ“Š ç”ŸæˆçŠ¶æ€",
                            lines=12
                        )

            # ========== æ­¥éª¤2 ========== #
            with gr.Accordion("ğŸ¤ æ­¥éª¤2: å¢é‡è¯­éŸ³å…‹éš†", open=False) as step2_accordion:
                gr.Markdown("""
                ### å·¥ä½œæµç¨‹
                1. **2A. å¢é‡è¯­éŸ³å…‹éš†**: é€ç‰‡æ®µç”ŸæˆéŸ³é¢‘å¹¶ç¼“å­˜
                2. **2B. å®¡æ ¸é¢„è§ˆ**: è¯•å¬éŸ³é¢‘,ä¿®æ”¹å­—å¹•
                3. **2C. é‡æ–°ç”Ÿæˆ**: åªé‡æ–°ç”Ÿæˆä¿®æ”¹è¿‡çš„ç‰‡æ®µ
                """)

                # 2A: è¯­éŸ³å…‹éš†
                with gr.Group():
                    gr.Markdown("### 2A. å¢é‡è¯­éŸ³å…‹éš†")

                    reference_audio = gr.File(
                        label="ğŸµ å‚è€ƒéŸ³é¢‘(å¯é€‰ï¼Œç•™ç©ºåˆ™ä»è§†é¢‘æå–)",
                        file_types=[".wav", ".mp3"]
                    )

                    # âœ… æ–°å¢: å‚è€ƒéŸ³é¢‘é…ç½®
                    with gr.Row():
                        ref_duration_slider = gr.Slider(
                            minimum=5,
                            maximum=60,
                            value=10,
                            step=5,
                            label="â±ï¸ å‚è€ƒéŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰",
                            info="æå–æˆ–ä½¿ç”¨çš„å‚è€ƒéŸ³é¢‘é•¿åº¦"
                        )

                        ref_offset_slider = gr.Slider(
                            minimum=0,
                            maximum=120,
                            value=0,
                            step=5,
                            label="ğŸ“ èµ·å§‹åç§»ï¼ˆç§’ï¼‰",
                            info="ä»è§†é¢‘çš„ç¬¬å‡ ç§’å¼€å§‹æå–ï¼Œ0è¡¨ç¤ºä»å¤´å¼€å§‹ï¼ˆæˆ–ä½¿ç”¨VADæ£€æµ‹ï¼‰"
                        )

                    clone_btn = gr.Button("ğŸ¤ å¼€å§‹å¢é‡è¯­éŸ³å…‹éš†", variant="primary")
                    clone_status = gr.Textbox(label="å…‹éš†çŠ¶æ€", lines=10)

                # 2B: å®¡æ ¸è¡¨æ ¼
                with gr.Group():
                    gr.Markdown("### 2B. å®¡æ ¸å’Œé¢„è§ˆ")

                    review_dataframe = gr.Dataframe(
                        headers=[
                            "ç´¢å¼•", "æ—¶é—´", "åŸæ–‡", "ç¿»è¯‘",
                            "æœ€å¤§é•¿åº¦", "å·²ç”Ÿæˆé•¿åº¦", "éŸ³é¢‘", "é—®é¢˜", "çŠ¶æ€"
                        ],
                        datatype=[
                            "number", "str", "str", "str",
                            "str", "str", "str", "str", "str"
                        ],
                        col_count=(9, "fixed"),
                        row_count=(10, "dynamic"),
                        interactive=True,
                        wrap=True,
                        label="å­—å¹•å®¡æ ¸è¡¨æ ¼ (ç‚¹å‡»è¡Œé¢„è§ˆéŸ³é¢‘)"
                    )

                    with gr.Row():
                        save_edits_btn = gr.Button("ğŸ’¾ ä¿å­˜ä¿®æ”¹", variant="secondary")
                        regenerate_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆä¿®æ”¹çš„ç‰‡æ®µ", variant="primary")

                    edit_status = gr.Textbox(label="ç¼–è¾‘çŠ¶æ€", lines=3)

                # ç‰‡æ®µé¢„è§ˆåŒº
                with gr.Group():
                    gr.Markdown("### ğŸ‘‚ ç‰‡æ®µé¢„è§ˆ (ç‚¹å‡»è¡¨æ ¼è¡Œé¢„è§ˆ)")

                    with gr.Row():
                        with gr.Column(scale=1):
                            preview_audio = gr.Audio(
                                label="ğŸ”Š éŸ³é¢‘æ’­æ”¾",
                                type="filepath"
                            )
                            preview_status = gr.Textbox(label="çŠ¶æ€", lines=1)

                        with gr.Column(scale=1):
                            preview_info = gr.Textbox(label="ç‰‡æ®µä¿¡æ¯", lines=3)
                            preview_text = gr.Textbox(label="å­—å¹•æ–‡æœ¬", lines=4)

            # ========== æ­¥éª¤3 ========== #
            with gr.Accordion("ğŸ¬ æ­¥éª¤3: æœ€ç»ˆåˆæˆ", open=False):
                gr.Markdown("""
                ### æç¤º
                - ç¡®ä¿æ‰€æœ‰å…³é”®ç‰‡æ®µéƒ½å·²å®¡æ ¸é€šè¿‡
                - é€‰æ‹©æ˜¯å¦çƒ§å½•åŒè¯­å­—å¹•
                """)

                # âœ… æ–°å¢: åŒè¯­å­—å¹•é€‰é¡¹
                enable_bilingual_checkbox = gr.Checkbox(
                    label="ğŸ“ çƒ§å½•åŒè¯­å­—å¹•ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰",
                    value=True,
                    info="å–æ¶ˆå‹¾é€‰åˆ™åªçƒ§å½•ä¸­æ–‡å­—å¹•"
                )

                final_btn = gr.Button("â–¶ï¸ ç”Ÿæˆæœ€ç»ˆè§†é¢‘", variant="primary", size="lg")
                final_status = gr.Textbox(label="åˆæˆçŠ¶æ€", lines=12)

                with gr.Row():
                    zh_srt_output = gr.File(label="ä¸­æ–‡å­—å¹•")
                    zh_en_ass_output = gr.File(label="åŒè¯­å­—å¹•")
                    final_video_output = gr.File(label="æœ€ç»ˆè§†é¢‘")

            # ========== äº‹ä»¶ç»‘å®š ========== #

            # æ­¥éª¤1
            step1_btn.click(
                step1_generate_and_check_v2,
                inputs=[
                    video_input, whisper_model, translation_model,
                    translation_context, source_lang
                ],
                outputs=[review_dataframe, step1_status, step2_accordion]
            ).then(
                lambda: gr.update(open=True),
                outputs=[step2_accordion]
            )

            # æ­¥éª¤2A: è¯­éŸ³å…‹éš†
            clone_btn.click(
                step2_incremental_voice_cloning,
                inputs=[
                    reference_audio,
                    ref_duration_slider,  # âœ… æ–°å¢
                    ref_offset_slider  # âœ… æ–°å¢
                ],
                outputs=[clone_status, review_dataframe]
            )

            # æ­¥éª¤2B: ç¼–è¾‘ä¿å­˜
            save_edits_btn.click(
                step2_save_edits_and_regenerate,
                inputs=[review_dataframe],
                outputs=[edit_status, review_dataframe]
            )

            # æ­¥éª¤2C: é‡æ–°ç”Ÿæˆ
            regenerate_btn.click(
                step2_regenerate_modified,
                outputs=[edit_status, review_dataframe]
            )

            # è¡¨æ ¼é€‰æ‹©äº‹ä»¶
            review_dataframe.select(
                preview_segment,
                outputs=[preview_audio, preview_status, preview_info, preview_text]
            )

            # æ­¥éª¤3: æœ€ç»ˆåˆæˆ
            final_btn.click(
                step3_final_synthesis,
                inputs=[enable_bilingual_checkbox],  # âœ… æ–°å¢
                outputs=[zh_srt_output, zh_en_ass_output, final_video_output, final_status]
            )

    return demo


def main():
    """å¯åŠ¨ WebUI V2"""
    demo = build_ui_v2()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        inbrowser=True,
        share=False
    )


if __name__ == "__main__":
    main()