"""
Infrastructure Layer - å¢å¼º WebUI V2 (ä¿®å¤ç‰ˆ)
ä¿®å¤éŸ³é¢‘ç‰‡æ®µç¼“å­˜å’Œé¢„è§ˆé—®é¢˜
"""
from pathlib import Path
from typing import Optional, Dict

import gradio as gr

# å¯¼å…¥æ–°çš„ç”¨ä¾‹
from application.use_cases.incremental_voice_cloning import (
    incremental_voice_cloning_use_case,
    regenerate_modified_segments_use_case
)
from domain.entities import (
    Video, Subtitle, LanguageCode,
    TextSegment, TimeRange, AudioSegment,
    SegmentReviewStatus
)
from domain.services import calculate_cache_key
from infrastructure.adapters.storage.audio_segment_repository_adapter import AudioSegmentRepositoryAdapter
from infrastructure.config.dependency_injection import container

# åˆå§‹åŒ–éŸ³é¢‘ç‰‡æ®µä»“å‚¨
audio_segment_repo = AudioSegmentRepositoryAdapter()


# ============== ä¼šè¯çŠ¶æ€ç®¡ç† V2 ============== #
class TranslationSessionV2:
    """å¢å¼ºçš„ç¿»è¯‘ä¼šè¯çŠ¶æ€"""

    def __init__(self):
        self.translation_context = None
        self.video: Optional[Video] = None
        self.original_subtitle: Optional[Subtitle] = None
        self.translated_subtitle: Optional[Subtitle] = None
        self.english_subtitle: Optional[Subtitle] = None
        self.detected_language: Optional[LanguageCode] = None
        self.source_language: Optional[LanguageCode] = None
        self.quality_report = None

        # æ–°å¢:éŸ³é¢‘ç‰‡æ®µç®¡ç†
        self.audio_segments: Dict[int, AudioSegment] = {}
        self.segment_review_status: Dict[int, SegmentReviewStatus] = {}

        # ä¿®æ”¹è¿½è¸ª
        self.edited_segments: Dict[int, str] = {}
        self.modified_indices: set[int] = set()

        # å‚è€ƒéŸ³é¢‘
        self.reference_audio_path: Optional[Path] = None

        self.approved = False


# å…¨å±€ä¼šè¯å¯¹è±¡
current_session = TranslationSessionV2()


# ============== ğŸ”§ ä¿®å¤å‡½æ•°:åŠ è½½å·²ç¼“å­˜çš„éŸ³é¢‘ç‰‡æ®µ ============== #
def _load_cached_audio_segments(video: Video, subtitle: Subtitle) -> Dict[int, AudioSegment]:
    """
    ä»ç£ç›˜åŠ è½½å·²ç¼“å­˜çš„éŸ³é¢‘ç‰‡æ®µ

    Returns:
        {segment_index: AudioSegment}
    """
    cached_segments = {}

    print(f"\nğŸ” æ£€æŸ¥éŸ³é¢‘ç‰‡æ®µç¼“å­˜:")
    print(f"   è§†é¢‘: {video.path.name}")
    print(f"   ç‰‡æ®µæ€»æ•°: {len(subtitle.segments)}")

    for idx, text_seg in enumerate(subtitle.segments):
        try:
            # å°è¯•ä»ä»“å‚¨åŠ è½½
            audio_seg = audio_segment_repo.load_segment(
                segment_index=idx,
                video_path=video.path,
                text_segment=text_seg
            )

            if audio_seg:
                cached_segments[idx] = audio_seg
                # print(f"   âœ… ç‰‡æ®µ {idx} å·²åŠ è½½")
            # else:
            #     print(f"   âš ï¸  ç‰‡æ®µ {idx} æœªç¼“å­˜")

        except Exception as e:
            print(f"   âŒ ç‰‡æ®µ {idx} åŠ è½½å¤±è´¥: {e}")
            continue

    print(f"âœ… å…±åŠ è½½ {len(cached_segments)}/{len(subtitle.segments)} ä¸ªç¼“å­˜ç‰‡æ®µ\n")

    return cached_segments


# ============== æ­¥éª¤1: ç”Ÿæˆå­—å¹•å’Œè´¨é‡æ£€æŸ¥ ============== #
def step1_generate_and_check_v2(
        video_file,
        whisper_model: str,
        translation_model: str,
        translation_context_name: str,
        source_language: str,
        progress=gr.Progress()
):
    """æ­¥éª¤1: ç”Ÿæˆå­—å¹•å¹¶è¿›è¡Œè´¨é‡æ£€æŸ¥"""
    global current_session

    if not video_file:
        return None, "âŒ è¯·ä¸Šä¼ è§†é¢‘", gr.update(visible=False)

    try:
        current_session = TranslationSessionV2()

        video_path = Path(video_file.name)

        # åˆ›å»ºè§†é¢‘å¯¹è±¡
        current_session.video = Video(
            path=video_path,
            duration=get_video_duration(video_path),
            has_audio=True
        )

        # åŠ è½½ç¿»è¯‘ä¸Šä¸‹æ–‡
        translation_context = container.translator_context_repo.load(
            translation_context_name
        )

        if not translation_context:
            return None, f"âŒ æ— æ³•åŠ è½½ç¿»è¯‘ä¸Šä¸‹æ–‡: {translation_context_name}", gr.update(visible=False)

        # è§£ææºè¯­è¨€
        src_lang = LanguageCode(source_language) if source_language != "auto" else None

        progress(0.0, "å¼€å§‹ç”Ÿæˆå­—å¹•...")

        # ä½¿ç”¨æ”¹è¿›çš„å­—å¹•ç”Ÿæˆç”¨ä¾‹
        from application.use_cases.improved_generate_subtitles import improved_generate_subtitles_use_case

        result = improved_generate_subtitles_use_case(
            video=current_session.video,
            asr_provider=container.get_asr(whisper_model),
            translation_provider=container.get_translator(),
            video_processor=container.video_processor,
            cache_repo=container.cache_repo,
            translation_context=translation_context,
            target_language=LanguageCode.CHINESE,
            source_language=src_lang,
            enable_quality_check=True,
            progress=lambda p, d: progress(p, d)
        )

        container.get_translator().unload()

        current_session.original_subtitle = result.original_subtitle
        current_session.translated_subtitle = result.translated_subtitle
        current_session.detected_language = result.detected_language
        current_session.quality_report = result.quality_report
        current_session.translation_context = translation_context
        current_session.source_language = src_lang

        src_lang = LanguageCode(
            current_session.source_language.value) if current_session.source_language and current_session.source_language.value != "auto" else None

        # ä»ç¼“å­˜åŠ è½½è‹±æ–‡å­—å¹•
        cache_params = {
            "target_language": LanguageCode.CHINESE.value,
            "source_language": src_lang
        }

        if translation_context:
            cache_params["context_domain"] = translation_context.domain

        cache_key = calculate_cache_key(
            current_session.video.path,
            "subtitles_v2",
            cache_params
        )

        try:
            cached = container.cache_repo.get(cache_key)
            if cached and "en_segments" in cached:
                en_segments = tuple(
                    TextSegment(
                        text=seg["text"],
                        time_range=TimeRange(seg["start"], seg["end"]),
                        language=LanguageCode.ENGLISH
                    )
                    for seg in cached["en_segments"]
                )
                current_session.english_subtitle = Subtitle(en_segments, LanguageCode.ENGLISH)
        except Exception as e:
            print(f"  âš ï¸  åŠ è½½è‹±æ–‡å­—å¹•å¤±è´¥: {e}")

        # ğŸ”§ å…³é”®ä¿®å¤1: åŠ è½½å·²ç¼“å­˜çš„éŸ³é¢‘ç‰‡æ®µ
        progress(0.95, "æ£€æŸ¥éŸ³é¢‘ç¼“å­˜...")
        current_session.audio_segments = _load_cached_audio_segments(
            current_session.video,
            current_session.translated_subtitle
        )

        # ğŸ”§ å…³é”®ä¿®å¤1.5: å¦‚æœæœ‰ç¼“å­˜éŸ³é¢‘ï¼Œå°è¯•æ¢å¤å‚è€ƒéŸ³é¢‘è·¯å¾„
        if current_session.audio_segments:
            # å°è¯•ä»è§†é¢‘ä¸­æå–å‚è€ƒéŸ³é¢‘ï¼ˆä¸ºåç»­ç¼–è¾‘åšå‡†å¤‡ï¼‰
            try:
                temp_ref_audio = container.video_processor.extract_reference_audio(
                    current_session.video,
                    duration=10.0
                )
                current_session.reference_audio_path = temp_ref_audio
                print(f"  âœ… å·²å‡†å¤‡å‚è€ƒéŸ³é¢‘: {temp_ref_audio}")
            except Exception as e:
                print(f"  âš ï¸  å‡†å¤‡å‚è€ƒéŸ³é¢‘å¤±è´¥: {e}")
                print(f"  ğŸ’¡ æç¤º: å¦‚éœ€ä¿®æ”¹å­—å¹•ï¼Œè¯·å…ˆæ‰§è¡Œæ­¥éª¤2Aè·å–å‚è€ƒéŸ³é¢‘")

        # åˆå§‹åŒ–å®¡æ ¸çŠ¶æ€
        for idx in range(len(result.translated_subtitle.segments)):
            # æ£€æŸ¥éŸ³é¢‘æ˜¯å¦å·²ç¼“å­˜
            audio_exists = idx in current_session.audio_segments

            current_session.segment_review_status[idx] = SegmentReviewStatus(
                segment_index=idx,
                subtitle_approved=False,
                audio_approved=audio_exists,  # å¦‚æœéŸ³é¢‘å·²ç¼“å­˜åˆ™æ ‡è®°ä¸ºå·²å®Œæˆ
                subtitle_modified=False,
                needs_regeneration=not audio_exists  # å¦‚æœéŸ³é¢‘ä¸å­˜åœ¨åˆ™éœ€è¦ç”Ÿæˆ
            )

        # ç”ŸæˆçŠ¶æ€æŠ¥å‘Š
        cached_audio_count = len(current_session.audio_segments)
        total_segments = len(result.translated_subtitle.segments)

        # ğŸ†• è®¡ç®—éŸ³é¢‘æ—¶é•¿ç»Ÿè®¡
        total_max_duration = sum(seg.time_range.duration for seg in result.translated_subtitle.segments)
        total_actual_duration = sum(
            len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            for audio_seg in current_session.audio_segments.values()
        )

        report_lines = [
            f"âœ… å­—å¹•ç”Ÿæˆå®Œæˆ",
            f"",
            f"ğŸ“Š åŸºæœ¬ä¿¡æ¯:",
            f"   è§†é¢‘: {current_session.video.path.name}",
            f"   æ—¶é•¿: {current_session.video.duration:.1f} ç§’",
            f"   æ£€æµ‹è¯­è¨€: {result.detected_language.value}",
            f"   æ€»ç‰‡æ®µæ•°: {total_segments}",
            f"   ä½¿ç”¨ä¸Šä¸‹æ–‡: {translation_context.domain}",
            f"",
            f"ğŸµ éŸ³é¢‘ç¼“å­˜çŠ¶æ€:",
            f"   å·²ç¼“å­˜ç‰‡æ®µ: {cached_audio_count}/{total_segments}",
            f"   éœ€è¦ç”Ÿæˆ: {total_segments - cached_audio_count}",
            f"   ç†è®ºæ€»æ—¶é•¿: {total_max_duration:.1f}s",
            f"   å·²ç”Ÿæˆæ—¶é•¿: {total_actual_duration:.1f}s",
        ]

        # è´¨é‡æŠ¥å‘Š
        if result.quality_report:
            qr = result.quality_report
            report_lines.extend([
                f"",
                f"ğŸ” è´¨é‡æ£€æŸ¥ç»“æœ:",
                f"   æ•´ä½“è´¨é‡: {qr.overall_quality}",
                f"   å‘ç°é—®é¢˜: {qr.issues_found} ä¸ª",
                f"   æ˜¯å¦éœ€è¦å®¡æ ¸: {'æ˜¯ âš ï¸' if qr.requires_review else 'å¦ âœ…'}",
            ])

        status_report = "\n".join(report_lines)

        # å‡†å¤‡å®¡æ ¸æ•°æ®(ä¸åŒ…å«éŸ³é¢‘)
        review_data = _prepare_review_data_v2()

        return review_data, status_report, gr.update(visible=True)

    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, error_msg, gr.update(visible=False)


def _prepare_review_data_v2():
    """å‡†å¤‡å®¡æ ¸æ•°æ®(åŒ…å«éŸ³é¢‘æ’­æ”¾å™¨å’Œæ—¶é•¿ä¿¡æ¯)"""
    global current_session

    if not current_session.translated_subtitle:
        return None

    data = []
    for idx, (orig_seg, trans_seg) in enumerate(
            zip(current_session.original_subtitle.segments,
                current_session.translated_subtitle.segments)
    ):
        # ä¼˜å…ˆæ‹¿è‹±æ–‡å­—å¹•
        en_text = (
            current_session.english_subtitle.segments[idx].text
            if current_session.english_subtitle and idx < len(current_session.english_subtitle.segments)
            else orig_seg.text
        )

        # é—®é¢˜æ ‡è®°
        has_issue = False
        issue_desc = ""
        if current_session.quality_report:
            segment_issues = [
                i for i in current_session.quality_report.issues
                if i.segment_index == idx
            ]
            if segment_issues:
                has_issue = True
                issue_desc = "; ".join([
                    f"{i.issue_type}({i.severity})"
                    for i in segment_issues
                ])

        # ğŸ”§ è®¡ç®—æ—¶é—´ç‰‡æœ€å¤§é•¿åº¦ï¼ˆç§’ï¼‰
        max_duration = trans_seg.time_range.duration

        # ğŸ”§ è®¡ç®—å·²ç”ŸæˆéŸ³é¢‘é•¿åº¦
        audio_seg = current_session.audio_segments.get(idx)
        if audio_seg:
            # å¦‚æœæœ‰éŸ³é¢‘ï¼Œè®¡ç®—å®é™…é•¿åº¦
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            audio_status = "âœ… å·²ç¼“å­˜"
            duration_str = f"{actual_duration:.2f}s"
        else:
            audio_status = "æœªç”Ÿæˆ"
            duration_str = "-"

        # å®¡æ ¸çŠ¶æ€
        review_status = current_session.segment_review_status.get(idx)
        if review_status:
            if review_status.subtitle_approved and review_status.audio_approved:
                review_mark = "âœ… å·²å®¡æ ¸"
            elif review_status.subtitle_modified:
                review_mark = "ğŸ”„ å·²ä¿®æ”¹"
            else:
                review_mark = "â³ å¾…å®¡æ ¸"
        else:
            review_mark = "â³ å¾…å®¡æ ¸"

        data.append([
            idx,
            f"{trans_seg.time_range.start_seconds:.2f}s",
            en_text,
            trans_seg.text,
            f"{max_duration:.2f}s",  # ğŸ†• æœ€å¤§é•¿åº¦
            duration_str,  # ğŸ†• å·²ç”Ÿæˆé•¿åº¦
            audio_status,
            "âš ï¸" if has_issue else "",
            review_mark
        ])

    return data


# ============== æ­¥éª¤2: å¢é‡è¯­éŸ³å…‹éš† ============== #
def step2_incremental_voice_cloning(
        reference_audio_file,
        progress=gr.Progress()
):
    """æ­¥éª¤2: å¢é‡è¯­éŸ³å…‹éš†(é€ç‰‡æ®µåˆæˆ)"""
    global current_session

    if not current_session.video or not current_session.translated_subtitle:
        return "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±", gr.update()

    try:
        # å‡†å¤‡å‚è€ƒéŸ³é¢‘
        if reference_audio_file:
            ref_audio_path = Path(reference_audio_file.name)
            current_session.reference_audio_path = ref_audio_path
        elif current_session.reference_audio_path:
            ref_audio_path = current_session.reference_audio_path
        else:
            progress(0.05, "æå–å‚è€ƒéŸ³é¢‘...")
            ref_audio_path = container.video_processor.extract_reference_audio(
                current_session.video,
                duration=10.0
            )
            current_session.reference_audio_path = ref_audio_path

        # å®æ—¶è¿›åº¦å›è°ƒ(æ›´æ–°è¡¨æ ¼)
        synthesis_log = []

        def segment_progress(ratio, msg, idx, audio_seg):
            synthesis_log.append(f"[{ratio * 100:.0f}%] {msg}")
            progress(ratio, msg)

            # ğŸ”§ å…³é”®ä¿®å¤3: å¦‚æœæœ‰éŸ³é¢‘ç‰‡æ®µ,æ›´æ–°ä¼šè¯çŠ¶æ€
            if audio_seg:
                current_session.audio_segments[idx] = audio_seg

                # æ›´æ–°å®¡æ ¸çŠ¶æ€
                status = current_session.segment_review_status.get(idx)
                if status and not status.subtitle_modified:
                    current_session.segment_review_status[idx] = SegmentReviewStatus(
                        segment_index=idx,
                        subtitle_approved=False,
                        audio_approved=True,  # æ ‡è®°éŸ³é¢‘å·²å®Œæˆ
                        subtitle_modified=False,
                        needs_regeneration=False
                    )

        # æ‰§è¡Œå¢é‡åˆæˆ
        result = incremental_voice_cloning_use_case(
            video=current_session.video,
            subtitle=current_session.translated_subtitle,
            tts_provider=container.get_tts(),
            video_processor=container.video_processor,
            audio_repo=audio_segment_repo,
            cache_repo=container.cache_repo,
            reference_audio_path=ref_audio_path,
            progress=segment_progress
        )

        #container.get_tts().unload()

        # ğŸ”§ å…³é”®ä¿®å¤4: æ›´æ–°æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µåˆ°ä¼šè¯
        for audio_seg in result.audio_segments:
            current_session.audio_segments[audio_seg.segment_index] = audio_seg

        status = f"""
âœ… å¢é‡è¯­éŸ³å…‹éš†å®Œæˆ!

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   æ€»ç‰‡æ®µæ•°: {result.total_segments}
   ç¼“å­˜å‘½ä¸­: {result.cached_segments}
   æ–°ç”Ÿæˆ: {result.regenerated_segments}
   è€—æ—¶: {result.synthesis_time:.1f} ç§’

ğŸ’¡ æç¤º: 
   - ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡ŒæŸ¥çœ‹å’Œæ’­æ”¾éŸ³é¢‘
   - ä¿®æ”¹å­—å¹•åéœ€è¦é‡æ–°ç”Ÿæˆå¯¹åº”ç‰‡æ®µ
   - å®¡æ ¸é€šè¿‡åå¯ä»¥ç»§ç»­æ­¥éª¤3
"""

        # æ›´æ–°è¡¨æ ¼æ•°æ®
        updated_data = _prepare_review_data_v2()

        return status, gr.update(value=updated_data)

    except Exception as e:
        import traceback
        error_msg = f"âŒ è¯­éŸ³å…‹éš†å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return error_msg, gr.update()


# ============== æ­¥éª¤2B: å­—å¹•ç¼–è¾‘å’Œé‡æ–°ç”Ÿæˆ ============== #
def step2_save_edits_and_regenerate(review_dataframe):
    """ä¿å­˜ç¼–è¾‘å¹¶æ ‡è®°éœ€è¦é‡æ–°ç”Ÿæˆçš„ç‰‡æ®µ"""
    global current_session

    if hasattr(review_dataframe, "values"):
        review_dataframe = review_dataframe.values.tolist()

    if not review_dataframe:
        return "âš ï¸ æ²¡æœ‰å¯ä¿å­˜çš„ä¿®æ”¹", gr.update()

    # è·³è¿‡è¡¨å¤´
    if review_dataframe and isinstance(review_dataframe[0][0], str):
        review_dataframe = review_dataframe[1:]

    edited_count = 0
    modified_indices = set()

    for row in review_dataframe:
        try:
            idx = int(row[0])
        except (ValueError, IndexError):
            continue

        if idx >= len(current_session.translated_subtitle.segments):
            continue

        original_text = current_session.translated_subtitle.segments[idx].text
        edited_text = row[3]  # ç¿»è¯‘åˆ—

        if edited_text != original_text:
            current_session.edited_segments[idx] = edited_text
            current_session.modified_indices.add(idx)
            edited_count += 1

            # æ›´æ–°å®¡æ ¸çŠ¶æ€
            status = current_session.segment_review_status.get(idx)
            if status:
                current_session.segment_review_status[idx] = status.mark_subtitle_modified()

    if edited_count:
        # åº”ç”¨ç¼–è¾‘åˆ°å­—å¹•
        _apply_edits_to_subtitle_v2()

        # ä¿å­˜åˆ°ç¼“å­˜
        _save_to_cache_v2("ä¿å­˜ä¿®æ”¹")

        updated_data = _prepare_review_data_v2()

        return (
            f"âœ… å·²ä¿å­˜ {edited_count} å¤„ä¿®æ”¹\n"
            f"âš ï¸  éœ€è¦é‡æ–°ç”Ÿæˆ {len(current_session.modified_indices)} ä¸ªéŸ³é¢‘ç‰‡æ®µ",
            gr.update(value=updated_data)
        )
    else:
        return "â„¹ï¸ æœªæ£€æµ‹åˆ°ä¿®æ”¹", gr.update()


def step2_regenerate_modified():
    """é‡æ–°ç”Ÿæˆä¿®æ”¹è¿‡çš„ç‰‡æ®µ"""
    global current_session

    if not current_session.modified_indices:
        return "â„¹ï¸ æ²¡æœ‰éœ€è¦é‡æ–°ç”Ÿæˆçš„ç‰‡æ®µ", gr.update()

    # ğŸ”§ å…³é”®ä¿®å¤: æ£€æŸ¥å‚è€ƒéŸ³é¢‘
    if not current_session.reference_audio_path:
        return "âŒ é”™è¯¯: ç¼ºå°‘å‚è€ƒéŸ³é¢‘ã€‚è¯·å…ˆå®Œæˆæ­¥éª¤2A(å¢é‡è¯­éŸ³å…‹éš†)", gr.update()

    if not current_session.reference_audio_path.exists():
        return f"âŒ é”™è¯¯: å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {current_session.reference_audio_path}", gr.update()

    try:
        print(f"\nğŸ”„ é‡æ–°ç”Ÿæˆä¿®æ”¹ç‰‡æ®µ:")
        print(f"   ä¿®æ”¹ç‰‡æ®µæ•°: {len(current_session.modified_indices)}")
        print(f"   å‚è€ƒéŸ³é¢‘: {current_session.reference_audio_path}")

        result = regenerate_modified_segments_use_case(
            video=current_session.video,
            original_subtitle=current_session.original_subtitle,
            modified_subtitle=current_session.translated_subtitle,
            modified_indices=current_session.modified_indices,
            tts_provider=container.get_tts(),
            video_processor=container.video_processor,
            audio_repo=audio_segment_repo,
            reference_audio_path=current_session.reference_audio_path,  # ğŸ”§ ç¡®ä¿ä¼ é€’
            progress=None
        )

        #container.get_tts().unload()

        # æ›´æ–°éŸ³é¢‘ç‰‡æ®µ
        for audio_seg in result.audio_segments:
            current_session.audio_segments[audio_seg.segment_index] = audio_seg

        # æ¸…é™¤ä¿®æ”¹æ ‡è®°
        for idx in current_session.modified_indices:
            status = current_session.segment_review_status.get(idx)
            if status:
                current_session.segment_review_status[idx] = SegmentReviewStatus(
                    segment_index=idx,
                    subtitle_approved=False,
                    audio_approved=True,  # éŸ³é¢‘å·²é‡æ–°ç”Ÿæˆ
                    subtitle_modified=True,
                    needs_regeneration=False
                )

        current_session.modified_indices.clear()

        updated_data = _prepare_review_data_v2()

        return (
            f"âœ… é‡æ–°ç”Ÿæˆå®Œæˆ!\n"
            f"   é‡æ–°ç”Ÿæˆ: {result.regenerated_segments} ä¸ªç‰‡æ®µ\n"
            f"   è€—æ—¶: {result.synthesis_time:.1f} ç§’",
            gr.update(value=updated_data)
        )

    except Exception as e:
        import traceback
        return f"âŒ é‡æ–°ç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}", gr.update()


# ============== ğŸ”§ ä¿®å¤å‡½æ•°: ç‰‡æ®µé¢„è§ˆåŠŸèƒ½ ============== #
def preview_segment(evt: gr.SelectData):
    """
    é¢„è§ˆé€‰ä¸­çš„ç‰‡æ®µ

    Args:
        evt: Gradio SelectData äº‹ä»¶,åŒ…å«é€‰ä¸­è¡Œçš„ä¿¡æ¯
    """
    global current_session

    # ğŸ› è°ƒè¯•ä¿¡æ¯
    print(f"\nğŸ” é¢„è§ˆç‰‡æ®µè°ƒè¯•ä¿¡æ¯:")
    print(f"   äº‹ä»¶å¯¹è±¡: {evt}")
    print(f"   äº‹ä»¶ç±»å‹: {type(evt)}")
    if evt:
        print(f"   evt.index: {evt.index}")
        print(f"   evt.index ç±»å‹: {type(evt.index)}")
        print(f"   evt.value: {getattr(evt, 'value', 'N/A')}")

    # ğŸ”§ é˜²å¾¡æ€§æ£€æŸ¥1: æ£€æŸ¥äº‹ä»¶å¯¹è±¡
    if evt is None:
        print(f"   âŒ äº‹ä»¶å¯¹è±¡ä¸º None")
        return None, "âš ï¸ äº‹ä»¶æ•°æ®ä¸ºç©º", "", ""

    # ğŸ”§ é˜²å¾¡æ€§æ£€æŸ¥2: æ£€æŸ¥ä¼šè¯çŠ¶æ€
    if not current_session.video:
        print(f"   âŒ ä¼šè¯çŠ¶æ€ä¸¢å¤±")
        return None, "âŒ ä¼šè¯çŠ¶æ€ä¸¢å¤±,è¯·é‡æ–°ä»æ­¥éª¤1å¼€å§‹", "", ""

    if not current_session.translated_subtitle:
        print(f"   âŒ æ²¡æœ‰å­—å¹•æ•°æ®")
        return None, "âŒ æ²¡æœ‰å­—å¹•æ•°æ®", "", ""

    # ğŸ”§ é˜²å¾¡æ€§æ£€æŸ¥3: æ£€æŸ¥ç´¢å¼•
    try:
        if evt.index is None:
            print(f"   âŒ evt.index ä¸º None")
            return None, "âš ï¸ æœªé€‰ä¸­ä»»ä½•è¡Œ", "", ""

        # ğŸ”§ å…³é”®ä¿®å¤: æ­£ç¡®è§£æ evt.index
        print(f"   åŸå§‹ evt.index: {evt.index}, ç±»å‹: {type(evt.index)}")

        if isinstance(evt.index, (tuple, list)):
            # [row, col] æˆ– (row, col) æ ¼å¼
            if len(evt.index) >= 1:
                selected_row_index = evt.index[0]
                print(f"   âœ… è§£æåºåˆ—ç´¢å¼•: {evt.index} -> è¡Œ {selected_row_index}")
            else:
                print(f"   âŒ ç©ºåºåˆ—ç´¢å¼•")
                return None, "âŒ ç´¢å¼•æ ¼å¼é”™è¯¯ï¼ˆç©ºåºåˆ—ï¼‰", "", ""
        elif isinstance(evt.index, (int, float)):
            # ç›´æ¥æ˜¯æ•°å­—
            selected_row_index = evt.index
            print(f"   âœ… ç›´æ¥ä½¿ç”¨ç´¢å¼•: {selected_row_index}")
        else:
            # æœªçŸ¥æ ¼å¼
            print(f"   âŒ æœªçŸ¥çš„ç´¢å¼•æ ¼å¼: {type(evt.index)}, å€¼: {evt.index}")
            return None, f"âŒ æœªçŸ¥çš„ç´¢å¼•æ ¼å¼: {type(evt.index)}", "", ""

        # ğŸ”§ é‡è¦ï¼šç¡®ä¿è½¬æ¢ä¸ºæ•´æ•°
        try:
            selected_row_index = int(selected_row_index)
        except (TypeError, ValueError) as e:
            print(f"   âŒ æ— æ³•è½¬æ¢ä¸ºæ•´æ•°: {selected_row_index}, é”™è¯¯: {e}")
            return None, f"âŒ ç´¢å¼•å€¼æ— æ³•è½¬æ¢ä¸ºæ•´æ•°: {selected_row_index}", "", ""

        print(f"   âœ… æœ€ç»ˆè¡Œç´¢å¼•: {selected_row_index} (ç±»å‹: {type(selected_row_index)})")

    except (TypeError, ValueError, IndexError) as e:
        print(f"   âŒ ç´¢å¼•è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, f"âŒ ç´¢å¼•è§£æå¤±è´¥: {e}", "", ""

    # ğŸ”§ é˜²å¾¡æ€§æ£€æŸ¥4: éªŒè¯ç´¢å¼•èŒƒå›´
    total_segments = len(current_session.translated_subtitle.segments)
    print(f"   æ€»ç‰‡æ®µæ•°: {total_segments}")

    if selected_row_index < 0 or selected_row_index >= total_segments:
        print(f"   âŒ ç´¢å¼•è¶…å‡ºèŒƒå›´: {selected_row_index}")
        return None, f"âŒ æ— æ•ˆçš„ç‰‡æ®µç´¢å¼•: {selected_row_index} (æ€»æ•°: {total_segments})", "", ""

    idx = selected_row_index

    try:
        text_seg = current_session.translated_subtitle.segments[idx]
        print(f"   âœ… è·å–ç‰‡æ®µ {idx}: {text_seg.text[:30]}...")
    except (IndexError, TypeError) as e:
        print(f"   âŒ æ— æ³•è·å–ç‰‡æ®µ {idx}: {e}")
        return None, f"âŒ æ— æ³•è·å–ç‰‡æ®µ {idx}: {e}", "", ""

    # ğŸ”§ è®¡ç®—æ—¶é•¿ä¿¡æ¯
    max_duration = text_seg.time_range.duration

    # ğŸ”§ å…³é”®ä¿®å¤5: ä»ä¼šè¯æˆ–ç£ç›˜è·å–éŸ³é¢‘
    audio_seg = current_session.audio_segments.get(idx)
    print(f"   å†…å­˜ä¸­éŸ³é¢‘: {audio_seg is not None}")

    # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰,å°è¯•ä»ç£ç›˜åŠ è½½
    if not audio_seg:
        print(f"   å°è¯•ä»ç£ç›˜åŠ è½½...")
        try:
            audio_seg = audio_segment_repo.load_segment(
                segment_index=idx,
                video_path=current_session.video.path,
                text_segment=text_seg
            )

            # å¦‚æœåŠ è½½æˆåŠŸ,æ›´æ–°åˆ°ä¼šè¯
            if audio_seg:
                current_session.audio_segments[idx] = audio_seg
                print(f"   âœ… ç‰‡æ®µ {idx} ä»ç£ç›˜åŠ è½½æˆåŠŸ")
            else:
                print(f"   âš ï¸  ç£ç›˜ä¹Ÿæ²¡æœ‰ç‰‡æ®µ {idx}")
        except Exception as e:
            print(f"   âŒ ç‰‡æ®µ {idx} åŠ è½½å¤±è´¥: {e}")

    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶å¹¶è®¡ç®—å®é™…æ—¶é•¿
    actual_duration = None
    if audio_seg and audio_seg.file_path:
        print(f"   éŸ³é¢‘æ–‡ä»¶è·¯å¾„: {audio_seg.file_path}")
        if audio_seg.file_path.exists():
            audio_path = str(audio_seg.file_path)
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate

            # ğŸ†• è®¡ç®—æ—¶é•¿å·®å¼‚
            duration_diff = actual_duration - max_duration
            duration_ratio = (actual_duration / max_duration) * 100 if max_duration > 0 else 0

            audio_status = f"âœ… éŸ³é¢‘å·²ç”Ÿæˆ ({duration_ratio:.1f}%)"
            print(f"   âœ… éŸ³é¢‘æ–‡ä»¶å­˜åœ¨ï¼Œæ—¶é•¿: {actual_duration:.2f}s")
        else:
            audio_path = None
            audio_status = f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_seg.file_path.name}"
            print(f"   âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
    else:
        audio_path = None
        audio_status = "âš ï¸  éŸ³é¢‘æœªç”Ÿæˆ"
        print(f"   âš ï¸  æ²¡æœ‰éŸ³é¢‘ç‰‡æ®µ")

    # ğŸ†• æ–‡æœ¬ä¿¡æ¯ - åŒ…å«è¯¦ç»†æ—¶é•¿ä¿¡æ¯
    if actual_duration:
        duration_diff = actual_duration - max_duration
        diff_sign = "+" if duration_diff > 0 else ""
        text_info = f"""
ç‰‡æ®µ #{idx}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  æ—¶é—´è½´: {text_seg.time_range.start_seconds:.2f}s - {text_seg.time_range.end_seconds:.2f}s

ğŸ“ æ—¶é•¿ä¿¡æ¯:
   â€¢ æœ€å¤§å…è®¸: {max_duration:.2f}s
   â€¢ å®é™…ç”Ÿæˆ: {actual_duration:.2f}s
   â€¢ å·®å¼‚: {diff_sign}{duration_diff:.2f}s ({diff_sign}{(duration_diff / max_duration * 100):.1f}%)

ğŸ“Š çŠ¶æ€: {'âœ… æ­£å¸¸' if abs(duration_diff) < 0.5 else 'âš ï¸ åå·®è¾ƒå¤§'}
"""
    else:
        text_info = f"""
ç‰‡æ®µ #{idx}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  æ—¶é—´è½´: {text_seg.time_range.start_seconds:.2f}s - {text_seg.time_range.end_seconds:.2f}s

ğŸ“ æ—¶é•¿ä¿¡æ¯:
   â€¢ æœ€å¤§å…è®¸: {max_duration:.2f}s
   â€¢ å®é™…ç”Ÿæˆ: æœªç”Ÿæˆ
"""

    subtitle_text = text_seg.text

    print(f"   è¿”å›ç»“æœ: audio={audio_path is not None}, status={audio_status}\n")

    return audio_path, audio_status, text_info, subtitle_text


# ============== è¾…åŠ©å‡½æ•° ============== #
def _apply_edits_to_subtitle_v2():
    """åº”ç”¨ç¼–è¾‘åˆ°å­—å¹•å¯¹è±¡"""
    global current_session

    if not current_session.edited_segments:
        return

    new_segments = []
    for idx, seg in enumerate(current_session.translated_subtitle.segments):
        if idx in current_session.edited_segments:
            new_seg = TextSegment(
                text=current_session.edited_segments[idx],
                time_range=seg.time_range,
                language=seg.language
            )
            new_segments.append(new_seg)
        else:
            new_segments.append(seg)

    current_session.translated_subtitle = Subtitle(
        segments=tuple(new_segments),
        language=current_session.translated_subtitle.language
    )


def _save_to_cache_v2(operation_name: str = "æ“ä½œ"):
    """ä¿å­˜åˆ°ç¼“å­˜"""
    global current_session

    try:
        if not current_session.video or not current_session.translated_subtitle:
            return
        src_lang = LanguageCode(current_session.source_language.value) if current_session.source_language and current_session.source_language.value != "auto" else None
        cache_params = {
            "target_language": LanguageCode.CHINESE.value,
            "source_language": src_lang,
        }

        if current_session.translation_context:
            cache_params["context_domain"] = current_session.translation_context.domain

        cache_key = calculate_cache_key(
            current_session.video.path,
            "subtitles_v2",
            cache_params
        )

        cached = container.cache_repo.get(cache_key) or {}

        # æ›´æ–°ä¸­æ–‡å­—å¹•
        cached["zh_segments"] = [
            {
                "text": seg.text,
                "start": seg.time_range.start_seconds,
                "end": seg.time_range.end_seconds,
            }
            for seg in current_session.translated_subtitle.segments
        ]

        container.cache_repo.set(cache_key, cached)
        print(f"âœ… {operation_name}: ä¸­æ–‡å­—å¹•å·²å†™å›ç¼“å­˜")

    except Exception as e:
        print(f"âš ï¸ {operation_name}: å†™å›ç¼“å­˜å¤±è´¥: {e}")


def get_video_duration(video_path: Path) -> float:
    """è·å–è§†é¢‘æ—¶é•¿"""
    import subprocess
    result = subprocess.run([
        'ffprobe', '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        str(video_path)
    ], capture_output=True, text=True)
    return float(result.stdout.strip())


# ============== æ­¥éª¤3: æœ€ç»ˆåˆæˆ ============== #
def step3_final_synthesis(progress=gr.Progress()):
    """æ­¥éª¤3: æœ€ç»ˆè§†é¢‘åˆæˆ"""
    global current_session
    container.get_tts().unload()
    if not current_session.video:
        return None, None, None, "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±"

    # ğŸ”§ å…³é”®ä¿®å¤6: é‡æ–°æ£€æŸ¥éŸ³é¢‘çŠ¶æ€
    total_segments = len(current_session.translated_subtitle.segments)
    audio_ready = len(current_session.audio_segments)

    print(f"\nğŸ” æœ€ç»ˆåˆæˆå‰æ£€æŸ¥:")
    print(f"   æ€»ç‰‡æ®µæ•°: {total_segments}")
    print(f"   éŸ³é¢‘å·²ç”Ÿæˆ: {audio_ready}")
    print(f"   ç¼ºå¤±ç‰‡æ®µ: {total_segments - audio_ready}")

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç‰‡æ®µéƒ½å·²å®¡æ ¸
    unreviewed = [
        idx for idx in range(total_segments)
        if idx not in current_session.audio_segments
    ]

    if unreviewed and len(unreviewed) > total_segments * 0.3:
        return None, None, None, f"âš ï¸  è¿˜æœ‰ {len(unreviewed)} ä¸ªç‰‡æ®µæœªå®ŒæˆéŸ³é¢‘ç”Ÿæˆ,è¯·å…ˆå®Œæˆæ­¥éª¤2"

    try:
        progress(0.1, "å‡†å¤‡åˆæˆ...")

        output_dir = current_session.video.path.parent / "output"
        output_dir.mkdir(exist_ok=True)

        # åˆå¹¶éŸ³é¢‘ç‰‡æ®µ
        progress(0.2, "åˆå¹¶éŸ³é¢‘ç‰‡æ®µ...")

        from domain.entities import AudioTrack
        from domain.value_objects import AudioSample

        # åˆ›å»ºå®Œæ•´éŸ³è½¨
        sample_rate = list(current_session.audio_segments.values())[0].audio.sample_rate
        total_samples = int(current_session.video.duration * sample_rate)
        buffer = [0.0] * total_samples

        for idx, audio_seg in current_session.audio_segments.items():
            text_seg = audio_seg.text_segment
            start_idx = int(text_seg.time_range.start_seconds * sample_rate)

            for i, sample in enumerate(audio_seg.audio.samples):
                target_idx = start_idx + i
                if target_idx < total_samples:
                    buffer[target_idx] = sample

        full_audio = AudioSample(
            samples=tuple(buffer),
            sample_rate=sample_rate
        )

        audio_track = AudioTrack(full_audio, current_session.translated_subtitle.language)

        # è§†é¢‘åˆæˆ
        progress(0.5, "åˆæˆè§†é¢‘...")

        from application.use_cases.synthesize_video_use_case import synthesize_video_use_case
        from domain.services import merge_bilingual_subtitles

        # åˆ›å»ºåŒè¯­å­—å¹•
        if current_session.english_subtitle:
            zh_en_subtitle = merge_bilingual_subtitles(
                current_session.translated_subtitle,
                current_session.english_subtitle
            )
            subtitles_tuple = (
                current_session.translated_subtitle,
                current_session.english_subtitle,
                zh_en_subtitle
            )
        else:
            subtitles_tuple = (current_session.translated_subtitle,)

        synthesis_result = synthesize_video_use_case(
            video=current_session.video,
            subtitles=subtitles_tuple,
            audio_track=audio_track,
            video_processor=container.video_processor,
            subtitle_writer=container.subtitle_writer,
            output_dir=output_dir,
            formats=("srt", "ass"),
            burn_subtitles=True,
            progress=lambda p, d: progress(0.5 + p * 0.5, d)
        )

        # æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶
        def find_file(patterns: list[str], suffix: str = None) -> Optional[str]:
            for pattern in patterns:
                matches = [
                    p for p in synthesis_result.output_paths
                    if pattern in p.name and (suffix is None or p.suffix == suffix)
                ]
                if matches:
                    return str(matches[0])
            return None

        zh_srt = find_file(['zh.srt'], '.srt')
        zh_en_ass = find_file(['zh_en'], '.ass')
        voiced_video = find_file(['_voiced_subtitled.mp4'])

        status = f"""
âœ… æœ€ç»ˆåˆæˆå®Œæˆ!

ğŸ“¦ è¾“å‡ºæ–‡ä»¶:
   - ä¸­æ–‡å­—å¹•: {zh_srt.split('/')[-1] if zh_srt else 'âŒ'}
   - åŒè¯­å­—å¹•: {zh_en_ass.split('/')[-1] if zh_en_ass else 'âŒ'}
   - é…éŸ³è§†é¢‘: {voiced_video.split('/')[-1] if voiced_video else 'âŒ'}

â±ï¸  å¤„ç†æ—¶é—´: {synthesis_result.processing_time:.1f} ç§’

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   æ€»ç‰‡æ®µæ•°: {len(current_session.audio_segments)}
   ä½¿ç”¨ç¼“å­˜: {len([s for s in current_session.segment_review_status.values() if not s.needs_regeneration])}
   é‡æ–°ç”Ÿæˆ: {len([s for s in current_session.segment_review_status.values() if s.subtitle_modified])}
"""

        return zh_srt, zh_en_ass, voiced_video, status

    except Exception as e:
        import traceback
        error_msg = f"âŒ åˆæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, None, None, error_msg


# ============== UI æ„å»º ============== #
def build_ui_v2():
    """æ„å»ºå¢å¼º UI V2"""

    with gr.Blocks(
            title="è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V2",
            css="""
        .gradio-container {max-width: 1800px !important}
        .segment-preview {border: 1px solid #ddd; padding: 10px; border-radius: 5px;}
        """
    ) as demo:
        gr.Markdown("""
        # ğŸ¬ è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V2 - åˆ†æ®µå®¡æ ¸ç‰ˆ (ä¿®å¤ç‰ˆ)

        ## âœ¨ V2 æ–°ç‰¹æ€§
        - ğŸµ **åˆ†æ®µè¯­éŸ³å…‹éš†**: é€ç‰‡æ®µç”Ÿæˆå¹¶ç¼“å­˜éŸ³é¢‘
        - ğŸ‘‚ **å®æ—¶é¢„è§ˆ**: è¾¹ç”Ÿæˆè¾¹è¯•å¬,å³æ—¶åé¦ˆ
        - âœï¸  **ç²¾ç»†ç¼–è¾‘**: ä¿®æ”¹å­—å¹•åä»…é‡æ–°ç”Ÿæˆå¯¹åº”ç‰‡æ®µ
        - ğŸ’¾ **æ™ºèƒ½ç¼“å­˜**: ç‰‡æ®µçº§ç¼“å­˜,æ–­ç‚¹ç»­ä¼ 
        - ğŸ”„ **å¢é‡åˆæˆ**: è·³è¿‡æœªä¿®æ”¹çš„ç‰‡æ®µ,æå‡æ•ˆç‡

        ## ğŸ”§ æœ¬æ¬¡ä¿®å¤
        - âœ… ä¿®å¤éŸ³é¢‘ç‰‡æ®µé¢„è§ˆæ— æ³•æ’­æ”¾é—®é¢˜
        - âœ… ä¿®å¤ç¼“å­˜åŠ è½½é€»è¾‘,æ­¥éª¤1åè‡ªåŠ¨åŠ è½½å·²ç¼“å­˜éŸ³é¢‘
        - âœ… ä¿®å¤æœ€ç»ˆåˆæˆæ—¶éŸ³é¢‘çŠ¶æ€æ£€æŸ¥
        - âœ… ä¼˜åŒ–ä¼šè¯çŠ¶æ€ç®¡ç†,ç¡®ä¿æ•°æ®ä¸€è‡´æ€§

        ## ğŸ“‹ ä¼˜åŒ–å·¥ä½œæµç¨‹
        1. **ç”Ÿæˆå­—å¹•** â†’ 2A. **å¢é‡è¯­éŸ³å…‹éš†** â†’ 2B. **å®¡æ ¸ä¿®æ”¹** â†’ 2C. **é‡æ–°ç”Ÿæˆ** â†’ 3. **æœ€ç»ˆåˆæˆ**
        """)

        with gr.Tab("ğŸ¬ å•è§†é¢‘å¤„ç† V2"):
            # ========== æ­¥éª¤1 ========== #
            with gr.Accordion("ğŸ” æ­¥éª¤1: ç”Ÿæˆå­—å¹•", open=True):
                with gr.Row():
                    with gr.Column(scale=1):
                        video_input = gr.File(
                            label="ğŸ“¹ ä¸Šä¼ è§†é¢‘",
                            file_types=[".mp4", ".avi", ".mov", ".mkv"]
                        )

                        whisper_model = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large", "large-v3"],
                            value="medium",
                            label="ğŸ™ï¸ Whisper æ¨¡å‹"
                        )

                        translation_model = gr.Dropdown(
                            choices=["Qwen/Qwen2.5-7B"],
                            value="Qwen/Qwen2.5-7B",
                            label="ğŸŒ ç¿»è¯‘æ¨¡å‹"
                        )

                        translation_context = gr.Dropdown(
                            choices=container.translator_context_repo.list_contexts(),
                            value="general",
                            label="ğŸ“š ç¿»è¯‘ä¸Šä¸‹æ–‡"
                        )

                        source_lang = gr.Dropdown(
                            choices=["auto", "en", "zh", "pt", "ja"],
                            value="auto",
                            label="ğŸ—£ï¸ æºè¯­è¨€"
                        )

                        step1_btn = gr.Button("â–¶ï¸ ç”Ÿæˆå­—å¹•", variant="primary")

                    with gr.Column(scale=1):
                        step1_status = gr.Textbox(
                            label="ğŸ“Š ç”ŸæˆçŠ¶æ€",
                            lines=12
                        )

            # ========== æ­¥éª¤2 ========== #
            with gr.Accordion("ğŸ¤ æ­¥éª¤2: å¢é‡è¯­éŸ³å…‹éš†", open=False) as step2_accordion:
                gr.Markdown("""
                ### å·¥ä½œæµç¨‹
                1. **2A. å¢é‡è¯­éŸ³å…‹éš†**: é€ç‰‡æ®µç”ŸæˆéŸ³é¢‘å¹¶ç¼“å­˜
                2. **2B. å®¡æ ¸é¢„è§ˆ**: è¯•å¬éŸ³é¢‘,ä¿®æ”¹å­—å¹•
                3. **2C. é‡æ–°ç”Ÿæˆ**: åªé‡æ–°ç”Ÿæˆä¿®æ”¹è¿‡çš„ç‰‡æ®µ
                """)

                # 2A: è¯­éŸ³å…‹éš†
                with gr.Group():
                    gr.Markdown("### 2A. å¢é‡è¯­éŸ³å…‹éš†")

                    reference_audio = gr.File(
                        label="ğŸµ å‚è€ƒéŸ³é¢‘(å¯é€‰)",
                        file_types=[".wav", ".mp3"]
                    )

                    clone_btn = gr.Button("ğŸ¤ å¼€å§‹å¢é‡è¯­éŸ³å…‹éš†", variant="primary")
                    clone_status = gr.Textbox(label="å…‹éš†çŠ¶æ€", lines=8)

                # 2B: å®¡æ ¸è¡¨æ ¼
                with gr.Group():
                    gr.Markdown("### 2B. å®¡æ ¸å’Œé¢„è§ˆ")

                    review_dataframe = gr.Dataframe(
                        headers=[
                            "ç´¢å¼•",
                            "æ—¶é—´",
                            "åŸæ–‡",
                            "ç¿»è¯‘",
                            "æœ€å¤§é•¿åº¦",  # ğŸ†• æ–°å¢åˆ—
                            "å·²ç”Ÿæˆé•¿åº¦",  # ğŸ†• æ–°å¢åˆ—
                            "éŸ³é¢‘",
                            "é—®é¢˜",
                            "çŠ¶æ€"
                        ],
                        datatype=[
                            "number",  # ç´¢å¼•
                            "str",  # æ—¶é—´
                            "str",  # åŸæ–‡
                            "str",  # ç¿»è¯‘
                            "str",  # ğŸ†• æœ€å¤§é•¿åº¦
                            "str",  # ğŸ†• å·²ç”Ÿæˆé•¿åº¦
                            "str",  # éŸ³é¢‘
                            "str",  # é—®é¢˜
                            "str"  # çŠ¶æ€
                        ],
                        col_count=(9, "fixed"),  # ğŸ”§ æ”¹ä¸º 9 åˆ—
                        row_count=(10, "dynamic"),
                        interactive=True,
                        wrap=True,
                        label="å­—å¹•å®¡æ ¸è¡¨æ ¼ (ç‚¹å‡»è¡Œé¢„è§ˆéŸ³é¢‘)"
                    )

                    with gr.Row():
                        save_edits_btn = gr.Button("ğŸ’¾ ä¿å­˜ä¿®æ”¹", variant="secondary")
                        regenerate_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆä¿®æ”¹çš„ç‰‡æ®µ", variant="primary")

                    edit_status = gr.Textbox(label="ç¼–è¾‘çŠ¶æ€", lines=3)

                # ç‰‡æ®µé¢„è§ˆåŒº
                with gr.Group():
                    gr.Markdown("### ğŸ‘‚ ç‰‡æ®µé¢„è§ˆ (ç‚¹å‡»è¡¨æ ¼è¡Œé¢„è§ˆ)")

                    with gr.Row():
                        with gr.Column(scale=1):
                            preview_audio = gr.Audio(
                                label="ğŸ”Š éŸ³é¢‘æ’­æ”¾",
                                type="filepath"
                            )
                            preview_status = gr.Textbox(
                                label="çŠ¶æ€",
                                lines=1
                            )

                        with gr.Column(scale=1):
                            preview_info = gr.Textbox(
                                label="ç‰‡æ®µä¿¡æ¯",
                                lines=3
                            )
                            preview_text = gr.Textbox(
                                label="å­—å¹•æ–‡æœ¬",
                                lines=4
                            )

            # ========== æ­¥éª¤3 ========== #
            with gr.Accordion("ğŸ¬ æ­¥éª¤3: æœ€ç»ˆåˆæˆ", open=False):
                gr.Markdown("""
                ### æç¤º
                - ç¡®ä¿æ‰€æœ‰å…³é”®ç‰‡æ®µéƒ½å·²å®¡æ ¸é€šè¿‡
                - ç³»ç»Ÿä¼šåˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå®Œæ•´è§†é¢‘
                """)

                final_btn = gr.Button("â–¶ï¸ ç”Ÿæˆæœ€ç»ˆè§†é¢‘", variant="primary", size="lg")
                final_status = gr.Textbox(label="åˆæˆçŠ¶æ€", lines=10)

                with gr.Row():
                    zh_srt_output = gr.File(label="ä¸­æ–‡å­—å¹•")
                    zh_en_ass_output = gr.File(label="åŒè¯­å­—å¹•")
                    final_video_output = gr.File(label="æœ€ç»ˆè§†é¢‘")

            # ========== äº‹ä»¶ç»‘å®š ========== #

            # æ­¥éª¤1
            step1_btn.click(
                step1_generate_and_check_v2,
                inputs=[
                    video_input, whisper_model, translation_model,
                    translation_context, source_lang
                ],
                outputs=[review_dataframe, step1_status, step2_accordion]
            ).then(
                lambda: gr.update(open=True),
                outputs=[step2_accordion]
            )

            # æ­¥éª¤2A: è¯­éŸ³å…‹éš†
            clone_btn.click(
                step2_incremental_voice_cloning,
                inputs=[reference_audio],
                outputs=[clone_status, review_dataframe]
            )

            # æ­¥éª¤2B: ç¼–è¾‘ä¿å­˜
            save_edits_btn.click(
                step2_save_edits_and_regenerate,
                inputs=[review_dataframe],
                outputs=[edit_status, review_dataframe]
            )

            # æ­¥éª¤2C: é‡æ–°ç”Ÿæˆ
            regenerate_btn.click(
                step2_regenerate_modified,
                outputs=[edit_status, review_dataframe]
            )

            # ğŸ”§ ä¿®å¤: è¡¨æ ¼é€‰æ‹©äº‹ä»¶ - ä½¿ç”¨ SelectData + é”™è¯¯å¤„ç†
            try:
                review_dataframe.select(
                    preview_segment,
                    outputs=[preview_audio, preview_status, preview_info, preview_text]
                )
            except Exception as e:
                print(f"âš ï¸ è¡¨æ ¼é€‰æ‹©äº‹ä»¶ç»‘å®šå¤±è´¥: {e}")
                # å¦‚æœç»‘å®šå¤±è´¥,æ·»åŠ ä¸€ä¸ªæ›¿ä»£æ–¹æ¡ˆ
                gr.Markdown("""
                âš ï¸ **ç‰‡æ®µé¢„è§ˆåŠŸèƒ½åˆå§‹åŒ–å¤±è´¥**
                
                å¯èƒ½åŸå› :
                - Gradio ç‰ˆæœ¬ä¸å…¼å®¹
                - è¡¨æ ¼æ•°æ®æ ¼å¼é—®é¢˜
                
                è§£å†³æ–¹æ¡ˆ:
                1. ç¡®ä¿ Gradio >= 4.0
                2. æ£€æŸ¥è¡¨æ ¼æ˜¯å¦æœ‰æ•°æ®
                3. æŸ¥çœ‹æ§åˆ¶å°é”™è¯¯æ—¥å¿—
                """)

            # æ­¥éª¤3: æœ€ç»ˆåˆæˆ
            final_btn.click(
                step3_final_synthesis,
                outputs=[zh_srt_output, zh_en_ass_output, final_video_output, final_status]
            )

        # ========== ä½¿ç”¨è¯´æ˜ ========== #
        with gr.Tab("ğŸ“š V2 ä½¿ç”¨æŒ‡å— + ä¿®å¤è¯´æ˜"):
            gr.Markdown("""
            ## ğŸ”§ æœ¬æ¬¡ä¿®å¤å†…å®¹
            
            ### ä¿®å¤çš„é—®é¢˜
            1. âŒ **ç‰‡æ®µé¢„è§ˆæ— æ³•æ’­æ”¾**: ç‚¹å‡»è¡¨æ ¼è¡ŒåéŸ³é¢‘æ— æ³•åŠ è½½
            2. âŒ **éŸ³é¢‘çŠ¶æ€ä¸å‡†ç¡®**: æ˜æ˜å·²ç¼“å­˜ä½†æ˜¾ç¤º"æœªç”Ÿæˆ"
            3. âŒ **æœ€ç»ˆåˆæˆå¤±è´¥**: æç¤ºéŸ³é¢‘æœªç”Ÿæˆ,æ— æ³•åˆæˆè§†é¢‘
            4. âŒ **ä¼šè¯çŠ¶æ€ä¸¢å¤±**: åˆ·æ–°åéŸ³é¢‘ç¼“å­˜ä¿¡æ¯ä¸¢å¤±
            
            ### æ ¹æœ¬åŸå› 
            ```python
            # é—®é¢˜1: éŸ³é¢‘ç‰‡æ®µæœªåŠ è½½åˆ°ä¼šè¯
            # æ­¥éª¤1å®Œæˆå,è™½ç„¶ç£ç›˜æœ‰ç¼“å­˜,ä½† current_session.audio_segments ä¸ºç©º
            
            # é—®é¢˜2: é¢„è§ˆåŠŸèƒ½åªä»å†…å­˜è¯»å–
            audio_seg = current_session.audio_segments.get(idx)
            # å¦‚æœå†…å­˜æ²¡æœ‰,ç›´æ¥è¿”å› None,ä¸å°è¯•ä»ç£ç›˜åŠ è½½
            
            # é—®é¢˜3: Gradio äº‹ä»¶ç»‘å®šé”™è¯¯
            review_dataframe.select(
                preview_segment,
                inputs=[],  # âŒ ç©ºè¾“å…¥,æ— æ³•è·å–é€‰ä¸­è¡Œ
                outputs=[...]
            )
            ```
            
            ### ä¿®å¤æ–¹æ¡ˆ
            
            #### 1. æ­¥éª¤1åè‡ªåŠ¨åŠ è½½ç¼“å­˜éŸ³é¢‘
            ```python
            # æ–°å¢å‡½æ•°: _load_cached_audio_segments()
            def step1_generate_and_check_v2(...):
                # ... ç”Ÿæˆå­—å¹• ...
                
                # ğŸ”§ å…³é”®ä¿®å¤: åŠ è½½å·²ç¼“å­˜çš„éŸ³é¢‘ç‰‡æ®µ
                progress(0.95, "æ£€æŸ¥éŸ³é¢‘ç¼“å­˜...")
                current_session.audio_segments = _load_cached_audio_segments(
                    current_session.video,
                    current_session.translated_subtitle
                )
                
                # æ›´æ–°å®¡æ ¸çŠ¶æ€
                for idx in range(len(segments)):
                    audio_exists = idx in current_session.audio_segments
                    status[idx] = SegmentReviewStatus(
                        audio_approved=audio_exists,  # æ­£ç¡®åæ˜ éŸ³é¢‘çŠ¶æ€
                        needs_regeneration=not audio_exists
                    )
            ```
            
            #### 2. é¢„è§ˆåŠŸèƒ½æ”¯æŒç£ç›˜åŠ è½½
            ```python
            def preview_segment(evt: gr.SelectData):  # ğŸ”§ ä½¿ç”¨ SelectData
                selected_row_index = evt.index[0]  # è·å–è¡Œç´¢å¼•
                
                # å…ˆä»å†…å­˜è·å–
                audio_seg = current_session.audio_segments.get(idx)
                
                # ğŸ”§ å¦‚æœå†…å­˜æ²¡æœ‰,å°è¯•ä»ç£ç›˜åŠ è½½
                if not audio_seg:
                    audio_seg = audio_segment_repo.load_segment(
                        segment_index=idx,
                        video_path=current_session.video.path,
                        text_segment=text_seg
                    )
                    
                    # åŠ è½½æˆåŠŸåæ›´æ–°åˆ°ä¼šè¯
                    if audio_seg:
                        current_session.audio_segments[idx] = audio_seg
                
                return str(audio_seg.file_path), ...
            ```
            
            #### 3. æ­£ç¡®çš„ Gradio äº‹ä»¶ç»‘å®š
            ```python
            # âŒ é”™è¯¯å†™æ³•
            review_dataframe.select(
                preview_segment,
                inputs=[],  # æ— æ³•è·å–é€‰ä¸­ä¿¡æ¯
                outputs=[...]
            )
            
            # âœ… æ­£ç¡®å†™æ³•
            review_dataframe.select(
                preview_segment,  # å‡½æ•°è‡ªåŠ¨æ¥æ”¶ SelectData å‚æ•°
                outputs=[preview_audio, preview_status, ...]
            )
            ```
            
            #### 4. æ­¥éª¤2 éŸ³é¢‘ç”Ÿæˆåæ›´æ–°ä¼šè¯
            ```python
            def step2_incremental_voice_cloning(...):
                # ... æ‰§è¡Œåˆæˆ ...
                
                # ğŸ”§ ç¡®ä¿æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µéƒ½æ›´æ–°åˆ°ä¼šè¯
                for audio_seg in result.audio_segments:
                    current_session.audio_segments[audio_seg.segment_index] = audio_seg
                    
                    # åŒæ—¶æ›´æ–°å®¡æ ¸çŠ¶æ€
                    current_session.segment_review_status[idx] = SegmentReviewStatus(
                        audio_approved=True,  # æ ‡è®°éŸ³é¢‘å·²å®Œæˆ
                        needs_regeneration=False
                    )
            ```
            
            ### éªŒè¯ä¿®å¤æ•ˆæœ
            
            #### æµ‹è¯•æ­¥éª¤
            ```bash
            # 1. ä¸Šä¼ è§†é¢‘,å®Œæˆæ­¥éª¤1
            âœ… æ£€æŸ¥æ§åˆ¶å°è¾“å‡º:
                "ğŸ” æ£€æŸ¥éŸ³é¢‘ç‰‡æ®µç¼“å­˜:"
                "âœ… å…±åŠ è½½ X/Y ä¸ªç¼“å­˜ç‰‡æ®µ"
            
            # 2. æŸ¥çœ‹å®¡æ ¸è¡¨æ ¼
            âœ… "éŸ³é¢‘"åˆ—åº”æ˜¾ç¤º:
                - "âœ… å·²ç¼“å­˜" (å¦‚æœç£ç›˜æœ‰ç¼“å­˜)
                - "æœªç”Ÿæˆ" (å¦‚æœéœ€è¦ç”Ÿæˆ)
            
            # 3. ç‚¹å‡»è¡¨æ ¼ä¸­ä»»æ„è¡Œ
            âœ… å¦‚æœéŸ³é¢‘å·²ç¼“å­˜:
                - å·¦ä¾§éŸ³é¢‘æ’­æ”¾å™¨å‡ºç°æ³¢å½¢
                - çŠ¶æ€æ˜¾ç¤º "âœ… éŸ³é¢‘å·²ç”Ÿæˆ"
                - å³ä¾§æ˜¾ç¤ºç‰‡æ®µä¿¡æ¯å’Œå­—å¹•æ–‡æœ¬
            
            # 4. ç‚¹å‡»"å¼€å§‹å¢é‡è¯­éŸ³å…‹éš†"
            âœ… ç”Ÿæˆè¿‡ç¨‹ä¸­:
                - è¡¨æ ¼å®æ—¶æ›´æ–° "éŸ³é¢‘"åˆ—
                - æ–°ç”Ÿæˆçš„ç‰‡æ®µå¯ç«‹å³é¢„è§ˆ
            
            # 5. ç‚¹å‡»"ç”Ÿæˆæœ€ç»ˆè§†é¢‘"
            âœ… ä¸å†æç¤º "éŸ³é¢‘æœªç”Ÿæˆ"
            âœ… æˆåŠŸåˆæˆå®Œæ•´è§†é¢‘
            ```
            
            #### ç¼“å­˜æ–‡ä»¶æ£€æŸ¥
            ```bash
            # æŸ¥çœ‹éŸ³é¢‘ç‰‡æ®µç¼“å­˜
            ls -lh .cache/audio_segments/video_name_*/
            
            # åº”è¯¥çœ‹åˆ°:
            seg_0000.wav
            seg_0000.json
            seg_0001.wav
            seg_0001.json
            ...
            ```
            
            ### æ•°æ®æµå›¾
            
            ```
            ç£ç›˜ç¼“å­˜                ä¼šè¯å†…å­˜               UIæ˜¾ç¤º
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            
            æ­¥éª¤1å®Œæˆå:
            .cache/audio_segments/  â†’  audio_segments  â†’  è¡¨æ ¼"âœ…å·²ç¼“å­˜"
                seg_0000.wav           {0: AudioSeg}       é¢„è§ˆå¯æ’­æ”¾
                seg_0001.wav           {1: AudioSeg}
            
            æ­¥éª¤2ç”Ÿæˆæ–°ç‰‡æ®µ:
            .cache/audio_segments/  â†  audio_segments  â†  TTSç”Ÿæˆ
                seg_0002.wav        â†  {2: AudioSeg}  â†  ç«‹å³æ›´æ–°
                                       
            ç‚¹å‡»é¢„è§ˆ:
            .cache/audio_segments/  â†’  audio_segments  â†’  éŸ³é¢‘æ’­æ”¾å™¨
                seg_0002.wav           è¯»å–file_path       åŠ è½½éŸ³é¢‘
            
            æ­¥éª¤3åˆæˆ:
            audio_segments  â†’  åˆå¹¶æ‰€æœ‰ç‰‡æ®µ  â†’  å®Œæ•´éŸ³è½¨  â†’  æœ€ç»ˆè§†é¢‘
            {0,1,2,...}
            ```
            
            ### æ€§èƒ½ä¼˜åŒ–
            
            #### æ‡’åŠ è½½ç­–ç•¥
            - **æ­¥éª¤1**: åªåŠ è½½å…ƒæ•°æ®,ä¸åŠ è½½éŸ³é¢‘æ•°æ®
            - **é¢„è§ˆæ—¶**: æŒ‰éœ€åŠ è½½éŸ³é¢‘æ–‡ä»¶
            - **æ­¥éª¤3**: æ‰¹é‡è¯»å–æ‰€æœ‰éŸ³é¢‘
            
            #### å†…å­˜ç®¡ç†
            ```python
            # AudioSegment åªå­˜å‚¨æ–‡ä»¶è·¯å¾„,ä¸å­˜å‚¨åŸå§‹éŸ³é¢‘æ•°æ®
            @dataclass(frozen=True)
            class AudioSegment:
                file_path: Path  # åªå­˜è·¯å¾„
                # samples: tuple  # ä¸åœ¨å†…å­˜ä¸­ä¿å­˜
            
            # æ’­æ”¾æ—¶æ‰è¯»å–æ–‡ä»¶
            audio_path = str(audio_seg.file_path)
            gr.Audio(value=audio_path)  # Gradio ä»æ–‡ä»¶è¯»å–
            ```
            
            ### å¸¸è§é—®é¢˜æ’æŸ¥
            
            #### Q: æ­¥éª¤1åè¡¨æ ¼æ˜¾ç¤º"æœªç”Ÿæˆ",ä½†ç£ç›˜æœ‰ç¼“å­˜?
            ```python
            # æ£€æŸ¥ _load_cached_audio_segments() æ˜¯å¦è¢«è°ƒç”¨
            print(f"âœ… å…±åŠ è½½ {len(cached_segments)} ä¸ªç¼“å­˜ç‰‡æ®µ")
            
            # å¦‚æœæœªè¾“å‡º,è¯´æ˜å‡½æ•°æœªæ‰§è¡Œ,æ£€æŸ¥æ­¥éª¤1ä»£ç 
            ```
            
            #### Q: ç‚¹å‡»è¡¨æ ¼è¡Œæ²¡æœ‰ååº”?
            ```python
            # æ£€æŸ¥äº‹ä»¶ç»‘å®š
            review_dataframe.select(
                preview_segment,  # âœ… æ­£ç¡®
                outputs=[...]
            )
            
            # ä¸è¦å†™æˆ:
            review_dataframe.select(
                preview_segment,
                inputs=[],  # âŒ é”™è¯¯
                outputs=[...]
            )
            ```
            
            #### Q: é¢„è§ˆæ—¶æç¤º"æ–‡ä»¶ä¸å­˜åœ¨"?
            ```python
            # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
            if audio_seg.file_path and audio_seg.file_path.exists():
                return str(audio_seg.file_path)
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {audio_seg.file_path}")
            ```
            
            #### Q: æ­¥éª¤3æç¤º"éŸ³é¢‘æœªç”Ÿæˆ"?
            ```python
            # æ£€æŸ¥ä¼šè¯çŠ¶æ€
            print(f"å†…å­˜ä¸­éŸ³é¢‘ç‰‡æ®µ: {len(current_session.audio_segments)}")
            print(f"æ€»ç‰‡æ®µæ•°: {len(current_session.translated_subtitle.segments)}")
            
            # å¦‚æœæ•°é‡ä¸åŒ¹é…,è¯´æ˜æ­¥éª¤2æœªæ­£ç¡®æ›´æ–°ä¼šè¯
            ```
            
            ### æ€»ç»“
            
            æœ¬æ¬¡ä¿®å¤é€šè¿‡**ç»Ÿä¸€ç£ç›˜ç¼“å­˜ä¸å†…å­˜çŠ¶æ€**,ç¡®ä¿:
            - âœ… æ­¥éª¤1åè‡ªåŠ¨åŠ è½½å·²ç¼“å­˜éŸ³é¢‘
            - âœ… é¢„è§ˆåŠŸèƒ½æ”¯æŒç£ç›˜+å†…å­˜åŒé‡æŸ¥æ‰¾
            - âœ… æ­¥éª¤2ç”Ÿæˆåç«‹å³æ›´æ–°ä¼šè¯
            - âœ… æ­¥éª¤3èƒ½æ­£ç¡®è·å–æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
            
            æ ¸å¿ƒæ€æƒ³: **ç£ç›˜æ˜¯çœŸç›¸,å†…å­˜æ˜¯ç¼“å­˜**
            - ç£ç›˜: æŒä¹…åŒ–å­˜å‚¨,æ–­ç‚¹ç»­ä¼ 
            - å†…å­˜: å¿«é€Ÿè®¿é—®,ä¼šè¯ç®¡ç†
            - åŒæ­¥: åŒå‘æ›´æ–°,ä¿æŒä¸€è‡´
            """)

    return demo


def main():
    """å¯åŠ¨ WebUI V2"""
    demo = build_ui_v2()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        inbrowser=True,
        share=False
    )


if __name__ == "__main__":
    main()