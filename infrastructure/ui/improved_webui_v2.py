"""
Infrastructure Layer - é‡æ„åçš„WebUI V2ï¼ˆå®Œæ•´ç‰ˆï¼‰

âœ… éœ€æ±‚1: æ”¯æŒå¯é…ç½®çš„å‚è€ƒéŸ³é¢‘èµ·å§‹åç§»
âœ… éœ€æ±‚2: æ”¯æŒåŒè¯­å­—å¹•å¯é€‰
âœ… éœ€æ±‚3: ä½¿ç”¨AudioFileRepositoryç®¡ç†å‚è€ƒéŸ³é¢‘
"""

from pathlib import Path
from typing import Optional, Dict

import gradio as gr

from application.use_cases.incremental_voice_cloning import (
    incremental_voice_cloning_use_case,
    regenerate_modified_segments_use_case
)
from domain.entities import (
    Video, Subtitle, LanguageCode,
    AudioSegment, SegmentReviewStatus
)
from infrastructure.config.dependency_injection import container

# åˆå§‹åŒ–ä»“å‚¨
audio_segment_repo = container.audio_segment_repo
audio_file_repo = container.audio_file_repo  # âœ… ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶ä»“å‚¨
cache_service = container.cache_service


# ============== ä¼šè¯çŠ¶æ€ ============== #

class TranslationSessionV2:
    """ç¿»è¯‘ä¼šè¯çŠ¶æ€"""

    def __init__(self):
        self.translation_context = None
        self.video: Optional[Video] = None
        self.original_subtitle: Optional[Subtitle] = None
        self.translated_subtitle: Optional[Subtitle] = None
        self.english_subtitle: Optional[Subtitle] = None
        self.detected_language: Optional[LanguageCode] = None
        self.source_language: Optional[LanguageCode] = None
        self.quality_report = None
        self.audio_segments: Dict[int, AudioSegment] = {}
        self.segment_review_status: Dict[int, SegmentReviewStatus] = {}
        self.edited_segments: Dict[int, str] = {}
        self.modified_indices: set[int] = set()
        self.reference_audio_path: Optional[Path] = None
        self.approved = False
        # âœ… æ–°å¢ï¼šTTS é…ç½®
        self.length_penalty: float = 0.0  # length_penalty å‚æ•°

        # âœ… æ–°å¢ï¼šæ—¶é•¿ç»Ÿè®¡
        self.duration_stats: Dict[int, dict] = {}  # {idx: {target, actual, error, ratio}}


current_session = TranslationSessionV2()


# ============== è¾…åŠ©å‡½æ•° ============== #

def _load_cached_audio_segments(video: Video, subtitle: Subtitle) -> Dict[int, AudioSegment]:
    """ä»ç£ç›˜åŠ è½½å·²ç¼“å­˜çš„éŸ³é¢‘ç‰‡æ®µ"""
    cached_segments = {}

    for idx, text_seg in enumerate(subtitle.segments):
        try:
            audio_seg = audio_segment_repo.load_segment(
                segment_index=idx,
                video_path=video.path,
                text_segment=text_seg
            )
            if audio_seg:
                cached_segments[idx] = audio_seg
        except Exception:
            continue

    return cached_segments


def _source_language_cache_format(source_language: str) -> Optional[LanguageCode]:
    """è½¬æ¢æºè¯­è¨€æ ¼å¼"""
    return LanguageCode(source_language) if source_language != "auto" else None


def _prepare_review_data_v2():
    """å‡†å¤‡å®¡æ ¸æ•°æ®"""
    if not current_session.translated_subtitle:
        return None

    data = []
    for idx, (orig_seg, trans_seg) in enumerate(
        zip(current_session.original_subtitle.segments,
            current_session.translated_subtitle.segments)
    ):
        en_text = (
            current_session.english_subtitle.segments[idx].text
            if current_session.english_subtitle
               and idx < len(current_session.english_subtitle.segments)
            else orig_seg.text
        )

        audio_seg = current_session.audio_segments.get(idx)
        if audio_seg:
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            audio_status = "âœ… å·²ç¼“å­˜"
            duration_str = f"{actual_duration:.2f}s"
        else:
            audio_status = "æœªç”Ÿæˆ"
            duration_str = "-"

        data.append([
            idx,
            f"{trans_seg.time_range.start_seconds:.2f}s",
            en_text,
            trans_seg.text,
            f"{trans_seg.time_range.duration:.2f}s",
            duration_str,
            audio_status,
            "",
            "â³ å¾…å®¡æ ¸"
        ])

    return data


def _apply_edits_to_subtitle_v2():
    """åº”ç”¨ç¼–è¾‘åˆ°å­—å¹•å¯¹è±¡"""
    if not current_session.edited_segments:
        return

    from domain.entities import TextSegment

    new_segments = []
    for idx, seg in enumerate(current_session.translated_subtitle.segments):
        if idx in current_session.edited_segments:
            new_seg = TextSegment(
                text=current_session.edited_segments[idx],
                time_range=seg.time_range,
                language=seg.language
            )
            new_segments.append(new_seg)
        else:
            new_segments.append(seg)

    current_session.translated_subtitle = Subtitle(
        segments=tuple(new_segments),
        language=current_session.translated_subtitle.language
    )


def get_video_duration(video_path: Path) -> float:
    """è·å–è§†é¢‘æ—¶é•¿"""
    import subprocess
    result = subprocess.run([
        'ffprobe', '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        str(video_path)
    ], capture_output=True, text=True)
    return float(result.stdout.strip())


# ============== æ­¥éª¤1: ç”Ÿæˆå­—å¹• ============== #

def step1_generate_and_check_v2(
    video_file,
    whisper_model: str,
    translation_model: str,
    translation_context_name: str,
    source_language: str,
    progress=gr.Progress()
):
    """æ­¥éª¤1: ç”Ÿæˆå­—å¹•"""
    if not video_file:
        return None, "âŒ è¯·ä¸Šä¼ è§†é¢‘", gr.update(visible=False)

    try:
        global current_session
        current_session = TranslationSessionV2()

        video_path = Path(video_file.name)
        current_session.video = Video(
            path=video_path,
            duration=get_video_duration(video_path),
            has_audio=True
        )

        translation_context = container.translator_context_repo.load(
            translation_context_name
        )

        src_lang = _source_language_cache_format(source_language)

        progress(0.1, "æ£€æŸ¥ç¼“å­˜...")
        cached_result = cache_service.load_subtitle_cache(
            video_path=video_path,
            source_language=src_lang,
            context_domain=translation_context.domain if translation_context else None
        )

        if cached_result:
            current_session.original_subtitle = cached_result["original_subtitle"]
            current_session.translated_subtitle = cached_result["chinese_subtitle"]
            current_session.english_subtitle = cached_result["english_subtitle"]
            current_session.detected_language = cached_result["detected_language"]
            current_session.source_language = src_lang
            current_session.translation_context = translation_context

            status_report = f"""
âœ… å­—å¹•ç¼“å­˜å‘½ä¸­

ğŸ“Š åŸºæœ¬ä¿¡æ¯:
   è§†é¢‘: {video_path.name}
   æ£€æµ‹è¯­è¨€: {cached_result['detected_language'].value}
   æ€»ç‰‡æ®µæ•°: {len(cached_result['chinese_subtitle'].segments)}
"""
            review_data = _prepare_review_data_v2()
            return review_data, status_report, gr.update(visible=True)

        progress(0.2, "ç”Ÿæˆå­—å¹•...")

        from application.use_cases.improved_generate_subtitles import improved_generate_subtitles_use_case

        result = improved_generate_subtitles_use_case(
            video=current_session.video,
            asr_provider=container.get_asr(whisper_model),
            translation_provider=container.get_translator(),
            video_processor=container.video_processor,
            cache_repo=container.cache_repo,
            translation_context=translation_context,
            target_language=LanguageCode.CHINESE,
            source_language=src_lang,
            enable_quality_check=True,
            progress=lambda p, d: progress(p, d)
        )

        container.get_translator().unload()

        current_session.original_subtitle = result.original_subtitle
        current_session.translated_subtitle = result.translated_subtitle
        current_session.detected_language = result.detected_language
        current_session.translation_context = translation_context
        current_session.source_language = src_lang

        cached_result = cache_service.load_subtitle_cache(
            video_path=video_path,
            source_language=src_lang,
            context_domain=translation_context.domain if translation_context else None
        )

        if cached_result and cached_result["english_subtitle"]:
            current_session.english_subtitle = cached_result["english_subtitle"]

        status_report = f"""
âœ… å­—å¹•ç”Ÿæˆå®Œæˆ

ğŸ“Š åŸºæœ¬ä¿¡æ¯:
   è§†é¢‘: {video_path.name}
   æ£€æµ‹è¯­è¨€: {result.detected_language.value}
   æ€»ç‰‡æ®µæ•°: {len(result.translated_subtitle.segments)}
"""

        review_data = _prepare_review_data_v2()
        return review_data, status_report, gr.update(visible=True)

    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, error_msg, gr.update(visible=False)


# ============== æ­¥éª¤2A: å¢é‡è¯­éŸ³å…‹éš† ============== #

def step2_incremental_voice_cloning(
    reference_audio_file,
    ref_audio_duration: float,  # âœ… æ–°å¢å‚æ•°
    ref_audio_start_offset: float,  # âœ… æ–°å¢å‚æ•°
    progress=gr.Progress()
):
    """æ­¥éª¤2A: å¢é‡è¯­éŸ³å…‹éš†ï¼ˆé‡æ„ç‰ˆï¼‰"""
    global current_session

    if not current_session.video or not current_session.translated_subtitle:
        return "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±", gr.update()

    try:
        # âœ… ä¿®å¤: å‡†å¤‡å‚è€ƒéŸ³é¢‘ï¼ˆä½¿ç”¨AudioFileRepositoryï¼‰
        if reference_audio_file:
            # ç”¨æˆ·ä¸Šä¼ äº†å‚è€ƒéŸ³é¢‘
            progress(0.05, "ä¿å­˜å‚è€ƒéŸ³é¢‘...")

            # ä½¿ç”¨AudioFileRepositoryæŒä¹…åŒ–Gradioä¸´æ—¶æ–‡ä»¶
            ref_audio_path = audio_file_repo.save_reference_audio(
                video_path=current_session.video.path,
                source_audio_path=Path(reference_audio_file.name)
            )
            current_session.reference_audio_path = ref_audio_path
            print(f"ğŸ“ ä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„å‚è€ƒéŸ³é¢‘: {ref_audio_path}")

        else:
            # å…ˆå°è¯•åŠ è½½å·²å­˜åœ¨çš„å‚è€ƒéŸ³é¢‘
            existing_ref_audio = audio_file_repo.load_reference_audio(
                current_session.video.path
            )

            if existing_ref_audio and existing_ref_audio.exists():
                ref_audio_path = existing_ref_audio
                current_session.reference_audio_path = ref_audio_path
                print(f"ğŸ“ å¤ç”¨å·²æœ‰å‚è€ƒéŸ³é¢‘: {ref_audio_path}")
            else:
                # ä»è§†é¢‘æå–å‚è€ƒéŸ³é¢‘
                progress(0.05, f"ä»è§†é¢‘æå–å‚è€ƒéŸ³é¢‘ï¼ˆåç§»: {ref_audio_start_offset}s, æ—¶é•¿: {ref_audio_duration}sï¼‰...")

                temp_ref_audio = container.video_processor.extract_reference_audio(
                    video=current_session.video,
                    duration=ref_audio_duration,
                    start_offset=ref_audio_start_offset  # âœ… ä½¿ç”¨å¯é…ç½®çš„åç§»
                )

                # æŒä¹…åŒ–æå–çš„éŸ³é¢‘
                ref_audio_path = audio_file_repo.save_reference_audio(
                    video_path=current_session.video.path,
                    source_audio_path=temp_ref_audio
                )
                current_session.reference_audio_path = ref_audio_path

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_ref_audio.exists():
                    temp_ref_audio.unlink()

                print(f"ğŸ“ ä»è§†é¢‘æå–å‚è€ƒéŸ³é¢‘: {ref_audio_path}")

        # å®æ—¶è¿›åº¦å›è°ƒ
        def segment_progress(ratio, msg, idx, audio_seg):
            progress(ratio, msg)
            if audio_seg:
                current_session.audio_segments[idx] = audio_seg
                status = current_session.segment_review_status.get(idx)
                if status and not status.subtitle_modified:
                    current_session.segment_review_status[idx] = SegmentReviewStatus(
                        segment_index=idx,
                        subtitle_approved=False,
                        audio_approved=True,
                        subtitle_modified=False,
                        needs_regeneration=False
                    )

        # æ‰§è¡Œå¢é‡åˆæˆ
        result = incremental_voice_cloning_use_case(
            video=current_session.video,
            subtitle=current_session.translated_subtitle,
            tts_provider=container.get_tts(),
            video_processor=container.video_processor,
            audio_repo=audio_segment_repo,
            cache_repo=container.cache_repo,
            reference_audio_path=ref_audio_path,
            progress=segment_progress
        )

        # æ›´æ–°æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µåˆ°ä¼šè¯
        for audio_seg in result.audio_segments:
            current_session.audio_segments[audio_seg.segment_index] = audio_seg

        status = f"""
âœ… å¢é‡è¯­éŸ³å…‹éš†å®Œæˆ!

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   æ€»ç‰‡æ®µæ•°: {result.total_segments}
   ç¼“å­˜å‘½ä¸­: {result.cached_segments}
   æ–°ç”Ÿæˆ: {result.regenerated_segments}
   è€—æ—¶: {result.synthesis_time:.1f} ç§’

ğŸ“ å‚è€ƒéŸ³é¢‘: {ref_audio_path.name}
   èµ·å§‹åç§»: {ref_audio_start_offset}s
   æ—¶é•¿: {ref_audio_duration}s

ğŸ’¡ æç¤º: 
   - å‚è€ƒéŸ³é¢‘å·²æŒä¹…åŒ–ï¼Œä¿®æ”¹å­—å¹•åå¯å®‰å…¨é‡æ–°ç”Ÿæˆ
   - ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡ŒæŸ¥çœ‹å’Œæ’­æ”¾éŸ³é¢‘
"""

        updated_data = _prepare_review_data_v2()
        return status, gr.update(value=updated_data)

    except Exception as e:
        import traceback
        error_msg = f"âŒ è¯­éŸ³å…‹éš†å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return error_msg, gr.update()


# ============== æ­¥éª¤2B: ä¿å­˜ç¼–è¾‘ ============== #

def step2_save_edits_and_regenerate(review_dataframe):
    """ä¿å­˜ç¼–è¾‘å¹¶æ ‡è®°éœ€è¦é‡æ–°ç”Ÿæˆçš„ç‰‡æ®µ"""
    global current_session

    if hasattr(review_dataframe, "values"):
        review_dataframe = review_dataframe.values.tolist()

    if not review_dataframe:
        return "âš ï¸ æ²¡æœ‰å¯ä¿å­˜çš„ä¿®æ”¹", gr.update()

    if review_dataframe and isinstance(review_dataframe[0][0], str):
        review_dataframe = review_dataframe[1:]

    edited_count = 0

    for row in review_dataframe:
        try:
            idx = int(row[0])
        except (ValueError, IndexError):
            continue

        if idx >= len(current_session.translated_subtitle.segments):
            continue

        original_text = current_session.translated_subtitle.segments[idx].text
        edited_text = row[3]

        if edited_text != original_text:
            current_session.edited_segments[idx] = edited_text
            current_session.modified_indices.add(idx)
            edited_count += 1

    if edited_count:
        _apply_edits_to_subtitle_v2()

        cache_service.update_chinese_subtitle(
            video_path=current_session.video.path,
            updated_subtitle=current_session.translated_subtitle,
            source_language=current_session.source_language,
            context_domain=current_session.translation_context.domain
            if current_session.translation_context else None
        )

        cache_service.invalidate_downstream_caches(
            video_path=current_session.video.path,
            detected_language=current_session.detected_language
        )

        updated_data = _prepare_review_data_v2()

        return (
            f"âœ… å·²ä¿å­˜ {edited_count} å¤„ä¿®æ”¹ï¼ˆå·²åŒæ­¥åˆ°ç¼“å­˜ï¼‰\n"
            f"âš ï¸ éœ€è¦é‡æ–°ç”Ÿæˆ {len(current_session.modified_indices)} ä¸ªéŸ³é¢‘ç‰‡æ®µ",
            gr.update(value=updated_data)
        )
    else:
        return "â„¹ï¸ æœªæ£€æµ‹åˆ°ä¿®æ”¹", gr.update()


# ============== âœ… æ–°å¢ï¼šæ—¶é•¿åˆ†æå·¥å…· ============== #

def calculate_duration_statistics() -> dict:
    """
    è®¡ç®—æ—¶é•¿ç»Ÿè®¡æ•°æ®

    Returns:
        {
            "total_segments": int,
            "analyzed_segments": int,
            "avg_error": float,
            "max_error": float,
            "over_limit_count": int,
            "over_limit_indices": list[int]
        }
    """
    if not current_session.translated_subtitle:
        return {
            "total_segments": 0,
            "analyzed_segments": 0,
            "avg_error": 0.0,
            "max_error": 0.0,
            "over_limit_count": 0,
            "over_limit_indices": []
        }

    total = len(current_session.translated_subtitle.segments)
    analyzed = 0
    errors = []
    over_limit_indices = []

    for idx, text_seg in enumerate(current_session.translated_subtitle.segments):
        audio_seg = current_session.audio_segments.get(idx)

        if audio_seg:
            target_duration = text_seg.time_range.duration
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            error = actual_duration - target_duration
            ratio = actual_duration / target_duration if target_duration > 0 else 0

            # ä¿å­˜ç»Ÿè®¡
            current_session.duration_stats[idx] = {
                "target": target_duration,
                "actual": actual_duration,
                "error": error,
                "ratio": ratio
            }

            analyzed += 1
            errors.append(abs(error))

            # æ£€æŸ¥æ˜¯å¦è¶…é™ï¼ˆè¶…è¿‡ç›®æ ‡æ—¶é•¿ï¼‰
            if error > 0.1:  # è¶…è¿‡ 0.1 ç§’ç®—è¶…é™
                over_limit_indices.append(idx)

    avg_error = sum(errors) / len(errors) if errors else 0.0
    max_error = max(errors) if errors else 0.0

    return {
        "total_segments": total,
        "analyzed_segments": analyzed,
        "avg_error": avg_error,
        "max_error": max_error,
        "over_limit_count": len(over_limit_indices),
        "over_limit_indices": over_limit_indices
    }


def _prepare_review_data_v2(filter_over_limit: bool = False):
    """
    å‡†å¤‡å®¡æ ¸æ•°æ®ï¼ˆæ”¯æŒç­›é€‰ï¼‰

    Args:
        filter_over_limit: æ˜¯å¦åªæ˜¾ç¤ºè¶…é™ç‰‡æ®µ
    """
    if not current_session.translated_subtitle:
        return None

    data = []
    for idx, (orig_seg, trans_seg) in enumerate(
            zip(current_session.original_subtitle.segments,
                current_session.translated_subtitle.segments)
    ):
        # è·å–è‹±æ–‡å­—å¹•
        en_text = (
            current_session.english_subtitle.segments[idx].text
            if current_session.english_subtitle
               and idx < len(current_session.english_subtitle.segments)
            else orig_seg.text
        )

        # è·å–éŸ³é¢‘ä¿¡æ¯
        audio_seg = current_session.audio_segments.get(idx)
        target_duration = trans_seg.time_range.duration

        if audio_seg:
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            duration_error = actual_duration - target_duration
            duration_ratio = (actual_duration / target_duration * 100) if target_duration > 0 else 0

            audio_status = "âœ… å·²ç”Ÿæˆ"
            duration_str = f"{actual_duration:.2f}s"

            # âœ… æ—¶é•¿çŠ¶æ€æ ‡è®°
            if duration_error > 0.5:
                duration_status = f"âš ï¸ è¶…æ—¶ {duration_error:.2f}s ({duration_ratio:.0f}%)"
            elif duration_error > 0.1:
                duration_status = f"âš¡ ç•¥è¶… {duration_error:.2f}s ({duration_ratio:.0f}%)"
            elif duration_error < -0.5:
                duration_status = f"ğŸ“‰ è¿‡çŸ­ {duration_error:.2f}s ({duration_ratio:.0f}%)"
            else:
                duration_status = f"âœ… æ­£å¸¸ ({duration_ratio:.0f}%)"

            # âœ… ç­›é€‰é€»è¾‘
            if filter_over_limit and duration_error <= 0.1:
                continue  # è·³è¿‡æ­£å¸¸ç‰‡æ®µ
        else:
            audio_status = "æœªç”Ÿæˆ"
            duration_str = "-"
            duration_status = "â³ å¾…ç”Ÿæˆ"

            if filter_over_limit:
                continue  # è·³è¿‡æœªç”Ÿæˆç‰‡æ®µ

        data.append([
            idx,
            f"{trans_seg.time_range.start_seconds:.2f}s",
            en_text,
            trans_seg.text,
            f"{target_duration:.2f}s",
            duration_str,
            duration_status,
            audio_status,
            "â³ å¾…å®¡æ ¸"
        ])

    return data
# ============== âœ… æ–°å¢ï¼šæ—¶é•¿ç»Ÿè®¡é¢æ¿ ============== #

def show_duration_statistics():
    """æ˜¾ç¤ºæ—¶é•¿ç»Ÿè®¡ä¿¡æ¯"""
    stats = calculate_duration_statistics()

    if stats["total_segments"] == 0:
        return "ğŸ“Š æš‚æ— æ•°æ®", gr.update(visible=False)

    analyzed_ratio = stats["analyzed_segments"] / stats["total_segments"] * 100

    report = f"""
ğŸ“Š æ—¶é•¿ç»Ÿè®¡æŠ¥å‘Š

**æ€»ä½“æƒ…å†µ:**
   â€¢ æ€»ç‰‡æ®µæ•°: {stats['total_segments']}
   â€¢ å·²ç”Ÿæˆ: {stats['analyzed_segments']} ({analyzed_ratio:.1f}%)
   â€¢ å¾…ç”Ÿæˆ: {stats['total_segments'] - stats['analyzed_segments']}

**æ—¶é•¿ç²¾åº¦:**
   â€¢ å¹³å‡è¯¯å·®: {stats['avg_error']:.3f}s
   â€¢ æœ€å¤§è¯¯å·®: {stats['max_error']:.3f}s
   â€¢ è¶…é™ç‰‡æ®µ: {stats['over_limit_count']} ä¸ª

**å»ºè®®:**
"""

    if stats['avg_error'] < 0.1:
        report += "   âœ… æ—¶é•¿æ§åˆ¶ä¼˜ç§€ï¼"
    elif stats['avg_error'] < 0.3:
        report += "   âœ… æ—¶é•¿æ§åˆ¶è‰¯å¥½"
    elif stats['avg_error'] < 0.5:
        report += "   âš ï¸  æ—¶é•¿è¯¯å·®è¾ƒå¤§ï¼Œå»ºè®®è°ƒæ•´ length_penalty"
    else:
        report += "   âŒ æ—¶é•¿ä¸¥é‡è¶…é™ï¼Œè¯·è°ƒæ•´ length_penalty å¹¶é‡æ–°ç”Ÿæˆ"

    if stats['over_limit_count'] > 0:
        report += f"\n   ğŸ’¡ å‘ç° {stats['over_limit_count']} ä¸ªè¶…é™ç‰‡æ®µï¼Œå¯ä½¿ç”¨ç­›é€‰åŠŸèƒ½æŸ¥çœ‹"

    return report, gr.update(visible=True)


# ============== âœ… æ–°å¢ï¼šlength_penalty è°ƒèŠ‚åŠŸèƒ½ ============== #

def update_length_penalty(length_penalty_value: float):
    """æ›´æ–° length_penalty å‚æ•°"""
    current_session.length_penalty = length_penalty_value

    # æ›´æ–°åˆ° TTS Provider
    tts = container.get_tts()
    if hasattr(tts, 'update_config'):
        tts.update_config(length_penalty=length_penalty_value)

    return f"âœ… length_penalty å·²æ›´æ–°ä¸º {length_penalty_value:.2f}"


def suggest_length_penalty():
    """æ ¹æ®ç»Ÿè®¡æ•°æ®æ¨è length_penalty å€¼"""
    stats = calculate_duration_statistics()

    if stats['analyzed_segments'] == 0:
        return 0.0, "âš ï¸ æš‚æ— ç»Ÿè®¡æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.0"

    avg_error = stats['avg_error']

    # æ¨èç®—æ³•
    if avg_error < 0.1:
        suggested = 0.0
        reason = "æ—¶é•¿æ§åˆ¶ä¼˜ç§€ï¼Œä¿æŒå½“å‰è®¾ç½®"
    elif avg_error < 0.3:
        suggested = 0.5
        reason = "æ—¶é•¿ç•¥æœ‰è¶…é™ï¼Œå»ºè®®ä½¿ç”¨è½»å¾®æƒ©ç½š"
    elif avg_error < 0.5:
        suggested = 1.0
        reason = "æ—¶é•¿è¶…é™è¾ƒå¤šï¼Œå»ºè®®ä½¿ç”¨ä¸­ç­‰æƒ©ç½š"
    else:
        suggested = 1.5
        reason = "æ—¶é•¿ä¸¥é‡è¶…é™ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå¼ºæƒ©ç½š"

    return suggested, f"ğŸ’¡ å»ºè®®å€¼: {suggested:.1f} ({reason})"
# ============== æ­¥éª¤2C: é‡æ–°ç”Ÿæˆ ============== #
def step2_incremental_voice_cloning(
        reference_audio_file,
        ref_audio_duration: float,
        ref_audio_start_offset: float,
        length_penalty: float,  # âœ… æ–°å¢å‚æ•°
        progress=gr.Progress()
):
    """æ­¥éª¤2A: å¢é‡è¯­éŸ³å…‹éš†ï¼ˆæ”¯æŒ length_penaltyï¼‰"""
    global current_session

    if not current_session.video or not current_session.translated_subtitle:
        return "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±", gr.update(), ""

    try:
        # âœ… ä¿å­˜ length_penalty åˆ°ä¼šè¯
        current_session.length_penalty = length_penalty

        # å‡†å¤‡å‚è€ƒéŸ³é¢‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        if reference_audio_file:
            ref_audio_path = audio_file_repo.save_reference_audio(
                video_path=current_session.video.path,
                source_audio_path=Path(reference_audio_file.name)
            )
            current_session.reference_audio_path = ref_audio_path
        else:
            existing_ref_audio = audio_file_repo.load_reference_audio(
                current_session.video.path
            )
            if existing_ref_audio and existing_ref_audio.exists():
                ref_audio_path = existing_ref_audio
            else:
                temp_ref_audio = container.video_processor.extract_reference_audio(
                    video=current_session.video,
                    duration=ref_audio_duration,
                    start_offset=ref_audio_start_offset
                )
                ref_audio_path = audio_file_repo.save_reference_audio(
                    video_path=current_session.video.path,
                    source_audio_path=temp_ref_audio
                )
                if temp_ref_audio.exists():
                    temp_ref_audio.unlink()

            current_session.reference_audio_path = ref_audio_path

        # âœ… æ›´æ–° TTS é…ç½®
        tts = container.get_tts()
        if hasattr(tts, 'update_config'):
            tts.update_config(length_penalty)

        print(f"  âš™ï¸  length_penalty: {length_penalty}")

        # å®æ—¶è¿›åº¦å›è°ƒ
        def segment_progress(ratio, msg, idx, audio_seg):
            progress(ratio, msg)
            if audio_seg:
                current_session.audio_segments[idx] = audio_seg

        # æ‰§è¡Œå¢é‡åˆæˆ
        result = incremental_voice_cloning_use_case(
            video=current_session.video,
            subtitle=current_session.translated_subtitle,
            tts_provider=container.get_tts(),
            video_processor=container.video_processor,
            audio_repo=audio_segment_repo,
            cache_repo=container.cache_repo,
            reference_audio_path=ref_audio_path,
            progress=segment_progress
        )

        # æ›´æ–°æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µåˆ°ä¼šè¯
        for audio_seg in result.audio_segments:
            current_session.audio_segments[audio_seg.segment_index] = audio_seg

        # âœ… ç”Ÿæˆæ—¶é•¿ç»Ÿè®¡
        stats_report, _ = show_duration_statistics()

        status = f"""
âœ… å¢é‡è¯­éŸ³å…‹éš†å®Œæˆ!

ğŸ“Š åˆæˆç»Ÿè®¡:
   æ€»ç‰‡æ®µæ•°: {result.total_segments}
   ç¼“å­˜å‘½ä¸­: {result.cached_segments}
   æ–°ç”Ÿæˆ: {result.regenerated_segments}
   è€—æ—¶: {result.synthesis_time:.1f} ç§’

âš™ï¸  é…ç½®å‚æ•°:
   length_penalty: {length_penalty}
   å‚è€ƒéŸ³é¢‘: {ref_audio_path.name}

{stats_report}

ğŸ’¡ æç¤º: 
   - ç‚¹å‡» "æ˜¾ç¤ºæ—¶é•¿ç»Ÿè®¡" æŸ¥çœ‹è¯¦ç»†åˆ†æ
   - ä½¿ç”¨ "åªçœ‹è¶…é™ç‰‡æ®µ" ç­›é€‰é—®é¢˜ç‰‡æ®µ
"""

        updated_data = _prepare_review_data_v2(filter_over_limit=False)
        return status, gr.update(value=updated_data), stats_report

    except Exception as e:
        import traceback
        error_msg = f"âŒ è¯­éŸ³å…‹éš†å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return error_msg, gr.update(), ""


# ============== ç‰‡æ®µé¢„è§ˆ ============== #

def preview_segment(evt: gr.SelectData):
    """é¢„è§ˆé€‰ä¸­çš„ç‰‡æ®µ"""
    global current_session

    if evt is None or not current_session.video or not current_session.translated_subtitle:
        return None, "âš ï¸ æ— æ•ˆçš„ä¼šè¯çŠ¶æ€", "", ""

    try:
        if isinstance(evt.index, (tuple, list)):
            selected_row_index = int(evt.index[0])
        else:
            selected_row_index = int(evt.index)

        total_segments = len(current_session.translated_subtitle.segments)
        if selected_row_index < 0 or selected_row_index >= total_segments:
            return None, f"âŒ æ— æ•ˆçš„ç‰‡æ®µç´¢å¼•: {selected_row_index}", "", ""

        idx = selected_row_index
        text_seg = current_session.translated_subtitle.segments[idx]
        max_duration = text_seg.time_range.duration

        audio_seg = current_session.audio_segments.get(idx)
        if not audio_seg:
            audio_seg = audio_segment_repo.load_segment(
                segment_index=idx,
                video_path=current_session.video.path,
                text_segment=text_seg
            )
            if audio_seg:
                current_session.audio_segments[idx] = audio_seg

        actual_duration = None
        if audio_seg and audio_seg.file_path and audio_seg.file_path.exists():
            audio_path = str(audio_seg.file_path)
            actual_duration = len(audio_seg.audio.samples) / audio_seg.audio.sample_rate
            duration_diff = actual_duration - max_duration
            diff_sign = "+" if duration_diff > 0 else ""
            audio_status = f"âœ… éŸ³é¢‘å·²ç”Ÿæˆ ({(actual_duration / max_duration * 100):.1f}%)"
        else:
            audio_path = None
            audio_status = "âš ï¸ éŸ³é¢‘æœªç”Ÿæˆ"

        if actual_duration:
            duration_diff = actual_duration - max_duration
            diff_sign = "+" if duration_diff > 0 else ""
            text_info = f"""
ç‰‡æ®µ #{idx}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  æ—¶é—´è½´: {text_seg.time_range.start_seconds:.2f}s - {text_seg.time_range.end_seconds:.2f}s

ğŸ“ æ—¶é•¿ä¿¡æ¯:
   â€¢ æœ€å¤§å…è®¸: {max_duration:.2f}s
   â€¢ å®é™…ç”Ÿæˆ: {actual_duration:.2f}s
   â€¢ å·®å¼‚: {diff_sign}{duration_diff:.2f}s ({diff_sign}{(duration_diff / max_duration * 100):.1f}%)

ğŸ“Š çŠ¶æ€: {'âœ… æ­£å¸¸' if abs(duration_diff) < 0.5 else 'âš ï¸ åå·®è¾ƒå¤§'}
"""
        else:
            text_info = f"""
ç‰‡æ®µ #{idx}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  æ—¶é—´è½´: {text_seg.time_range.start_seconds:.2f}s - {text_seg.time_range.end_seconds:.2f}s

ğŸ“ æ—¶é•¿ä¿¡æ¯:
   â€¢ æœ€å¤§å…è®¸: {max_duration:.2f}s
   â€¢ å®é™…ç”Ÿæˆ: æœªç”Ÿæˆ
"""

        subtitle_text = text_seg.text
        return audio_path, audio_status, text_info, subtitle_text

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ é¢„è§ˆå¤±è´¥: {e}", "", ""


# ============== æ­¥éª¤3: æœ€ç»ˆåˆæˆ ============== #

def step3_final_synthesis(
    enable_bilingual: bool,  # âœ… æ–°å¢å‚æ•°ï¼šæ˜¯å¦å¯ç”¨åŒè¯­å­—å¹•
    progress=gr.Progress()
):
    """æ­¥éª¤3: æœ€ç»ˆè§†é¢‘åˆæˆï¼ˆæ”¯æŒåŒè¯­å­—å¹•å¯é€‰ï¼‰"""
    global current_session
    container.get_tts().unload()

    if not current_session.video:
        return None, None, None, "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±"

    total_segments = len(current_session.translated_subtitle.segments)
    audio_ready = len(current_session.audio_segments)

    print(f"\nğŸ” æœ€ç»ˆåˆæˆå‰æ£€æŸ¥:")
    print(f"   æ€»ç‰‡æ®µæ•°: {total_segments}")
    print(f"   éŸ³é¢‘å·²ç”Ÿæˆ: {audio_ready}")
    print(f"   ç¼ºå¤±ç‰‡æ®µ: {total_segments - audio_ready}")
    print(f"   åŒè¯­å­—å¹•: {'å¯ç”¨' if enable_bilingual else 'ç¦ç”¨'}")

    unreviewed = [
        idx for idx in range(total_segments)
        if idx not in current_session.audio_segments
    ]

    if unreviewed and len(unreviewed) > total_segments * 0.3:
        return None, None, None, f"âš ï¸ è¿˜æœ‰ {len(unreviewed)} ä¸ªç‰‡æ®µæœªå®ŒæˆéŸ³é¢‘ç”Ÿæˆ,è¯·å…ˆå®Œæˆæ­¥éª¤2"

    try:
        progress(0.1, "å‡†å¤‡åˆæˆ...")

        output_dir = current_session.video.path.parent / "output"
        output_dir.mkdir(exist_ok=True)

        # åˆå¹¶éŸ³é¢‘ç‰‡æ®µ
        progress(0.2, "åˆå¹¶éŸ³é¢‘ç‰‡æ®µ...")

        from domain.entities import AudioTrack
        from domain.value_objects import AudioSample

        sample_rate = list(current_session.audio_segments.values())[0].audio.sample_rate
        total_samples = int(current_session.video.duration * sample_rate)
        buffer = [0.0] * total_samples

        for idx, audio_seg in current_session.audio_segments.items():
            text_seg = audio_seg.text_segment
            start_idx = int(text_seg.time_range.start_seconds * sample_rate)

            for i, sample in enumerate(audio_seg.audio.samples):
                target_idx = start_idx + i
                if target_idx < total_samples:
                    buffer[target_idx] = sample

        full_audio = AudioSample(
            samples=tuple(buffer),
            sample_rate=sample_rate
        )

        audio_track = AudioTrack(full_audio, current_session.translated_subtitle.language)

        # è§†é¢‘åˆæˆ
        progress(0.5, "åˆæˆè§†é¢‘...")

        from application.use_cases.synthesize_video_use_case import synthesize_video_use_case
        from domain.services import merge_bilingual_subtitles

        # âœ… éœ€æ±‚2: æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šå­—å¹•æ–¹æ¡ˆ
        if enable_bilingual and current_session.english_subtitle:
            # å¯ç”¨åŒè¯­å­—å¹•
            zh_en_subtitle = merge_bilingual_subtitles(
                current_session.translated_subtitle,
                current_session.english_subtitle
            )
            subtitles_tuple = (
                current_session.translated_subtitle,
                current_session.english_subtitle,
                zh_en_subtitle
            )
            subtitle_mode = "åŒè¯­ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰"
        else:
            # åªç”¨ä¸­æ–‡å­—å¹•
            subtitles_tuple = (current_session.translated_subtitle,)
            subtitle_mode = "å•è¯­ï¼ˆä»…ä¸­æ–‡ï¼‰"

        synthesis_result = synthesize_video_use_case(
            video=current_session.video,
            subtitles=subtitles_tuple,
            audio_track=audio_track,
            video_processor=container.video_processor,
            subtitle_writer=container.subtitle_writer,
            output_dir=output_dir,
            formats=("srt", "ass"),
            burn_subtitles=True,
            progress=lambda p, d: progress(0.5 + p * 0.5, d)
        )

        # æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶
        def find_file(patterns: list[str], suffix: str = None) -> Optional[str]:
            for pattern in patterns:
                matches = [
                    p for p in synthesis_result.output_paths
                    if pattern in p.name and (suffix is None or p.suffix == suffix)
                ]
                if matches:
                    return str(matches[0])
            return None

        zh_srt = find_file(['zh.srt'], '.srt')
        zh_en_ass = find_file(['zh_en'], '.ass') if enable_bilingual else None
        voiced_video = find_file(['_voiced_subtitled.mp4'])

        status = f"""
âœ… æœ€ç»ˆåˆæˆå®Œæˆ!

ğŸ“¦ è¾“å‡ºæ–‡ä»¶:
   - ä¸­æ–‡å­—å¹•: {zh_srt.split('/')[-1] if zh_srt else 'âŒ'}
   - åŒè¯­å­—å¹•: {zh_en_ass.split('/')[-1] if zh_en_ass else 'æœªå¯ç”¨'}
   - é…éŸ³è§†é¢‘: {voiced_video.split('/')[-1] if voiced_video else 'âŒ'}

âš™ï¸  å­—å¹•æ¨¡å¼: {subtitle_mode}
â±ï¸  å¤„ç†æ—¶é—´: {synthesis_result.processing_time:.1f} ç§’

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   æ€»ç‰‡æ®µæ•°: {len(current_session.audio_segments)}
   ä½¿ç”¨ç¼“å­˜: {audio_ready - len(current_session.modified_indices)}
   é‡æ–°ç”Ÿæˆ: {len(current_session.modified_indices) if hasattr(current_session, 'modified_indices') else 0}
"""

        return zh_srt, zh_en_ass, voiced_video, status

    except Exception as e:
        import traceback
        error_msg = f"âŒ åˆæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, None, None, error_msg


def toggle_filter_over_limit(filter_enabled: bool):
    """åˆ‡æ¢è¶…é™ç‰‡æ®µç­›é€‰"""
    updated_data = _prepare_review_data_v2(filter_over_limit=filter_enabled)

    if filter_enabled:
        stats = calculate_duration_statistics()
        message = f"ğŸ” å·²ç­›é€‰ï¼Œæ˜¾ç¤º {stats['over_limit_count']} ä¸ªè¶…é™ç‰‡æ®µ"
    else:
        message = "ğŸ“‹ æ˜¾ç¤ºå…¨éƒ¨ç‰‡æ®µ"

    return gr.update(value=updated_data), message


def refresh_review_table(filter_enabled: bool):
    """åˆ·æ–°å®¡æ ¸è¡¨æ ¼"""
    updated_data = _prepare_review_data_v2(filter_over_limit=filter_enabled)
    stats_report, _ = show_duration_statistics()
    return gr.update(value=updated_data), stats_report
def step2_regenerate_modified(length_penalty: float):
    """é‡æ–°ç”Ÿæˆä¿®æ”¹è¿‡çš„ç‰‡æ®µï¼ˆæ”¯æŒ length_penaltyï¼‰"""
    global current_session

    if not current_session.modified_indices:
        return "â„¹ï¸ æ²¡æœ‰éœ€è¦é‡æ–°ç”Ÿæˆçš„ç‰‡æ®µ", gr.update(), ""

    ref_audio_path = current_session.reference_audio_path
    if not ref_audio_path:
        ref_audio_path = audio_file_repo.load_reference_audio(current_session.video.path)

    if not ref_audio_path:
        return "âŒ é”™è¯¯: ç¼ºå°‘å‚è€ƒéŸ³é¢‘", gr.update(), ""

    try:
        # âœ… æ›´æ–° length_penalty
        tts = container.get_tts()
        if hasattr(tts, 'update_config'):
            tts.update_config(length_penalty=length_penalty)

        print(f"  âš™ï¸  é‡æ–°ç”Ÿæˆé…ç½®: length_penalty={length_penalty}")

        result = regenerate_modified_segments_use_case(
            video=current_session.video,
            original_subtitle=current_session.original_subtitle,
            modified_subtitle=current_session.translated_subtitle,
            modified_indices=current_session.modified_indices,
            tts_provider=container.get_tts(),
            video_processor=container.video_processor,
            audio_repo=audio_segment_repo,
            reference_audio_path=ref_audio_path,
            progress=None
        )

        for audio_seg in result.audio_segments:
            current_session.audio_segments[audio_seg.segment_index] = audio_seg

        current_session.modified_indices.clear()

        # âœ… æ›´æ–°ç»Ÿè®¡
        stats_report, _ = show_duration_statistics()
        updated_data = _prepare_review_data_v2(filter_over_limit=False)

        status = f"""
âœ… é‡æ–°ç”Ÿæˆå®Œæˆ!
   é‡æ–°ç”Ÿæˆ: {result.regenerated_segments} ä¸ªç‰‡æ®µ
   è€—æ—¶: {result.synthesis_time:.1f} ç§’
   length_penalty: {length_penalty}

{stats_report}
"""

        return status, gr.update(value=updated_data), stats_report

    except Exception as e:
        import traceback
        return f"âŒ é‡æ–°ç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}", gr.update(), ""


# ============== UI æ„å»º ============== #
def build_ui_v2():
    """æ„å»ºå¢å¼º UI V2ï¼ˆå®Œæ•´é‡æ„ç‰ˆï¼‰"""

    with gr.Blocks(
        title="è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V2",
        css="""
        .gradio-container {max-width: 1800px !important}
        .segment-preview {border: 1px solid #ddd; padding: 10px; border-radius: 5px;}
        """
    ) as demo:
        gr.Markdown("""
        # ğŸ¬ è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V2 - å®Œæ•´é‡æ„ç‰ˆ

        ## âœ¨ æ–°å¢ç‰¹æ€§
        - ğŸµ **å¯é…ç½®å‚è€ƒéŸ³é¢‘**: æ”¯æŒè‡ªå®šä¹‰èµ·å§‹åç§»å’Œæ—¶é•¿
        - ğŸ“ **åŒè¯­å­—å¹•å¯é€‰**: åˆæˆè§†é¢‘å‰é€‰æ‹©æ˜¯å¦çƒ§å½•åŒè¯­å­—å¹•
        - ğŸ—ï¸ **æ¶æ„ä¼˜åŒ–**: å‚è€ƒéŸ³é¢‘ç®¡ç†ä¸‹æ²‰åˆ°Infrastructureå±‚
        
        ## ğŸ“‹ å·¥ä½œæµç¨‹
        1. **ç”Ÿæˆå­—å¹•** â†’ 2A. **å¢é‡è¯­éŸ³å…‹éš†** â†’ 2B. **å®¡æ ¸ä¿®æ”¹** â†’ 2C. **é‡æ–°ç”Ÿæˆ** â†’ 3. **æœ€ç»ˆåˆæˆ**
        """)

        with gr.Tab("ğŸ¬ å•è§†é¢‘å¤„ç† V2"):
            # ========== æ­¥éª¤1 ========== #
            with gr.Accordion("ğŸ” æ­¥éª¤1: ç”Ÿæˆå­—å¹•", open=True):
                with gr.Row():
                    with gr.Column(scale=1):
                        video_input = gr.File(
                            label="ğŸ“¹ ä¸Šä¼ è§†é¢‘",
                            file_types=[".mp4", ".avi", ".mov", ".mkv"]
                        )

                        whisper_model = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large", "large-v3"],
                            value="medium",
                            label="ğŸ™ï¸ Whisper æ¨¡å‹"
                        )

                        translation_model = gr.Dropdown(
                            choices=["Qwen/Qwen2.5-7B"],
                            value="Qwen/Qwen2.5-7B",
                            label="ğŸŒ ç¿»è¯‘æ¨¡å‹"
                        )

                        translation_context = gr.Dropdown(
                            choices=container.translator_context_repo.list_contexts(),
                            value="general",
                            label="ğŸ“š ç¿»è¯‘ä¸Šä¸‹æ–‡"
                        )

                        source_lang = gr.Dropdown(
                            choices=["auto", "en", "zh", "pt", "ja"],
                            value="auto",
                            label="ğŸ—£ï¸ æºè¯­è¨€"
                        )

                        step1_btn = gr.Button("â–¶ï¸ ç”Ÿæˆå­—å¹•", variant="primary")

                    with gr.Column(scale=1):
                        step1_status = gr.Textbox(
                            label="ğŸ“Š ç”ŸæˆçŠ¶æ€",
                            lines=12
                        )

            # ========== æ­¥éª¤2 ========== #
            with gr.Accordion("ğŸ¤ æ­¥éª¤2: å¢é‡è¯­éŸ³å…‹éš†", open=False) as step2_accordion:
                gr.Markdown("""
                ### å·¥ä½œæµç¨‹
                1. **2A. å¢é‡è¯­éŸ³å…‹éš†**: é€ç‰‡æ®µç”ŸæˆéŸ³é¢‘å¹¶ç¼“å­˜
                2. **ğŸ“Š æ—¶é•¿åˆ†æ**: æŸ¥çœ‹ç»Ÿè®¡å’Œç­›é€‰è¶…é™ç‰‡æ®µ
                3. **2B. å®¡æ ¸é¢„è§ˆ**: è¯•å¬éŸ³é¢‘,ä¿®æ”¹å­—å¹•
                4. **2C. é‡æ–°ç”Ÿæˆ**: è°ƒæ•´ length_penalty å¹¶é‡æ–°ç”Ÿæˆ
                """)

                # 2A: è¯­éŸ³å…‹éš†
                with gr.Group():
                    gr.Markdown("### 2A. å¢é‡è¯­éŸ³å…‹éš†")

                    reference_audio = gr.File(
                        label="ğŸµ å‚è€ƒéŸ³é¢‘(å¯é€‰)",
                        file_types=[".wav", ".mp3"]
                    )

                    with gr.Row():
                        ref_duration_slider = gr.Slider(
                            minimum=5, maximum=60, value=10, step=5,
                            label="â±ï¸ å‚è€ƒéŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"
                        )
                        ref_offset_slider = gr.Slider(
                            minimum=0, maximum=120, value=0, step=5,
                            label="ğŸ“ èµ·å§‹åç§»ï¼ˆç§’ï¼‰"
                        )

                    # âœ… length_penalty æ§åˆ¶
                    with gr.Row():
                        length_penalty_slider = gr.Slider(
                            minimum=-2.0,
                            maximum=2.0,
                            value=0.0,
                            step=0.1,
                            label="âš™ï¸ length_penalty",
                            info="è´Ÿå€¼é¼“åŠ±æ›´é•¿è¾“å‡ºï¼Œæ­£å€¼é¼“åŠ±æ›´çŸ­è¾“å‡ºï¼ˆé»˜è®¤0.0ï¼‰"
                        )
                        suggest_penalty_btn = gr.Button(
                            "ğŸ’¡ æ™ºèƒ½æ¨è",
                            size="sm",
                            variant="secondary"
                        )

                    length_penalty_status = gr.Textbox(
                        label="length_penalty çŠ¶æ€",
                        lines=2,
                        visible=False
                    )

                    clone_btn = gr.Button("ğŸ¤ å¼€å§‹å¢é‡è¯­éŸ³å…‹éš†", variant="primary")
                    clone_status = gr.Textbox(label="å…‹éš†çŠ¶æ€", lines=12)

                # âœ… æ—¶é•¿ç»Ÿè®¡é¢æ¿
                with gr.Group():
                    gr.Markdown("### ğŸ“Š æ—¶é•¿ç»Ÿè®¡åˆ†æ")

                    with gr.Row():
                        show_stats_btn = gr.Button("ğŸ“Š æ˜¾ç¤ºæ—¶é•¿ç»Ÿè®¡", variant="secondary")
                        refresh_table_btn = gr.Button("ğŸ”„ åˆ·æ–°è¡¨æ ¼", variant="secondary")

                    duration_stats_display = gr.Markdown(
                        "ç‚¹å‡» 'æ˜¾ç¤ºæ—¶é•¿ç»Ÿè®¡' æŸ¥çœ‹åˆ†ææŠ¥å‘Š",
                        elem_classes=["stats-box"]
                    )

                    with gr.Row():
                        filter_over_limit = gr.Checkbox(
                            label="ğŸ” åªçœ‹è¶…é™ç‰‡æ®µï¼ˆè¶…è¿‡ç›®æ ‡æ—¶é•¿>0.1sï¼‰",
                            value=False
                        )
                        filter_status = gr.Textbox(
                            label="ç­›é€‰çŠ¶æ€",
                            value="ğŸ“‹ æ˜¾ç¤ºå…¨éƒ¨ç‰‡æ®µ",
                            lines=1
                        )

                # 2B: å®¡æ ¸è¡¨æ ¼
                with gr.Group():
                    gr.Markdown("### 2B. å®¡æ ¸å’Œé¢„è§ˆ")

                    review_dataframe = gr.Dataframe(
                        headers=[
                            "ç´¢å¼•", "æ—¶é—´", "åŸæ–‡", "ç¿»è¯‘",
                            "ç›®æ ‡é•¿åº¦", "å®é™…é•¿åº¦", "æ—¶é•¿çŠ¶æ€", "éŸ³é¢‘", "å®¡æ ¸"
                        ],
                        datatype=[
                            "number", "str", "str", "str",
                            "str", "str", "str", "str", "str"
                        ],
                        col_count=(9, "fixed"),
                        row_count=(10, "dynamic"),
                        interactive=True,
                        wrap=True,
                        label="å­—å¹•å®¡æ ¸è¡¨æ ¼ (ç‚¹å‡»è¡Œé¢„è§ˆéŸ³é¢‘)"
                    )

                    with gr.Row():
                        save_edits_btn = gr.Button("ğŸ’¾ ä¿å­˜ä¿®æ”¹", variant="secondary")
                        regenerate_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆä¿®æ”¹çš„ç‰‡æ®µ", variant="primary")

                    edit_status = gr.Textbox(label="ç¼–è¾‘çŠ¶æ€", lines=3)

                # ç‰‡æ®µé¢„è§ˆåŒº
                with gr.Group():
                    gr.Markdown("### ğŸ‘‚ ç‰‡æ®µé¢„è§ˆ")

                    with gr.Row():
                        with gr.Column(scale=1):
                            preview_audio = gr.Audio(
                                label="ğŸ”Š éŸ³é¢‘æ’­æ”¾",
                                type="filepath"
                            )
                            preview_status = gr.Textbox(label="çŠ¶æ€", lines=1)

                        with gr.Column(scale=1):
                            preview_info = gr.Textbox(label="ç‰‡æ®µä¿¡æ¯", lines=5)
                            preview_text = gr.Textbox(label="å­—å¹•æ–‡æœ¬", lines=4)

            # ========== æ­¥éª¤3 ========== #
            with gr.Accordion("ğŸ¬ æ­¥éª¤3: æœ€ç»ˆåˆæˆ", open=False):
                gr.Markdown("""
                ### æç¤º
                - ç¡®ä¿æ‰€æœ‰å…³é”®ç‰‡æ®µéƒ½å·²å®¡æ ¸é€šè¿‡
                - é€‰æ‹©æ˜¯å¦çƒ§å½•åŒè¯­å­—å¹•
                """)

                # âœ… æ–°å¢: åŒè¯­å­—å¹•é€‰é¡¹
                enable_bilingual_checkbox = gr.Checkbox(
                    label="ğŸ“ çƒ§å½•åŒè¯­å­—å¹•ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰",
                    value=True,
                    info="å–æ¶ˆå‹¾é€‰åˆ™åªçƒ§å½•ä¸­æ–‡å­—å¹•"
                )

                final_btn = gr.Button("â–¶ï¸ ç”Ÿæˆæœ€ç»ˆè§†é¢‘", variant="primary", size="lg")
                final_status = gr.Textbox(label="åˆæˆçŠ¶æ€", lines=12)

                with gr.Row():
                    zh_srt_output = gr.File(label="ä¸­æ–‡å­—å¹•")
                    zh_en_ass_output = gr.File(label="åŒè¯­å­—å¹•")
                    final_video_output = gr.File(label="æœ€ç»ˆè§†é¢‘")

            # ========== äº‹ä»¶ç»‘å®š ========== #

            # æ­¥éª¤1
            step1_btn.click(
                step1_generate_and_check_v2,
                inputs=[
                    video_input, whisper_model, translation_model,
                    translation_context, source_lang
                ],
                outputs=[review_dataframe, step1_status, step2_accordion]
            ).then(
                lambda: gr.update(open=True),
                outputs=[step2_accordion]
            )

            # æ­¥éª¤2A: è¯­éŸ³å…‹éš†
            # æ™ºèƒ½æ¨è length_penalty
            def on_suggest_penalty_click():
                suggested, reason = suggest_length_penalty()
                return suggested, reason, gr.update(visible=True)

            suggest_penalty_btn.click(
                on_suggest_penalty_click,
                outputs=[length_penalty_slider, length_penalty_status, length_penalty_status]
            )

            # æ­¥éª¤2A: è¯­éŸ³å…‹éš†
            clone_btn.click(
                step2_incremental_voice_cloning,
                inputs=[
                    reference_audio,
                    ref_duration_slider,
                    ref_offset_slider,
                    length_penalty_slider  # âœ… ä¼ é€’ length_penalty
                ],
                outputs=[clone_status, review_dataframe, duration_stats_display]
            )

            # æ˜¾ç¤ºç»Ÿè®¡
            show_stats_btn.click(
                show_duration_statistics,
                outputs=[duration_stats_display, duration_stats_display]
            )

            # ç­›é€‰åˆ‡æ¢
            filter_over_limit.change(
                toggle_filter_over_limit,
                inputs=[filter_over_limit],
                outputs=[review_dataframe, filter_status]
            )

            # åˆ·æ–°è¡¨æ ¼
            refresh_table_btn.click(
                refresh_review_table,
                inputs=[filter_over_limit],
                outputs=[review_dataframe, duration_stats_display]
            )

            # æ­¥éª¤2B: ç¼–è¾‘ä¿å­˜
            save_edits_btn.click(
                step2_save_edits_and_regenerate,
                inputs=[review_dataframe],
                outputs=[edit_status, review_dataframe]
            )

            # æ­¥éª¤2C: é‡æ–°ç”Ÿæˆ
            regenerate_btn.click(
                step2_regenerate_modified,
                inputs=[length_penalty_slider],  # âœ… ä¼ é€’å½“å‰ length_penalty
                outputs=[edit_status, review_dataframe, duration_stats_display]
            )

            # è¡¨æ ¼é€‰æ‹©äº‹ä»¶
            review_dataframe.select(
                preview_segment,
                outputs=[preview_audio, preview_status, preview_info, preview_text]
            )

            # æ­¥éª¤3: æœ€ç»ˆåˆæˆ
            final_btn.click(
                step3_final_synthesis,
                inputs=[enable_bilingual_checkbox],  # âœ… æ–°å¢
                outputs=[zh_srt_output, zh_en_ass_output, final_video_output, final_status]
            )

    return demo


def main():
    """å¯åŠ¨ WebUI V2"""
    demo = build_ui_v2()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        inbrowser=True,
        share=False
    )


if __name__ == "__main__":
    main()