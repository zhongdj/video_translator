"""
Infrastructure Layer - å¢å¼º WebUI V2
æ”¯æŒåˆ†æ®µè¯­éŸ³å…‹éš†ã€å®æ—¶é¢„è§ˆå’Œå¢é‡åˆæˆ
"""
from pathlib import Path
from typing import Optional, Dict

import gradio as gr

# å¯¼å…¥æ–°çš„ç”¨ä¾‹
from application.use_cases.incremental_voice_cloning import (
    incremental_voice_cloning_use_case,
    regenerate_modified_segments_use_case
)
from domain.entities import (
    Video, Subtitle, LanguageCode,
    TextSegment, TimeRange, AudioSegment,
    SegmentReviewStatus
)
from domain.services import calculate_cache_key
from infrastructure.adapters.storage.audio_segment_repository_adapter import AudioSegmentRepositoryAdapter
from infrastructure.config.dependency_injection import container

# åˆå§‹åŒ–éŸ³é¢‘ç‰‡æ®µä»“å‚¨


audio_segment_repo = AudioSegmentRepositoryAdapter()


# ============== ä¼šè¯çŠ¶æ€ç®¡ç† V2 ============== #
class TranslationSessionV2:
    """å¢å¼ºçš„ç¿»è¯‘ä¼šè¯çŠ¶æ€"""

    def __init__(self):
        self.translation_context = None
        self.video: Optional[Video] = None
        self.original_subtitle: Optional[Subtitle] = None
        self.translated_subtitle: Optional[Subtitle] = None
        self.english_subtitle: Optional[Subtitle] = None
        self.detected_language: Optional[LanguageCode] = None
        self.source_language: Optional[LanguageCode] = None
        self.quality_report = None

        # æ–°å¢ï¼šéŸ³é¢‘ç‰‡æ®µç®¡ç†
        self.audio_segments: Dict[int, AudioSegment] = {}
        self.segment_review_status: Dict[int, SegmentReviewStatus] = {}

        # ä¿®æ”¹è¿½è¸ª
        self.edited_segments: Dict[int, str] = {}  # {index: edited_text}
        self.modified_indices: set[int] = set()

        # å‚è€ƒéŸ³é¢‘
        self.reference_audio_path: Optional[Path] = None

        self.approved = False


# å…¨å±€ä¼šè¯å¯¹è±¡
current_session = TranslationSessionV2()


# ============== æ­¥éª¤1: ç”Ÿæˆå­—å¹•å’Œè´¨é‡æ£€æŸ¥ ============== #
def step1_generate_and_check_v2(
        video_file,
        whisper_model: str,
        translation_model: str,
        translation_context_name: str,
        source_language: str,
        progress=gr.Progress()
):
    """æ­¥éª¤1: ç”Ÿæˆå­—å¹•å¹¶è¿›è¡Œè´¨é‡æ£€æŸ¥"""
    global current_session

    if not video_file:
        return None, "âŒ è¯·ä¸Šä¼ è§†é¢‘", gr.update(visible=False)

    try:
        current_session = TranslationSessionV2()

        video_path = Path(video_file.name)

        # åˆ›å»ºè§†é¢‘å¯¹è±¡
        current_session.video = Video(
            path=video_path,
            duration=get_video_duration(video_path),
            has_audio=True
        )

        # åŠ è½½ç¿»è¯‘ä¸Šä¸‹æ–‡
        translation_context = container.translator_context_repo.load(
            translation_context_name
        )

        if not translation_context:
            return None, f"âŒ æ— æ³•åŠ è½½ç¿»è¯‘ä¸Šä¸‹æ–‡: {translation_context_name}", gr.update(visible=False)

        # è§£ææºè¯­è¨€
        src_lang = LanguageCode(source_language) if source_language != "auto" else None

        progress(0.0, "å¼€å§‹ç”Ÿæˆå­—å¹•...")

        # ä½¿ç”¨æ”¹è¿›çš„å­—å¹•ç”Ÿæˆç”¨ä¾‹
        from application.use_cases.improved_generate_subtitles import improved_generate_subtitles_use_case

        result = improved_generate_subtitles_use_case(
            video=current_session.video,
            asr_provider=container.get_asr(whisper_model),
            translation_provider=container.get_translator(),
            video_processor=container.video_processor,
            cache_repo=container.cache_repo,
            translation_context=translation_context,
            target_language=LanguageCode.CHINESE,
            source_language=src_lang,
            enable_quality_check=True,
            progress=lambda p, d: progress(p, d)
        )

        container.get_translator().unload()

        current_session.original_subtitle = result.original_subtitle
        current_session.translated_subtitle = result.translated_subtitle
        current_session.detected_language = result.detected_language
        current_session.quality_report = result.quality_report
        current_session.translation_context = translation_context
        current_session.source_language = src_lang

        # ä»ç¼“å­˜åŠ è½½è‹±æ–‡å­—å¹•
        cache_params = {
            "target_language": LanguageCode.CHINESE.value,
            "source_language": src_lang.value if src_lang else "auto"
        }

        if translation_context:
            cache_params["context_domain"] = translation_context.domain

        cache_key = calculate_cache_key(
            current_session.video.path,
            "subtitles_v2",
            cache_params
        )

        try:
            cached = container.cache_repo.get(cache_key)
            if cached and "en_segments" in cached:
                en_segments = tuple(
                    TextSegment(
                        text=seg["text"],
                        time_range=TimeRange(seg["start"], seg["end"]),
                        language=LanguageCode.ENGLISH
                    )
                    for seg in cached["en_segments"]
                )
                current_session.english_subtitle = Subtitle(en_segments, LanguageCode.ENGLISH)
        except Exception as e:
            print(f"  âš ï¸  åŠ è½½è‹±æ–‡å­—å¹•å¤±è´¥: {e}")

        # åˆå§‹åŒ–å®¡æ ¸çŠ¶æ€
        for idx in range(len(result.translated_subtitle.segments)):
            current_session.segment_review_status[idx] = SegmentReviewStatus(
                segment_index=idx,
                subtitle_approved=False,
                audio_approved=False,
                subtitle_modified=False,
                needs_regeneration=True
            )

        # ç”ŸæˆçŠ¶æ€æŠ¥å‘Š
        report_lines = [
            f"âœ… å­—å¹•ç”Ÿæˆå®Œæˆ",
            f"",
            f"ğŸ“Š åŸºæœ¬ä¿¡æ¯:",
            f"   è§†é¢‘: {current_session.video.path.name}",
            f"   æ—¶é•¿: {current_session.video.duration:.1f} ç§’",
            f"   æ£€æµ‹è¯­è¨€: {result.detected_language.value}",
            f"   æ€»ç‰‡æ®µæ•°: {len(result.translated_subtitle.segments)}",
            f"   ä½¿ç”¨ä¸Šä¸‹æ–‡: {translation_context.domain}",
        ]

        # è´¨é‡æŠ¥å‘Š
        if result.quality_report:
            qr = result.quality_report
            report_lines.extend([
                f"",
                f"ğŸ” è´¨é‡æ£€æŸ¥ç»“æœ:",
                f"   æ•´ä½“è´¨é‡: {qr.overall_quality}",
                f"   å‘ç°é—®é¢˜: {qr.issues_found} ä¸ª",
                f"   æ˜¯å¦éœ€è¦å®¡æ ¸: {'æ˜¯ âš ï¸' if qr.requires_review else 'å¦ âœ…'}",
            ])

        status_report = "\n".join(report_lines)

        # å‡†å¤‡å®¡æ ¸æ•°æ®ï¼ˆä¸åŒ…å«éŸ³é¢‘ï¼‰
        review_data = _prepare_review_data_v2()

        return review_data, status_report, gr.update(visible=True)

    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, error_msg, gr.update(visible=False)


def _prepare_review_data_v2():
    """å‡†å¤‡å®¡æ ¸æ•°æ®ï¼ˆåŒ…å«éŸ³é¢‘æ’­æ”¾å™¨ï¼‰"""
    global current_session

    if not current_session.translated_subtitle:
        return None

    data = []
    for idx, (orig_seg, trans_seg) in enumerate(
            zip(current_session.original_subtitle.segments,
                current_session.translated_subtitle.segments)
    ):
        # ä¼˜å…ˆæ‹¿è‹±æ–‡å­—å¹•
        en_text = (
            current_session.english_subtitle.segments[idx].text
            if current_session.english_subtitle and idx < len(current_session.english_subtitle.segments)
            else orig_seg.text
        )

        # é—®é¢˜æ ‡è®°
        has_issue = False
        issue_desc = ""
        if current_session.quality_report:
            segment_issues = [
                i for i in current_session.quality_report.issues
                if i.segment_index == idx
            ]
            if segment_issues:
                has_issue = True
                issue_desc = "; ".join([
                    f"{i.issue_type}({i.severity})"
                    for i in segment_issues
                ])

        # éŸ³é¢‘çŠ¶æ€
        audio_status = "æœªç”Ÿæˆ"
        if idx in current_session.audio_segments:
            audio_status = "âœ… å·²ç¼“å­˜"

        # å®¡æ ¸çŠ¶æ€
        review_status = current_session.segment_review_status.get(idx)
        if review_status:
            if review_status.subtitle_approved and review_status.audio_approved:
                review_mark = "âœ… å·²å®¡æ ¸"
            elif review_status.subtitle_modified:
                review_mark = "ğŸ”„ å·²ä¿®æ”¹"
            else:
                review_mark = "â³ å¾…å®¡æ ¸"
        else:
            review_mark = "â³ å¾…å®¡æ ¸"

        data.append([
            idx,
            f"{trans_seg.time_range.start_seconds:.2f}s",
            en_text,
            trans_seg.text,
            audio_status,
            "âš ï¸" if has_issue else "",
            review_mark
        ])

    return data


# ============== æ­¥éª¤2: å¢é‡è¯­éŸ³å…‹éš† ============== #
def step2_incremental_voice_cloning(
        reference_audio_file,
        progress=gr.Progress()
):
    """æ­¥éª¤2: å¢é‡è¯­éŸ³å…‹éš†ï¼ˆé€ç‰‡æ®µåˆæˆï¼‰"""
    global current_session

    if not current_session.video or not current_session.translated_subtitle:
        return "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±", gr.update()

    try:
        # å‡†å¤‡å‚è€ƒéŸ³é¢‘
        if reference_audio_file:
            ref_audio_path = Path(reference_audio_file.name)
            current_session.reference_audio_path = ref_audio_path
        elif current_session.reference_audio_path:
            ref_audio_path = current_session.reference_audio_path
        else:
            progress(0.05, "æå–å‚è€ƒéŸ³é¢‘...")
            ref_audio_path = container.video_processor.extract_reference_audio(
                current_session.video,
                duration=10.0
            )
            current_session.reference_audio_path = ref_audio_path

        # å®æ—¶è¿›åº¦å›è°ƒï¼ˆæ›´æ–°è¡¨æ ¼ï¼‰
        synthesis_log = []

        def segment_progress(ratio, msg, idx, audio_seg):
            synthesis_log.append(f"[{ratio * 100:.0f}%] {msg}")
            progress(ratio, msg)

            # å¦‚æœæœ‰éŸ³é¢‘ç‰‡æ®µï¼Œæ›´æ–°ä¼šè¯çŠ¶æ€
            if audio_seg:
                current_session.audio_segments[idx] = audio_seg

                # æ›´æ–°å®¡æ ¸çŠ¶æ€
                status = current_session.segment_review_status.get(idx)
                if status and not status.subtitle_modified:
                    current_session.segment_review_status[idx] = SegmentReviewStatus(
                        segment_index=idx,
                        subtitle_approved=False,
                        audio_approved=False,
                        subtitle_modified=False,
                        needs_regeneration=False
                    )

        # æ‰§è¡Œå¢é‡åˆæˆ
        result = incremental_voice_cloning_use_case(
            video=current_session.video,
            subtitle=current_session.translated_subtitle,
            tts_provider=container.get_tts(),
            video_processor=container.video_processor,
            audio_repo=audio_segment_repo,
            cache_repo=container.cache_repo,
            reference_audio_path=ref_audio_path,
            progress=segment_progress
        )

        container.get_tts().unload()

        # æ›´æ–°æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        for audio_seg in result.audio_segments:
            current_session.audio_segments[audio_seg.segment_index] = audio_seg

        status = f"""
âœ… å¢é‡è¯­éŸ³å…‹éš†å®Œæˆ!

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   æ€»ç‰‡æ®µæ•°: {result.total_segments}
   ç¼“å­˜å‘½ä¸­: {result.cached_segments}
   æ–°ç”Ÿæˆ: {result.regenerated_segments}
   è€—æ—¶: {result.synthesis_time:.1f} ç§’

ğŸ’¡ æç¤º: 
   - ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡ŒæŸ¥çœ‹å’Œæ’­æ”¾éŸ³é¢‘
   - ä¿®æ”¹å­—å¹•åéœ€è¦é‡æ–°ç”Ÿæˆå¯¹åº”ç‰‡æ®µ
   - å®¡æ ¸é€šè¿‡åå¯ä»¥ç»§ç»­æ­¥éª¤3
"""

        # æ›´æ–°è¡¨æ ¼æ•°æ®
        updated_data = _prepare_review_data_v2()

        return status, gr.update(value=updated_data)

    except Exception as e:
        import traceback
        error_msg = f"âŒ è¯­éŸ³å…‹éš†å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return error_msg, gr.update()


# ============== æ­¥éª¤2B: å­—å¹•ç¼–è¾‘å’Œé‡æ–°ç”Ÿæˆ ============== #
def step2_save_edits_and_regenerate(review_dataframe):
    """ä¿å­˜ç¼–è¾‘å¹¶æ ‡è®°éœ€è¦é‡æ–°ç”Ÿæˆçš„ç‰‡æ®µ"""
    global current_session

    if hasattr(review_dataframe, "values"):
        review_dataframe = review_dataframe.values.tolist()

    if not review_dataframe:
        return "âš ï¸ æ²¡æœ‰å¯ä¿å­˜çš„ä¿®æ”¹", gr.update()

    # è·³è¿‡è¡¨å¤´
    if review_dataframe and isinstance(review_dataframe[0][0], str):
        review_dataframe = review_dataframe[1:]

    edited_count = 0
    modified_indices = set()

    for row in review_dataframe:
        try:
            idx = int(row[0])
        except (ValueError, IndexError):
            continue

        if idx >= len(current_session.translated_subtitle.segments):
            continue

        original_text = current_session.translated_subtitle.segments[idx].text
        edited_text = row[3]  # ç¿»è¯‘åˆ—

        if edited_text != original_text:
            current_session.edited_segments[idx] = edited_text
            current_session.modified_indices.add(idx)
            edited_count += 1

            # æ›´æ–°å®¡æ ¸çŠ¶æ€
            status = current_session.segment_review_status.get(idx)
            if status:
                current_session.segment_review_status[idx] = status.mark_subtitle_modified()

    if edited_count:
        # åº”ç”¨ç¼–è¾‘åˆ°å­—å¹•
        _apply_edits_to_subtitle_v2()

        # ä¿å­˜åˆ°ç¼“å­˜
        _save_to_cache_v2("ä¿å­˜ä¿®æ”¹")

        updated_data = _prepare_review_data_v2()

        return (
            f"âœ… å·²ä¿å­˜ {edited_count} å¤„ä¿®æ”¹\n"
            f"âš ï¸  éœ€è¦é‡æ–°ç”Ÿæˆ {len(current_session.modified_indices)} ä¸ªéŸ³é¢‘ç‰‡æ®µ",
            gr.update(value=updated_data)
        )
    else:
        return "â„¹ï¸ æœªæ£€æµ‹åˆ°ä¿®æ”¹", gr.update()


def step2_regenerate_modified():
    """é‡æ–°ç”Ÿæˆä¿®æ”¹è¿‡çš„ç‰‡æ®µ"""
    global current_session

    if not current_session.modified_indices:
        return "â„¹ï¸ æ²¡æœ‰éœ€è¦é‡æ–°ç”Ÿæˆçš„ç‰‡æ®µ", gr.update()

    try:
        print(f"  ğŸ”„ é‡æ–°ç”Ÿæˆ {len(current_session.modified_indices)} ä¸ªç‰‡æ®µ")

        result = regenerate_modified_segments_use_case(
            video=current_session.video,
            original_subtitle=current_session.original_subtitle,
            modified_subtitle=current_session.translated_subtitle,
            modified_indices=current_session.modified_indices,
            tts_provider=container.get_tts(),
            video_processor=container.video_processor,
            audio_repo=audio_segment_repo,
            reference_audio_path=current_session.reference_audio_path,
            progress=None
        )

        container.get_tts().unload()

        # æ›´æ–°éŸ³é¢‘ç‰‡æ®µ
        for audio_seg in result.audio_segments:
            current_session.audio_segments[audio_seg.segment_index] = audio_seg

        # æ¸…é™¤ä¿®æ”¹æ ‡è®°
        for idx in current_session.modified_indices:
            status = current_session.segment_review_status.get(idx)
            if status:
                current_session.segment_review_status[idx] = SegmentReviewStatus(
                    segment_index=idx,
                    subtitle_approved=False,
                    audio_approved=False,
                    subtitle_modified=True,
                    needs_regeneration=False
                )

        current_session.modified_indices.clear()

        updated_data = _prepare_review_data_v2()

        return (
            f"âœ… é‡æ–°ç”Ÿæˆå®Œæˆ!\n"
            f"   é‡æ–°ç”Ÿæˆ: {result.regenerated_segments} ä¸ªç‰‡æ®µ\n"
            f"   è€—æ—¶: {result.synthesis_time:.1f} ç§’",
            gr.update(value=updated_data)
        )

    except Exception as e:
        import traceback
        return f"âŒ é‡æ–°ç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}", gr.update()


# ============== ç‰‡æ®µé¢„è§ˆåŠŸèƒ½ ============== #
def preview_segment(selected_row_index):
    """é¢„è§ˆé€‰ä¸­çš„ç‰‡æ®µ"""
    global current_session

    if selected_row_index is None or selected_row_index < 0:
        return None, "è¯·é€‰æ‹©ä¸€ä¸ªç‰‡æ®µ", "", ""

    if selected_row_index >= len(current_session.translated_subtitle.segments):
        return None, "æ— æ•ˆçš„ç‰‡æ®µç´¢å¼•", "", ""

    idx = selected_row_index
    text_seg = current_session.translated_subtitle.segments[idx]

    # è·å–éŸ³é¢‘
    audio_seg = current_session.audio_segments.get(idx)

    if audio_seg and audio_seg.file_path:
        audio_path = str(audio_seg.file_path)
        audio_status = "âœ… éŸ³é¢‘å·²ç”Ÿæˆ"
    else:
        audio_path = None
        audio_status = "âš ï¸  éŸ³é¢‘æœªç”Ÿæˆ"

    # æ–‡æœ¬ä¿¡æ¯
    text_info = f"""
ç‰‡æ®µ #{idx}
æ—¶é—´: {text_seg.time_range.start_seconds:.2f}s - {text_seg.time_range.end_seconds:.2f}s
æ—¶é•¿: {text_seg.time_range.duration:.2f}s
"""

    subtitle_text = text_seg.text

    return audio_path, audio_status, text_info, subtitle_text


# ============== è¾…åŠ©å‡½æ•° ============== #
def _apply_edits_to_subtitle_v2():
    """åº”ç”¨ç¼–è¾‘åˆ°å­—å¹•å¯¹è±¡"""
    global current_session

    if not current_session.edited_segments:
        return

    new_segments = []
    for idx, seg in enumerate(current_session.translated_subtitle.segments):
        if idx in current_session.edited_segments:
            new_seg = TextSegment(
                text=current_session.edited_segments[idx],
                time_range=seg.time_range,
                language=seg.language
            )
            new_segments.append(new_seg)
        else:
            new_segments.append(seg)

    current_session.translated_subtitle = Subtitle(
        segments=tuple(new_segments),
        language=current_session.translated_subtitle.language
    )


def _save_to_cache_v2(operation_name: str = "æ“ä½œ"):
    """ä¿å­˜åˆ°ç¼“å­˜"""
    global current_session

    try:
        if not current_session.video or not current_session.translated_subtitle:
            return

        cache_params = {
            "target_language": LanguageCode.CHINESE.value,
            "source_language": current_session.source_language.value if current_session.source_language else "auto",
        }

        if current_session.translation_context:
            cache_params["context_domain"] = current_session.translation_context.domain

        cache_key = calculate_cache_key(
            current_session.video.path,
            "subtitles_v2",
            cache_params
        )

        cached = container.cache_repo.get(cache_key) or {}

        # æ›´æ–°ä¸­æ–‡å­—å¹•
        cached["zh_segments"] = [
            {
                "text": seg.text,
                "start": seg.time_range.start_seconds,
                "end": seg.time_range.end_seconds,
            }
            for seg in current_session.translated_subtitle.segments
        ]

        container.cache_repo.set(cache_key, cached)
        print(f"âœ… {operation_name}: ä¸­æ–‡å­—å¹•å·²å†™å›ç¼“å­˜")

    except Exception as e:
        print(f"âš ï¸ {operation_name}: å†™å›ç¼“å­˜å¤±è´¥: {e}")


def get_video_duration(video_path: Path) -> float:
    """è·å–è§†é¢‘æ—¶é•¿"""
    import subprocess
    result = subprocess.run([
        'ffprobe', '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        str(video_path)
    ], capture_output=True, text=True)
    return float(result.stdout.strip())


# ============== æ­¥éª¤3: æœ€ç»ˆåˆæˆ ============== #
def step3_final_synthesis(progress=gr.Progress()):
    """æ­¥éª¤3: æœ€ç»ˆè§†é¢‘åˆæˆ"""
    global current_session

    if not current_session.video:
        return None, None, None, "âŒ é”™è¯¯: ä¼šè¯çŠ¶æ€ä¸¢å¤±"

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç‰‡æ®µéƒ½å·²å®¡æ ¸
    unreviewed = [
        idx for idx, status in current_session.segment_review_status.items()
        if not status.audio_approved
    ]

    if unreviewed and len(unreviewed) > len(current_session.segment_review_status) * 0.3:
        return None, None, None, f"âš ï¸  è¿˜æœ‰ {len(unreviewed)} ä¸ªç‰‡æ®µæœªå®ŒæˆéŸ³é¢‘ç”Ÿæˆ"

    try:
        progress(0.1, "å‡†å¤‡åˆæˆ...")

        output_dir = current_session.video.path.parent / "output"
        output_dir.mkdir(exist_ok=True)

        # åˆå¹¶éŸ³é¢‘ç‰‡æ®µ
        progress(0.2, "åˆå¹¶éŸ³é¢‘ç‰‡æ®µ...")

        from domain.entities import AudioTrack
        from domain.value_objects import AudioSample

        # åˆ›å»ºå®Œæ•´éŸ³è½¨
        sample_rate = list(current_session.audio_segments.values())[0].audio.sample_rate
        total_samples = int(current_session.video.duration * sample_rate)
        buffer = [0.0] * total_samples

        for idx, audio_seg in current_session.audio_segments.items():
            text_seg = audio_seg.text_segment
            start_idx = int(text_seg.time_range.start_seconds * sample_rate)

            for i, sample in enumerate(audio_seg.audio.samples):
                target_idx = start_idx + i
                if target_idx < total_samples:
                    buffer[target_idx] = sample

        full_audio = AudioSample(
            samples=tuple(buffer),
            sample_rate=sample_rate
        )

        audio_track = AudioTrack(full_audio, current_session.translated_subtitle.language)

        # è§†é¢‘åˆæˆ
        progress(0.5, "åˆæˆè§†é¢‘...")

        from application.use_cases.synthesize_video_use_case import synthesize_video_use_case
        from domain.services import merge_bilingual_subtitles

        # åˆ›å»ºåŒè¯­å­—å¹•
        if current_session.english_subtitle:
            zh_en_subtitle = merge_bilingual_subtitles(
                current_session.translated_subtitle,
                current_session.english_subtitle
            )
            subtitles_tuple = (
                current_session.translated_subtitle,
                current_session.english_subtitle,
                zh_en_subtitle
            )
        else:
            subtitles_tuple = (current_session.translated_subtitle,)

        synthesis_result = synthesize_video_use_case(
            video=current_session.video,
            subtitles=subtitles_tuple,
            audio_track=audio_track,
            video_processor=container.video_processor,
            subtitle_writer=container.subtitle_writer,
            output_dir=output_dir,
            formats=("srt", "ass"),
            burn_subtitles=True,
            progress=lambda p, d: progress(0.5 + p * 0.5, d)
        )

        # æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶
        def find_file(patterns: list[str], suffix: str = None) -> Optional[str]:
            for pattern in patterns:
                matches = [
                    p for p in synthesis_result.output_paths
                    if pattern in p.name and (suffix is None or p.suffix == suffix)
                ]
                if matches:
                    return str(matches[0])
            return None

        zh_srt = find_file(['zh.srt'], '.srt')
        zh_en_ass = find_file(['zh_en'], '.ass')
        voiced_video = find_file(['_voiced_subtitled.mp4'])

        status = f"""
âœ… æœ€ç»ˆåˆæˆå®Œæˆ!

ğŸ“¦ è¾“å‡ºæ–‡ä»¶:
   - ä¸­æ–‡å­—å¹•: {zh_srt.split('/')[-1] if zh_srt else 'âŒ'}
   - åŒè¯­å­—å¹•: {zh_en_ass.split('/')[-1] if zh_en_ass else 'âŒ'}
   - é…éŸ³è§†é¢‘: {voiced_video.split('/')[-1] if voiced_video else 'âŒ'}

â±ï¸  å¤„ç†æ—¶é—´: {synthesis_result.processing_time:.1f} ç§’

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   æ€»ç‰‡æ®µæ•°: {len(current_session.audio_segments)}
   ä½¿ç”¨ç¼“å­˜: {len([s for s in current_session.segment_review_status.values() if not s.needs_regeneration])}
   é‡æ–°ç”Ÿæˆ: {len([s for s in current_session.segment_review_status.values() if s.subtitle_modified])}
"""

        return zh_srt, zh_en_ass, voiced_video, status

    except Exception as e:
        import traceback
        error_msg = f"âŒ åˆæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, None, None, error_msg


# ============== UI æ„å»º ============== #
def build_ui_v2():
    """æ„å»ºå¢å¼º UI V2"""

    with gr.Blocks(
            title="è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V2",
            css="""
        .gradio-container {max-width: 1800px !important}
        .segment-preview {border: 1px solid #ddd; padding: 10px; border-radius: 5px;}
        """
    ) as demo:
        gr.Markdown("""
        # ğŸ¬ è§†é¢‘ç¿»è¯‘å·¥å‚ Pro V2 - åˆ†æ®µå®¡æ ¸ç‰ˆ

        ## âœ¨ V2 æ–°ç‰¹æ€§
        - ğŸµ **åˆ†æ®µè¯­éŸ³å…‹éš†**: é€ç‰‡æ®µç”Ÿæˆå¹¶ç¼“å­˜éŸ³é¢‘
        - ğŸ‘‚ **å®æ—¶é¢„è§ˆ**: è¾¹ç”Ÿæˆè¾¹è¯•å¬ï¼Œå³æ—¶åé¦ˆ
        - âœï¸  **ç²¾ç»†ç¼–è¾‘**: ä¿®æ”¹å­—å¹•åä»…é‡æ–°ç”Ÿæˆå¯¹åº”ç‰‡æ®µ
        - ğŸ’¾ **æ™ºèƒ½ç¼“å­˜**: ç‰‡æ®µçº§ç¼“å­˜ï¼Œæ–­ç‚¹ç»­ä¼ 
        - ğŸ”„ **å¢é‡åˆæˆ**: è·³è¿‡æœªä¿®æ”¹çš„ç‰‡æ®µï¼Œæå‡æ•ˆç‡

        ## ğŸ“‹ ä¼˜åŒ–å·¥ä½œæµç¨‹
        1. **ç”Ÿæˆå­—å¹•** â†’ 2A. **å¢é‡è¯­éŸ³å…‹éš†** â†’ 2B. **å®¡æ ¸ä¿®æ”¹** â†’ 2C. **é‡æ–°ç”Ÿæˆ** â†’ 3. **æœ€ç»ˆåˆæˆ**
        """)

        with gr.Tab("ğŸ¬ å•è§†é¢‘å¤„ç† V2"):
            # ========== æ­¥éª¤1 ========== #
            with gr.Accordion("ğŸ” æ­¥éª¤1: ç”Ÿæˆå­—å¹•", open=True):
                with gr.Row():
                    with gr.Column(scale=1):
                        video_input = gr.File(
                            label="ğŸ“¹ ä¸Šä¼ è§†é¢‘",
                            file_types=[".mp4", ".avi", ".mov", ".mkv"]
                        )

                        whisper_model = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large", "large-v3"],
                            value="medium",
                            label="ğŸ™ï¸ Whisper æ¨¡å‹"
                        )

                        translation_model = gr.Dropdown(
                            choices=["Qwen/Qwen2.5-7B"],
                            value="Qwen/Qwen2.5-7B",
                            label="ğŸŒ ç¿»è¯‘æ¨¡å‹"
                        )

                        translation_context = gr.Dropdown(
                            choices=container.translator_context_repo.list_contexts(),
                            value="general",
                            label="ğŸ“š ç¿»è¯‘ä¸Šä¸‹æ–‡"
                        )

                        source_lang = gr.Dropdown(
                            choices=["auto", "en", "zh", "pt", "ja"],
                            value="auto",
                            label="ğŸ—£ï¸ æºè¯­è¨€"
                        )

                        step1_btn = gr.Button("â–¶ï¸ ç”Ÿæˆå­—å¹•", variant="primary")

                    with gr.Column(scale=1):
                        step1_status = gr.Textbox(
                            label="ğŸ“Š ç”ŸæˆçŠ¶æ€",
                            lines=12
                        )

            # ========== æ­¥éª¤2 ========== #
            with gr.Accordion("ğŸ¤ æ­¥éª¤2: å¢é‡è¯­éŸ³å…‹éš†", open=False) as step2_accordion:
                gr.Markdown("""
                ### å·¥ä½œæµç¨‹
                1. **2A. å¢é‡è¯­éŸ³å…‹éš†**: é€ç‰‡æ®µç”ŸæˆéŸ³é¢‘å¹¶ç¼“å­˜
                2. **2B. å®¡æ ¸é¢„è§ˆ**: è¯•å¬éŸ³é¢‘ï¼Œä¿®æ”¹å­—å¹•
                3. **2C. é‡æ–°ç”Ÿæˆ**: åªé‡æ–°ç”Ÿæˆä¿®æ”¹è¿‡çš„ç‰‡æ®µ
                """)

                # 2A: è¯­éŸ³å…‹éš†
                with gr.Group():
                    gr.Markdown("### 2A. å¢é‡è¯­éŸ³å…‹éš†")

                    reference_audio = gr.File(
                        label="ğŸµ å‚è€ƒéŸ³é¢‘ï¼ˆå¯é€‰ï¼‰",
                        file_types=[".wav", ".mp3"]
                    )

                    clone_btn = gr.Button("ğŸ¤ å¼€å§‹å¢é‡è¯­éŸ³å…‹éš†", variant="primary")
                    clone_status = gr.Textbox(label="å…‹éš†çŠ¶æ€", lines=8)

                # 2B: å®¡æ ¸è¡¨æ ¼
                with gr.Group():
                    gr.Markdown("### 2B. å®¡æ ¸å’Œé¢„è§ˆ")

                    review_dataframe = gr.Dataframe(
                        headers=["ç´¢å¼•", "æ—¶é—´", "åŸæ–‡", "ç¿»è¯‘", "éŸ³é¢‘", "é—®é¢˜", "çŠ¶æ€"],
                        datatype=["number", "str", "str", "str", "str", "str", "str"],
                        col_count=(7, "fixed"),
                        row_count=(10, "dynamic"),
                        interactive=True,
                        wrap=True,
                        label="å­—å¹•å®¡æ ¸è¡¨æ ¼"
                    )

                    with gr.Row():
                        save_edits_btn = gr.Button("ğŸ’¾ ä¿å­˜ä¿®æ”¹", variant="secondary")
                        regenerate_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆä¿®æ”¹çš„ç‰‡æ®µ", variant="primary")

                    edit_status = gr.Textbox(label="ç¼–è¾‘çŠ¶æ€", lines=3)

                # ç‰‡æ®µé¢„è§ˆåŒº
                with gr.Group():
                    gr.Markdown("### ğŸ‘‚ ç‰‡æ®µé¢„è§ˆï¼ˆç‚¹å‡»è¡¨æ ¼è¡Œé¢„è§ˆï¼‰")

                    with gr.Row():
                        with gr.Column(scale=1):
                            preview_audio = gr.Audio(
                                label="ğŸ”Š éŸ³é¢‘æ’­æ”¾",
                                type="filepath"
                            )
                            preview_status = gr.Textbox(
                                label="çŠ¶æ€",
                                lines=1
                            )

                        with gr.Column(scale=1):
                            preview_info = gr.Textbox(
                                label="ç‰‡æ®µä¿¡æ¯",
                                lines=3
                            )
                            preview_text = gr.Textbox(
                                label="å­—å¹•æ–‡æœ¬",
                                lines=4
                            )

            # ========== æ­¥éª¤3 ========== #
            with gr.Accordion("ğŸ¬ æ­¥éª¤3: æœ€ç»ˆåˆæˆ", open=False):
                gr.Markdown("""
                ### æç¤º
                - ç¡®ä¿æ‰€æœ‰å…³é”®ç‰‡æ®µéƒ½å·²å®¡æ ¸é€šè¿‡
                - ç³»ç»Ÿä¼šåˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå®Œæ•´è§†é¢‘
                """)

                final_btn = gr.Button("â–¶ï¸ ç”Ÿæˆæœ€ç»ˆè§†é¢‘", variant="primary", size="lg")
                final_status = gr.Textbox(label="åˆæˆçŠ¶æ€", lines=10)

                with gr.Row():
                    zh_srt_output = gr.File(label="ä¸­æ–‡å­—å¹•")
                    zh_en_ass_output = gr.File(label="åŒè¯­å­—å¹•")
                    final_video_output = gr.File(label="æœ€ç»ˆè§†é¢‘")

            # ========== äº‹ä»¶ç»‘å®š ========== #

            # æ­¥éª¤1
            step1_btn.click(
                step1_generate_and_check_v2,
                inputs=[
                    video_input, whisper_model, translation_model,
                    translation_context, source_lang
                ],
                outputs=[review_dataframe, step1_status, step2_accordion]
            ).then(
                lambda: gr.update(open=True),
                outputs=[step2_accordion]
            )

            # æ­¥éª¤2A: è¯­éŸ³å…‹éš†
            clone_btn.click(
                step2_incremental_voice_cloning,
                inputs=[reference_audio],
                outputs=[clone_status, review_dataframe]
            )

            # æ­¥éª¤2B: ç¼–è¾‘ä¿å­˜
            save_edits_btn.click(
                step2_save_edits_and_regenerate,
                inputs=[review_dataframe],
                outputs=[edit_status, review_dataframe]
            )

            # æ­¥éª¤2C: é‡æ–°ç”Ÿæˆ
            regenerate_btn.click(
                step2_regenerate_modified,
                outputs=[edit_status, review_dataframe]
            )

            # è¡¨æ ¼é€‰æ‹©äº‹ä»¶ - é¢„è§ˆç‰‡æ®µ
            review_dataframe.select(
                preview_segment,
                inputs=[],  # Gradio ä¼šè‡ªåŠ¨ä¼ å…¥é€‰ä¸­çš„è¡Œç´¢å¼•
                outputs=[preview_audio, preview_status, preview_info, preview_text]
            )

            # æ­¥éª¤3: æœ€ç»ˆåˆæˆ
            final_btn.click(
                step3_final_synthesis,
                outputs=[zh_srt_output, zh_en_ass_output, final_video_output, final_status]
            )

        # ========== ä½¿ç”¨è¯´æ˜ ========== #
        with gr.Tab("ğŸ“š V2 ä½¿ç”¨æŒ‡å—"):
            gr.Markdown("""
            ## ğŸ¯ V2 æ ¸å¿ƒæ”¹è¿›

            ### é—®é¢˜èƒŒæ™¯
            ä¼ ç»Ÿæµç¨‹çš„ç—›ç‚¹ï¼š
            1. **å…¨é‡åˆæˆè€—æ—¶é•¿**: æ‰€æœ‰ç‰‡æ®µå¿…é¡»å…¨éƒ¨ç”Ÿæˆå®Œæ‰èƒ½å®¡æ ¸
            2. **ä¿®æ”¹æˆæœ¬é«˜**: å‘ç°é—®é¢˜åéœ€è¦é‡æ–°ç”Ÿæˆæ•´ä¸ªè§†é¢‘
            3. **æ— æ³•é¢„è§ˆ**: å¬ä¸åˆ°éŸ³é¢‘æ•ˆæœï¼Œåªèƒ½ç›²å®¡å­—å¹•
            4. **ç¼“å­˜ç²’åº¦ç²—**: åªèƒ½å…¨æœ‰æˆ–å…¨æ— ï¼Œæ— æ³•éƒ¨åˆ†å¤ç”¨

            ### V2 è§£å†³æ–¹æ¡ˆ

            #### 1. åˆ†æ®µç”Ÿæˆæ¶æ„
            ```
            ä¼ ç»Ÿæµç¨‹:
            å­—å¹•ç”Ÿæˆ â†’ [ç­‰å¾…] â†’ å…¨é‡è¯­éŸ³åˆæˆ â†’ [ç­‰å¾…] â†’ å®¡æ ¸ â†’ [å‘ç°é—®é¢˜] â†’ é‡æ–°å…¨é‡åˆæˆ

            V2 æµç¨‹:
            å­—å¹•ç”Ÿæˆ â†’ ç‰‡æ®µ1åˆæˆ â†’ [ç«‹å³é¢„è§ˆ] â†’ ç‰‡æ®µ2åˆæˆ â†’ [ç«‹å³é¢„è§ˆ] â†’ ...
            â†’ å‘ç°é—®é¢˜ â†’ ä¿®æ”¹å­—å¹• â†’ [åªé‡æ–°ç”Ÿæˆè¯¥ç‰‡æ®µ] â†’ å®Œæˆ
            ```

            #### 2. å¢é‡ç¼“å­˜æœºåˆ¶
            ```
            .cache/audio_segments/
            â”œâ”€â”€ video_abc123/
            â”‚   â”œâ”€â”€ seg_0000.wav      # ç‰‡æ®µ0éŸ³é¢‘
            â”‚   â”œâ”€â”€ seg_0000.json     # ç‰‡æ®µ0å…ƒæ•°æ®
            â”‚   â”œâ”€â”€ seg_0001.wav
            â”‚   â”œâ”€â”€ seg_0001.json
            â”‚   â””â”€â”€ ...
            ```

            æ¯ä¸ªç‰‡æ®µç‹¬ç«‹ç¼“å­˜ï¼Œä¿®æ”¹æŸä¸ªå­—å¹•åï¼š
            - âœ… åªåˆ é™¤å¯¹åº”ç‰‡æ®µçš„ç¼“å­˜
            - âœ… åªé‡æ–°ç”Ÿæˆè¯¥ç‰‡æ®µ
            - âœ… å…¶ä»–ç‰‡æ®µç›´æ¥å¤ç”¨

            #### 3. å®æ—¶åé¦ˆå¾ªç¯
            ```python
            def synthesis_progress(ratio, msg, segment_index, audio_segment):
                # æ¯å®Œæˆä¸€ä¸ªç‰‡æ®µå°±å›è°ƒ
                if audio_segment:
                    # ç«‹å³æ›´æ–°UI
                    # ç«‹å³å¯ä»¥é¢„è§ˆ
                    # ç«‹å³ä¿å­˜ç¼“å­˜
            ```

            #### 4. å®¡æ ¸çŠ¶æ€ç®¡ç†
            ```python
            SegmentReviewStatus:
                - subtitle_approved: å­—å¹•æ˜¯å¦å®¡æ ¸é€šè¿‡
                - audio_approved: éŸ³é¢‘æ˜¯å¦å®¡æ ¸é€šè¿‡
                - subtitle_modified: æ˜¯å¦è¢«ä¿®æ”¹
                - needs_regeneration: æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ
            ```

            ### ä½¿ç”¨æœ€ä½³å®è·µ

            #### å¿«é€Ÿè¯•é”™æµç¨‹
            1. ä¸Šä¼ è§†é¢‘ï¼Œç”Ÿæˆå­—å¹•ï¼ˆæ­¥éª¤1ï¼‰
            2. å¼€å§‹å¢é‡è¯­éŸ³å…‹éš†ï¼ˆæ­¥éª¤2Aï¼‰
            3. **è¾¹ç”Ÿæˆè¾¹é¢„è§ˆ**: ç”Ÿæˆå‡ ä¸ªç‰‡æ®µåå°±å¯ä»¥å¼€å§‹è¯•å¬
            4. **å‘ç°é—®é¢˜ç«‹å³ä¿®æ”¹**: ä¸ç”¨ç­‰å…¨éƒ¨å®Œæˆ
            5. **åªé‡æ–°ç”Ÿæˆä¿®æ”¹çš„ç‰‡æ®µ**ï¼ˆæ­¥éª¤2Cï¼‰
            6. æœ€ç»ˆåˆæˆï¼ˆæ­¥éª¤3ï¼‰

            #### å¤§æ‰¹é‡å¤„ç†
            1. å…ˆå®Œæ•´ç”Ÿæˆç¬¬ä¸€ä¸ªè§†é¢‘
            2. æ£€æŸ¥éŸ³è´¨å’Œç¿»è¯‘è´¨é‡
            3. è°ƒæ•´å‚æ•°å’Œä¸Šä¸‹æ–‡
            4. åç»­è§†é¢‘å¯ä»¥å¤ç”¨å‚è€ƒéŸ³é¢‘
            5. åˆ©ç”¨ç¼“å­˜å¿«é€Ÿè¿­ä»£

            ### æ€§èƒ½å¯¹æ¯”

            | åœºæ™¯ | ä¼ ç»Ÿæµç¨‹ | V2 æµç¨‹ | æå‡ |
            |------|---------|---------|------|
            | é¦–æ¬¡ç”Ÿæˆ | 10åˆ†é’Ÿ | 10åˆ†é’Ÿ | 0% |
            | ä¿®æ”¹1ä¸ªç‰‡æ®µ | 10åˆ†é’Ÿ | 10ç§’ | **60x** â­ |
            | ä¿®æ”¹5ä¸ªç‰‡æ®µ | 10åˆ†é’Ÿ | 50ç§’ | **12x** |
            | æ–­ç‚¹ç»­ä¼  | ä»å¤´å¼€å§‹ | ç»§ç»­ç”Ÿæˆ | **âˆ** |
            | é¢„è§ˆæ—¶æœº | å…¨éƒ¨å®Œæˆå | è¾¹ç”Ÿæˆè¾¹é¢„è§ˆ | **å®æ—¶** |

            ### æŠ€æœ¯æ¶æ„

            #### é¢†åŸŸå±‚æ–°å®ä½“
            ```python
            @dataclass(frozen=True)
            class AudioSegment:
                segment_index: int
                audio: AudioSample
                text_segment: TextSegment
                cache_key: str
                file_path: Optional[Path]
            ```

            #### ä»“å‚¨æ¥å£
            ```python
            class AudioSegmentRepository(Protocol):
                def save_segment(idx, audio_seg, video_path) -> Path
                def load_segment(idx, video_path, text_seg) -> AudioSegment
                def exists(idx, video_path) -> bool
                def delete_segment(idx, video_path) -> bool
            ```

            #### åº”ç”¨å±‚ç”¨ä¾‹
            ```python
            incremental_voice_cloning_use_case(
                video, subtitle, tts_provider,
                audio_repo,  # æ–°å¢ï¼šç‰‡æ®µä»“å‚¨
                progress=lambda ratio, msg, idx, audio_seg: ...
                # å›è°ƒæºå¸¦éŸ³é¢‘ç‰‡æ®µï¼Œå®æ—¶æ›´æ–°UI
            )
            ```

            ### ç¼“å­˜ä¸€è‡´æ€§ä¿è¯

            #### ç¼“å­˜é”®ç”Ÿæˆ
            ```python
            cache_key = md5(f"{video_name}_{segment_index}_{text_content}")
            ```

            æ–‡æœ¬æ”¹å˜ â†’ cache_key æ”¹å˜ â†’ è‡ªåŠ¨å¤±æ•ˆ

            #### è‡ªåŠ¨å¤±æ•ˆç­–ç•¥
            ```python
            # å­—å¹•ä¿®æ”¹æ—¶
            if text_modified:
                audio_repo.delete_segment(idx, video_path)
                status = status.mark_subtitle_modified()
            ```

            ### æ•…éšœæ¢å¤

            #### æ–­ç‚¹ç»­ä¼ 
            ```python
            cached_segments = audio_repo.list_segments(video_path)
            # ç»§ç»­ç”Ÿæˆç¼ºå¤±çš„ç‰‡æ®µ
            for idx in missing:
                synthesize_and_cache(idx)
            ```

            #### ä¼šè¯æ¢å¤
            ```python
            # ä¼šè¯ä¸¢å¤±æ—¶ä»ç¼“å­˜åŠ è½½
            if session.video is None:
                audio_segments = audio_repo.list_segments(video_path)
                # æ¢å¤éŸ³é¢‘ç‰‡æ®µ
            ```

            ### æ³¨æ„äº‹é¡¹

            âš ï¸  **é‡è¦æé†’**:
            1. ä¿®æ”¹å­—å¹•å**å¿…é¡»**ç‚¹å‡»"é‡æ–°ç”Ÿæˆ"
            2. é¢„è§ˆæ—¶ç³»ç»Ÿä¼šåŠ è½½ç¼“å­˜ï¼Œç¡®ä¿æ–‡ä»¶è·¯å¾„æœ‰æ•ˆ
            3. æ¸…ç†ç¼“å­˜å‰è¯·ç¡®è®¤å·²ä¿å­˜æœ€ç»ˆè§†é¢‘
            4. å¤§é‡ä¿®æ”¹æ—¶å»ºè®®åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜

            ### æ‰©å±•æ€§

            #### æ”¯æŒå…¶ä»–TTSå¼•æ“
            ```python
            class CustomTTSAdapter(TTSProvider):
                def synthesize(self, text, voice_profile, target_duration):
                    # è‡ªå®šä¹‰å®ç°
                    pass
            ```

            #### æ”¯æŒäº‘å­˜å‚¨
            ```python
            class S3AudioSegmentRepository(AudioSegmentRepository):
                def save_segment(self, idx, audio_seg, video_path):
                    # ä¸Šä¼ åˆ°S3
                    pass
            ```

            ### æ€»ç»“

            V2 ç‰ˆæœ¬é€šè¿‡**åˆ†æ®µç”Ÿæˆ + å¢é‡ç¼“å­˜ + å®æ—¶é¢„è§ˆ**çš„è®¾è®¡ï¼š
            - âœ… å¤§å¹…é™ä½è¿­ä»£æˆæœ¬ï¼ˆä¿®æ”¹1ä¸ªç‰‡æ®µä»10åˆ†é’Ÿé™åˆ°10ç§’ï¼‰
            - âœ… æå‡ç”¨æˆ·ä½“éªŒï¼ˆå®æ—¶åé¦ˆï¼Œæ— éœ€ç­‰å¾…ï¼‰
            - âœ… æé«˜ç³»ç»Ÿé²æ£’æ€§ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼Œæ•…éšœæ¢å¤ï¼‰
            - âœ… ä¿æŒæ¶æ„æ¸…æ™°ï¼ˆéµå¾ªæ´‹è‘±æ¶æ„å’ŒDDDåŸåˆ™ï¼‰

            è¿™æ˜¯**ç”Ÿäº§çº§**çš„å¢é‡å¤„ç†æ–¹æ¡ˆï¼Œé€‚åˆå¤§è§„æ¨¡è§†é¢‘å¤„ç†åœºæ™¯ã€‚
            """)

    return demo


def main():
    """å¯åŠ¨ WebUI V2"""
    demo = build_ui_v2()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        inbrowser=True,
        share=False
    )


if __name__ == "__main__":
    main()