"""
Infrastructure Layer - WebUI
åŸºäº Gradio çš„ Web ç”¨æˆ·ç•Œé¢
"""
import tempfile
from pathlib import Path

import gradio as gr
import numpy as np
import torch
import torchaudio

from application.use_cases.synthesize_video_use_case import synthesize_video_use_case
# å¯¼å…¥åº”ç”¨å±‚ç”¨ä¾‹
from application.use_cases.test_component_use_case import *
from application.use_cases.batch_process import batch_process_use_case
from application.use_cases.clone_voice import clone_voice_use_case
from application.use_cases.generate_subtitles import generate_subtitles_use_case
# å¯¼å…¥é¢†åŸŸå±‚
from domain.entities import (
    Video, LanguageCode,
)
from infrastructure.config.dependency_injection import container


# ============== UI å¤„ç†å‡½æ•° ============== #
def process_single_video_ui(
        video_file,
        whisper_model: str,
        translation_model: str,
        enable_voice: bool,
        reference_audio_file,
        source_language: str,
        progress=gr.Progress()
):
    """
    å•è§†é¢‘å¤„ç† UI å¤„ç†å‡½æ•° - å§‹ç»ˆè¾“å‡ºä¸­è‹±åŒè¯­

    è¾“å‡ºè§„èŒƒ:
    - ä¸­æ–‡å­—å¹• (zh.srt)
    - è‹±æ–‡å­—å¹• (en.srt)
    - ä¸­è‹±åŒè¯­å­—å¹• (zh_en.ass)
    - ä¸­æ–‡é…éŸ³è§†é¢‘ï¼ˆæ— å­—å¹•ï¼‰
    - ä¸­æ–‡é…éŸ³+åŒè¯­å­—å¹•è§†é¢‘
    - åŸå§‹éŸ³é¢‘+ä¸­æ–‡ç¡¬å­—å¹•è§†é¢‘
    """
    if not video_file:
        return None, None, None, None, None, "âŒ è¯·ä¸Šä¼ è§†é¢‘"

    try:
        video_path = Path(video_file.name)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = video_path.parent / "output"
        output_dir.mkdir(exist_ok=True)

        # åˆ›å»ºé¢†åŸŸå¯¹è±¡
        video = Video(
            path=video_path,
            duration=get_video_duration(video_path),
            has_audio=True
        )

        # è§£ææºè¯­è¨€
        src_lang = LanguageCode(source_language) if source_language and source_language != "auto" else None

        # è¿›åº¦å›è°ƒ
        def prog_callback(p: float, desc: str):
            progress(p, desc=desc)

        print(f"\n{'=' * 60}")
        print(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video.path.name}")
        print(f"{'=' * 60}")

        # ============== 1. ç”Ÿæˆå­—å¹•ï¼ˆä¼šåŒæ—¶ç”Ÿæˆä¸­è‹±æ–‡ï¼‰============== #
        subtitle_result = generate_subtitles_use_case(
            video=video,
            asr_provider=container.get_asr(whisper_model),
            translation_provider=container.get_translator(),
            video_processor=container.video_processor,
            cache_repo=container.cache_repo,
            target_language=LanguageCode.CHINESE,
            source_language=src_lang,
            progress=lambda p, d: prog_callback(p * 0.5, d)
        )

        detected_lang = subtitle_result.detected_language
        zh_subtitle = subtitle_result.translated_subtitle  # ä¸­æ–‡å­—å¹•

        print(f"\nğŸ“ å­—å¹•ç”Ÿæˆå®Œæˆ:")
        print(f"   æ£€æµ‹è¯­è¨€: {detected_lang.value}")
        print(f"   ä¸­æ–‡å­—å¹•: {len(zh_subtitle.segments)} ç‰‡æ®µ")

        # ============== 2. ä»ç¼“å­˜è·å–è‹±æ–‡å­—å¹• ============== #
        # å› ä¸º generate_subtitles_use_case å·²ç»ç”Ÿæˆäº†è‹±æ–‡ç‰ˆæœ¬å¹¶ç¼“å­˜
        from domain.services import calculate_cache_key
        cache_key = calculate_cache_key(
            video.path,
            "subtitles",
            {
                "target_language": LanguageCode.CHINESE.value,
                "source_language": src_lang.value if src_lang else "auto"
            }
        )

        cached = container.cache_repo.get(cache_key)
        en_segments = tuple(
            TextSegment(
                text=seg["text"],
                time_range=TimeRange(seg["start"], seg["end"]),
                language=LanguageCode.ENGLISH
            )
            for seg in cached.get("en_segments", [])
        )
        en_subtitle = Subtitle(en_segments, LanguageCode.ENGLISH)
        print(f"   è‹±æ–‡å­—å¹•: {len(en_subtitle.segments)} ç‰‡æ®µ")

        # ============== 3. è¯­éŸ³å…‹éš†ï¼ˆä½¿ç”¨ä¸­æ–‡å­—å¹•ï¼‰============== #
        audio_track = None
        if enable_voice:
            ref_audio_path = Path(reference_audio_file.name) if reference_audio_file else None

            print(f"\nğŸ¤ å¼€å§‹è¯­éŸ³å…‹éš†ï¼ˆä¸­æ–‡é…éŸ³ï¼‰:")
            if ref_audio_path:
                print(f"   å‚è€ƒéŸ³é¢‘: {ref_audio_path.name}")

            voice_result = clone_voice_use_case(
                video=video,
                subtitle=zh_subtitle,  # ä½¿ç”¨ä¸­æ–‡å­—å¹•é…éŸ³
                tts_provider=container.get_tts(),
                video_processor=container.video_processor,
                cache_repo=container.cache_repo,
                reference_audio_path=ref_audio_path,
                progress=lambda p, d: prog_callback(0.5 + p * 0.3, d)
            )
            audio_track = voice_result.audio_track
            print(f"âœ… ä¸­æ–‡é…éŸ³å®Œæˆ")

        # ============== 4. åˆæˆè§†é¢‘ ============== #
        # åˆ›å»ºä¸­è‹±åŒè¯­å­—å¹•ï¼ˆä¸­æ–‡åœ¨ä¸Šï¼Œè‹±æ–‡åœ¨ä¸‹ï¼‰
        from domain.services import merge_bilingual_subtitles
        zh_en_subtitle = merge_bilingual_subtitles(
            zh_subtitle,  # ä¸­æ–‡ï¼ˆä¸Šï¼‰
            en_subtitle  # è‹±æ–‡ï¼ˆä¸‹ï¼‰
        )
        print(f"\nğŸ“ ä¸­è‹±åŒè¯­å­—å¹•åˆ›å»ºå®Œæˆ")

        synthesis_result = synthesize_video_use_case(
            video=video,
            subtitles=(
                zh_subtitle,  # zh.srt / zh.ass
                en_subtitle,  # en.srt / en.ass
                zh_en_subtitle  # zh_en.ass (åŒè¯­)
            ),
            audio_track=audio_track,
            video_processor=container.video_processor,
            subtitle_writer=container.subtitle_writer,
            output_dir=output_dir,
            formats=("srt", "ass"),
            burn_subtitles=True,
            progress=lambda p, d: prog_callback(0.8 + p * 0.2, d)
        )

        # ============== 5. æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶ ============== #
        print(f"\nğŸ” æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶:")
        for path in synthesis_result.output_paths:
            print(f"   - {path.name}")

        # æ™ºèƒ½æŸ¥æ‰¾æ–‡ä»¶
        def find_file(patterns: list[str], suffix: str = None) -> Optional[str]:
            for pattern in patterns:
                matches = [
                    p for p in synthesis_result.output_paths
                    if pattern in p.name and (suffix is None or p.suffix == suffix)
                ]
                if matches:
                    print(f"   âœ… {pattern}: {matches[0].name}")
                    return str(matches[0])
            print(f"   âš ï¸  æœªæ‰¾åˆ°åŒ¹é… {patterns}")
            return None

        # æŸ¥æ‰¾å„ç±»æ–‡ä»¶
        zh_srt = find_file(['zh.srt', 'translated.zh'], '.srt')
        en_srt = find_file(['en.srt', 'translated.en'], '.srt')
        zh_en_ass = find_file(['zh_en', 'bilingual'], '.ass')

        # é…éŸ³è§†é¢‘ï¼ˆçº¯é…éŸ³ï¼Œæ— å­—å¹•ï¼‰
        voiced_video = find_file(['_voiced.mp4']) if audio_track else None

        # é…éŸ³+åŒè¯­å­—å¹•è§†é¢‘
        voiced_subtitled_video = find_file(['_voiced_subtitled.mp4']) if audio_track else None

        # åŸå§‹è§†é¢‘+ä¸­æ–‡ç¡¬å­—å¹•
        subtitled_video = find_file(['_subtitled.mp4'])

        # ============== 6. ç”ŸæˆçŠ¶æ€æŠ¥å‘Š ============== #
        status_lines = [
            f"âœ… å¤„ç†å®Œæˆï¼è€—æ—¶ {synthesis_result.processing_time:.1f} ç§’",
            f"",
            f"ğŸ“Š å­—å¹•ä¿¡æ¯:",
            f"   æ£€æµ‹è¯­è¨€: {detected_lang.value}",
            f"   ä¸­æ–‡å­—å¹•: {len(zh_subtitle.segments)} ç‰‡æ®µ",
            f"   è‹±æ–‡å­—å¹•: {len(en_subtitle.segments)} ç‰‡æ®µ",
            f"",
            f"ğŸ“¦ ç”Ÿæˆæ–‡ä»¶: {len(synthesis_result.output_paths)} ä¸ª"
        ]

        if subtitle_result.cache_hit:
            status_lines.append("ğŸ’¾ å­—å¹•ç¼“å­˜å‘½ä¸­")

        # æ–‡ä»¶æ£€æŸ¥
        file_status = []
        if zh_srt:
            file_status.append(f"âœ… ä¸­æ–‡å­—å¹•")
        else:
            file_status.append(f"âŒ ä¸­æ–‡å­—å¹•ç¼ºå¤±")

        if en_srt:
            file_status.append(f"âœ… è‹±æ–‡å­—å¹•")
        else:
            file_status.append(f"âŒ è‹±æ–‡å­—å¹•ç¼ºå¤±")

        if zh_en_ass:
            file_status.append(f"âœ… ä¸­è‹±åŒè¯­å­—å¹•")
        else:
            file_status.append(f"âŒ ä¸­è‹±åŒè¯­å­—å¹•ç¼ºå¤±")

        if voiced_video:
            file_status.append(f"âœ… ä¸­æ–‡é…éŸ³è§†é¢‘ï¼ˆæ— å­—å¹•ï¼‰")
        elif enable_voice:
            file_status.append(f"âŒ ä¸­æ–‡é…éŸ³è§†é¢‘ç¼ºå¤±")

        if voiced_subtitled_video:
            file_status.append(f"âœ… ä¸­æ–‡é…éŸ³+åŒè¯­å­—å¹•è§†é¢‘")
        elif enable_voice:
            file_status.append(f"âŒ é…éŸ³å­—å¹•è§†é¢‘ç¼ºå¤±")

        if subtitled_video:
            file_status.append(f"âœ… åŸå§‹éŸ³é¢‘+ä¸­æ–‡ç¡¬å­—å¹•")
        else:
            file_status.append(f"âŒ ç¡¬å­—å¹•è§†é¢‘ç¼ºå¤±")

        if file_status:
            status_lines.append("")
            status_lines.append("ğŸ“ æ–‡ä»¶çŠ¶æ€:")
            status_lines.extend([f"   {s}" for s in file_status])

        status = "\n".join(status_lines)
        print(f"\n{status}")
        print(f"{'=' * 60}\n")

        return zh_srt, en_srt, zh_en_ass, voiced_video, subtitled_video, status

    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return None, None, None, None, None, error_msg


def batch_process_videos_ui(
        video_files,
        whisper_model: str,
        translation_model: str,
        enable_voice: bool,
        reference_audio_file: Path,
        progress=gr.Progress()
):
    """æ‰¹é‡å¤„ç† UI å¤„ç†å‡½æ•°"""
    if not video_files:
        return None, "âŒ è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶"

    try:
        # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
        output_dir = Path(tempfile.mkdtemp(prefix="video_batch_"))

        # è½¬æ¢ä¸º Video å¯¹è±¡
        videos = []
        for vf in video_files:
            video_path = Path(vf.name)
            video = Video(
                path=video_path,
                duration=get_video_duration(video_path),
                has_audio=True
            )
            videos.append(video)

        # è¿›åº¦å›è°ƒ
        log_lines = []

        def log_callback(line: str):
            log_lines.append(line)
            if len(log_lines) > 100:
                log_lines.pop(0)

        def prog_callback(p: float, desc: str):
            progress(p, desc=desc)
            log_callback(desc)

        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        results = batch_process_use_case(
            videos=tuple(videos),
            asr_provider=container.get_asr(whisper_model),
            translation_provider=container.get_translator(),
            tts_provider=container.get_tts() if enable_voice else None,
            video_processor=container.video_processor,
            subtitle_writer=container.subtitle_writer,
            cache_repo=container.cache_repo,
            output_dir=output_dir,
            enable_voice_cloning=enable_voice,
            reference_audio_file=reference_audio_file,
            progress=prog_callback
        )

        # æ‰“åŒ…ç»“æœ
        import zipfile
        zip_path = output_dir / "batch_results.zip"
        with zipfile.ZipFile(zip_path, 'w') as zipf:
            for result in results:
                for out_path in result.output_paths:
                    if out_path.exists():
                        zipf.write(out_path, out_path.name)

        log_callback(f"ğŸ“¦ æ‰“åŒ…å®Œæˆï¼å…±å¤„ç† {len(results)} ä¸ªè§†é¢‘")
        final_log = "\n".join(log_lines)

        return str(zip_path), final_log

    except Exception as e:
        import traceback
        error_msg = f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, error_msg


def test_asr_ui(audio_file, whisper_model: str, language: str, progress=gr.Progress()):
    """æµ‹è¯• ASR ç»„ä»¶"""
    if not audio_file:
        return "âŒ è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"

    try:
        progress(0.1, "æ£€æŸ¥è®¾å¤‡ç¯å¢ƒ...")

        # æ·»åŠ è®¾å¤‡æ£€æŸ¥
        import torch
        if torch.cuda.is_available():
            device = "cuda"
            device_info = f"GPU ({torch.cuda.get_device_name()})"
        else:
            device = "cpu"
            device_info = "CPU"

        progress(0.2, f"ä½¿ç”¨è®¾å¤‡: {device_info}")

        # ç¡®ä¿å®¹å™¨ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
        asr_provider = container.get_asr(whisper_model, device=device)

        progress(0.3, "è¯­éŸ³è¯†åˆ«ä¸­...")

        result = test_component_use_case(
            component_type="asr",
            test_input={
                "audio_path": audio_file.name,
                "language": language
            },
            asr_provider=asr_provider,
            progress=None
        )

        # æ ¼å¼åŒ–è¾“å‡º
        output = f"""
âœ… ASR æµ‹è¯•å®Œæˆ ({device_info})

æ£€æµ‹è¯­è¨€: {result['detected_language']}
ç‰‡æ®µæ•°é‡: {result['total_segments']}

å‰ 5 ä¸ªç‰‡æ®µ:
"""
        for i, seg in enumerate(result['segments'][:5], 1):
            output += f"\n{i}. [{seg['start']:.2f}s - {seg['end']:.2f}s] {seg['text']}"

        return output

    except Exception as e:
        return f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}"

def test_translation_ui(text: str, source_lang: str, target_lang: str, progress=gr.Progress()):
    """æµ‹è¯•ç¿»è¯‘ç»„ä»¶"""
    if not text.strip():
        return "âŒ è¯·è¾“å…¥æ–‡æœ¬"

    try:
        progress(0.5, "ç¿»è¯‘ä¸­...")

        result = test_component_use_case(
            component_type="translation",
            test_input={
                "text": text,
                "source_language": source_lang,
                "target_language": target_lang
            },
            translation_provider=container.get_translator(),
            progress=lambda p, d: progress(p, d)
        )

        output = f"""
âœ… ç¿»è¯‘æµ‹è¯•å®Œæˆ

åŸæ–‡ ({result['source_language']}): {result['original']}

è¯‘æ–‡ ({result['target_language']}): {result['translated']}
"""
        return output

    except Exception as e:
        return f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}"


def test_tts_ui(text: str, reference_audio, progress=gr.Progress()):
    """æµ‹è¯• TTS ç»„ä»¶"""
    if not text.strip():
        return None, "âŒ è¯·è¾“å…¥æ–‡æœ¬"

    if not reference_audio:
        return None, "âŒ è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘"

    try:
        progress(0.5, "åˆæˆè¯­éŸ³ä¸­...")

        result = test_component_use_case(
            component_type="tts",
            test_input={
                "text": text,
                "reference_audio": reference_audio.name,
                "target_duration": None
            },
            tts_provider=container.get_tts(),
            progress=lambda p, d: progress(p, d)
        )

        # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        import tempfile
        import torchaudio
        import torch

        temp_audio = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        temp_path = Path(temp_audio.name)
        print(f"temp_path:{temp_path}")
        temp_audio.close()

        # è¿™é‡Œéœ€è¦ä» result è·å–éŸ³é¢‘æ•°æ®å¹¶ä¿å­˜
        # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ ¹æ® F5-TTS è¿”å›æ ¼å¼è°ƒæ•´

        status = f"""
âœ… TTS æµ‹è¯•å®Œæˆ

æ–‡æœ¬: {result['text']}
æ—¶é•¿: {result['duration']:.2f} ç§’
é‡‡æ ·ç‡: {result['sample_rate']} Hz
"""
        return str(temp_path), status

    except Exception as e:
        return None, f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}"


def get_video_duration(video_path: Path) -> float:
    """è·å–è§†é¢‘æ—¶é•¿"""
    import subprocess
    result = subprocess.run([
        'ffprobe', '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        str(video_path)
    ], capture_output=True, text=True)
    return float(result.stdout.strip())


# åœ¨ webui.py ä¸­æ·»åŠ ä»¥ä¸‹å‡½æ•°

def test_tts_simple(
        text: str,
        reference_audio,
        speed: float = 1.0,
        progress=gr.Progress()
):
    """ç®€åŒ–çš„ TTS æµ‹è¯•ç•Œé¢"""
    if not text.strip():
        return None, "âŒ è¯·è¾“å…¥æ–‡æœ¬"

    if not reference_audio:
        return None, "âŒ è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘"

    try:
        progress(0.3, "åˆå§‹åŒ– TTS å¼•æ“...")

        # åˆ›å»º IndexTTSAdapter å®ä¾‹
        from infrastructure.adapters.tts.indextts_adapter import IndexTTSAdapter
        tts_adapter = IndexTTSAdapter()
        tts_adapter.update_config(speed=speed)

        progress(0.5, "åˆæˆè¯­éŸ³ä¸­...")

        # åˆ›å»º VoiceProfile
        from domain.entities import VoiceProfile
        voice_profile = VoiceProfile(
            reference_audio_path=Path(reference_audio.name),
            language=LanguageCode.CHINESE,
            duration=0.1
        )

        # æ‰§è¡Œåˆæˆ
        audio_sample = tts_adapter.synthesize(
            text=text,
            voice_profile=voice_profile
        )

        progress(0.8, "ä¿å­˜éŸ³é¢‘æ–‡ä»¶...")

        # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        import tempfile
        temp_audio = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        temp_path = Path(temp_audio.name)
        temp_audio.close()

        # ä½¿ç”¨ torchaudio ä¿å­˜
        audio_array = np.array(audio_sample.samples, dtype=np.float32)
        torchaudio.save(
            str(temp_path),
            torch.from_numpy(audio_array).unsqueeze(0),
            audio_sample.sample_rate
        )

        status = f"""
âœ… TTS æµ‹è¯•å®Œæˆ

æ–‡æœ¬: {text}
æ—¶é•¿: {audio_sample.duration:.2f} ç§’
é‡‡æ ·ç‡: {audio_sample.sample_rate} Hz
è¯­é€Ÿ: {speed}
"""

        return str(temp_path), status

    except Exception as e:
        import traceback
        error_msg = f"âŒ TTS æµ‹è¯•å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        return None, error_msg
# ============== Gradio UI æ„å»º ============== #

def build_ui():
    """æ„å»º Gradio UI"""

    with gr.Blocks(
            title="è§†é¢‘ç¿»è¯‘å·¥å‚ - æ´‹è‘±æ¶æ„ç‰ˆ",
            css=".gradio-container {max-width: 1400px !important}"
    ) as demo:
        gr.Markdown("""
        # ğŸ¬ è§†é¢‘ç¿»è¯‘å·¥å‚ï¼ˆæ´‹è‘±æ¶æ„ç‰ˆï¼‰

        åŸºäº **é¢†åŸŸé©±åŠ¨è®¾è®¡** + **æ´‹è‘±æ¶æ„** + **å‡½æ•°å¼ç¼–ç¨‹** æ„å»º

        ### âœ¨ æ ¸å¿ƒç‰¹æ€§
        - ğŸ¯ çº¯å‡½æ•°æ ¸å¿ƒï¼Œæ˜“äºæµ‹è¯•å’Œç»´æŠ¤
        - ğŸ”Œ å¯æ’æ‹”ç»„ä»¶ï¼Œæ”¯æŒå¤šç§æ¨¡å‹
        - ğŸ’¾ æ™ºèƒ½ç¼“å­˜ï¼Œæ–­ç‚¹ç»­ä¼ 
        - ğŸ¤ F5-TTS è¯­éŸ³å…‹éš†ï¼ˆIndexTTS 2.0ï¼‰
        """)

        # åœ¨ build_ui() å‡½æ•°ä¸­ï¼Œæ›´æ–°å•è§†é¢‘å¤„ç†çš„è¾“å‡ºéƒ¨åˆ†

        with gr.Tab("ğŸ¬ å•è§†é¢‘å¤„ç†"):
            gr.Markdown("""
            ### å¤„ç†æµç¨‹
            1. ä¸Šä¼ è§†é¢‘
            2. é€‰æ‹©æ¨¡å‹é…ç½®
            3. ï¼ˆå¯é€‰ï¼‰ä¸Šä¼ å‚è€ƒéŸ³é¢‘è¿›è¡Œè¯­éŸ³å…‹éš†
            4. å¼€å§‹å¤„ç†

            ### è¾“å‡ºè¯´æ˜
            - å§‹ç»ˆç”Ÿæˆ**ä¸­æ–‡**å’Œ**è‹±æ–‡**å­—å¹•
            - å¦‚æœå¯ç”¨è¯­éŸ³å…‹éš†ï¼Œç”Ÿæˆ**ä¸­æ–‡é…éŸ³è§†é¢‘**
            - é…éŸ³è§†é¢‘ä¼šè‡ªåŠ¨çƒ§å½•**ä¸­è‹±åŒè¯­å­—å¹•** â­
            """)

            with gr.Row():
                with gr.Column():
                    video_input = gr.File(
                        label="ğŸ“¹ ä¸Šä¼ è§†é¢‘",
                        file_types=[".mp4", ".avi", ".mov", ".mkv"]
                    )

                    with gr.Row():
                        whisper_dropdown = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large", "large-v3"],
                            value="medium",
                            label="ğŸ™ï¸ Whisper æ¨¡å‹"
                        )

                        translation_dropdown = gr.Dropdown(
                            choices=["Qwen/Qwen2.5-1.5B", "Qwen/Qwen2.5-7B", "Qwen/Qwen2.5-14B"],
                            value="Qwen/Qwen2.5-7B",
                            label="ğŸŒ ç¿»è¯‘æ¨¡å‹"
                        )

                    with gr.Row():
                        source_lang_input = gr.Dropdown(
                            choices=["auto", "en", "zh", "pt", "ja", "ko"],
                            value="auto",
                            label="ğŸ—£ï¸ æºè¯­è¨€"
                        )

                    enable_voice_checkbox = gr.Checkbox(
                        label="ğŸ¤ å¯ç”¨è¯­éŸ³å…‹éš†ï¼ˆç”Ÿæˆä¸­æ–‡é…éŸ³ï¼‰",
                        value=False
                    )

                    reference_audio_input = gr.File(
                        label="ğŸµ å‚è€ƒéŸ³é¢‘ï¼ˆå¯é€‰ï¼Œç•™ç©ºåˆ™è‡ªåŠ¨æå–ï¼‰",
                        file_types=[".wav", ".mp3"],
                        visible=False
                    )

                    # åŠ¨æ€æ˜¾ç¤ºå‚è€ƒéŸ³é¢‘ä¸Šä¼ 
                    enable_voice_checkbox.change(
                        lambda x: gr.update(visible=x),
                        inputs=[enable_voice_checkbox],
                        outputs=[reference_audio_input]
                    )

                    process_btn = gr.Button("â–¶ï¸ å¼€å§‹å¤„ç†", variant="primary", size="lg")

                with gr.Column():
                    status_output = gr.Textbox(label="ğŸ“Š å¤„ç†çŠ¶æ€", lines=8)

                    gr.Markdown("### ğŸ“ å­—å¹•æ–‡ä»¶")
                    with gr.Row():
                        zh_srt_output = gr.File(label="ä¸­æ–‡å­—å¹• (SRT)")
                        en_srt_output = gr.File(label="è‹±æ–‡å­—å¹• (SRT)")

                    zh_en_ass_output = gr.File(label="ä¸­è‹±åŒè¯­å­—å¹• (ASS)")

                    gr.Markdown("### ğŸ¬ è§†é¢‘æ–‡ä»¶")

                    with gr.Accordion("ğŸ’¡ æŸ¥çœ‹æ–‡ä»¶è¯´æ˜", open=False):
                        gr.Markdown("""
                        - **é…éŸ³è§†é¢‘ï¼ˆæ— å­—å¹•ï¼‰**: ä»…å«ä¸­æ–‡é…éŸ³ï¼Œæ— çƒ§å½•å­—å¹•
                        - **é…éŸ³+åŒè¯­å­—å¹•è§†é¢‘** â­: ä¸­æ–‡é…éŸ³ + ä¸­è‹±åŒè¯­ç¡¬å­—å¹•ï¼ˆæ¨èè§‚çœ‹ï¼‰
                        - **åŸéŸ³+ä¸­æ–‡å­—å¹•è§†é¢‘**: ä¿ç•™åŸå§‹éŸ³é¢‘ + ä¸­æ–‡ç¡¬å­—å¹•
                        """)

                    voiced_output = gr.File(label="ä¸­æ–‡é…éŸ³è§†é¢‘ï¼ˆæ— å­—å¹•ï¼‰")
                    voiced_subtitled_output = gr.File(label="â­ ä¸­æ–‡é…éŸ³+åŒè¯­å­—å¹•è§†é¢‘ï¼ˆæ¨èï¼‰")
                    subtitled_output = gr.File(label="åŸéŸ³+ä¸­æ–‡ç¡¬å­—å¹•è§†é¢‘")

            process_btn.click(
                process_single_video_ui,
                inputs=[
                    video_input,
                    whisper_dropdown,
                    translation_dropdown,
                    enable_voice_checkbox,
                    reference_audio_input,
                    source_lang_input
                ],
                outputs=[
                    zh_srt_output,  # ä¸­æ–‡å­—å¹•
                    en_srt_output,  # è‹±æ–‡å­—å¹•
                    zh_en_ass_output,  # åŒè¯­å­—å¹•
                    voiced_output,  # é…éŸ³è§†é¢‘ï¼ˆæ— å­—å¹•ï¼‰
                    voiced_subtitled_output,  # é…éŸ³+åŒè¯­å­—å¹•ï¼ˆæ¨èï¼‰
                    #subtitled_output,  # åŸéŸ³+ä¸­æ–‡å­—å¹•
                    status_output  # çŠ¶æ€ä¿¡æ¯
                ]
            )

        with gr.Tab("ğŸï¸ æ‰¹é‡å¤„ç†"):
            gr.Markdown("""
            ### æ‰¹é‡å¤„ç†è¯´æ˜
            - æ”¯æŒåŒæ—¶å¤„ç†å¤šä¸ªè§†é¢‘
            - æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼Œæ•ˆç‡æ›´é«˜
            - æ‰€æœ‰ç»“æœæ‰“åŒ…ä¸º ZIP ä¸‹è½½
            """)

            with gr.Row():
                with gr.Column():
                    batch_videos = gr.File(
                        label="ğŸ“¹ ä¸Šä¼ å¤šä¸ªè§†é¢‘",
                        file_count="multiple",
                        file_types=[".mp4", ".avi", ".mov", ".mkv"]
                    )

                    with gr.Row():
                        batch_whisper = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large"],
                            value="medium",
                            label="ğŸ™ï¸ Whisper æ¨¡å‹"
                        )

                        batch_translation = gr.Dropdown(
                            choices=["Qwen/Qwen2.5-7B"],
                            value="Qwen/Qwen2.5-7B",
                            label="ğŸŒ ç¿»è¯‘æ¨¡å‹"
                        )

                    with gr.Row():
                        batch_enable_voice = gr.Checkbox(
                            label="ğŸ¤ å¯ç”¨è¯­éŸ³å…‹éš†",
                            value=False
                        )
                        reference_audio_input = gr.File(
                            label="ğŸµ å‚è€ƒéŸ³é¢‘ï¼ˆå¯é€‰ï¼Œç•™ç©ºåˆ™è‡ªåŠ¨æå–ï¼‰",
                            file_types=[".wav", ".mp3"],
                            visible=False
                        )

                    # åŠ¨æ€æ˜¾ç¤ºå‚è€ƒéŸ³é¢‘ä¸Šä¼ 
                    batch_enable_voice.change(
                        lambda x: gr.update(visible=x),
                        inputs=[batch_enable_voice],
                        outputs=[reference_audio_input]
                    )

                    batch_btn = gr.Button("â–¶ï¸ å¼€å§‹æ‰¹é‡å¤„ç†", variant="primary", size="lg")

                with gr.Column():
                    batch_log = gr.Textbox(label="ğŸ“Š å¤„ç†æ—¥å¿—", lines=15)
                    batch_output = gr.File(label="ğŸ“¦ ä¸‹è½½ç»“æœï¼ˆZIPï¼‰")

            batch_btn.click(
                batch_process_videos_ui,
                inputs=[
                    batch_videos,
                    batch_whisper,
                    batch_translation,
                    batch_enable_voice,
                    reference_audio_input
                ],
                outputs=[batch_output, batch_log]
            )

        with gr.Tab("ğŸ§ª ç»„ä»¶æµ‹è¯•"):
            gr.Markdown("""
            ### ç»„ä»¶æµ‹è¯•å·¥å…·
            åœ¨é›†æˆåˆ°ä¸»æµç¨‹å‰ï¼Œå…ˆæµ‹è¯•å„ä¸ªç»„ä»¶çš„æ•ˆæœå’Œå‚æ•°
            """)

            with gr.Tab("ğŸ™ï¸ æµ‹è¯• ASR"):
                with gr.Row():
                    with gr.Column():
                        test_asr_audio = gr.File(label="ä¸Šä¼ éŸ³é¢‘", file_types=[".wav", ".mp3"])
                        test_asr_model = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large"],
                            value="medium",
                            label="Whisper æ¨¡å‹"
                        )
                        test_asr_lang = gr.Dropdown(
                            choices=["auto", "en", "zh", "pt"],
                            value="auto",
                            label="è¯­è¨€"
                        )
                        test_asr_btn = gr.Button("æµ‹è¯• ASR")

                    with gr.Column():
                        test_asr_output = gr.Textbox(label="ASR ç»“æœ", lines=15)

                test_asr_btn.click(
                    test_asr_ui,
                    inputs=[test_asr_audio, test_asr_model, test_asr_lang],
                    outputs=[test_asr_output]
                )

            with gr.Tab("ğŸŒ æµ‹è¯•ç¿»è¯‘"):
                with gr.Row():
                    with gr.Column():
                        test_trans_text = gr.Textbox(label="è¾“å…¥æ–‡æœ¬", lines=5)
                        test_trans_src = gr.Dropdown(
                            choices=["en", "zh", "pt"],
                            value="en",
                            label="æºè¯­è¨€"
                        )
                        test_trans_tgt = gr.Dropdown(
                            choices=["en", "zh", "pt"],
                            value="zh",
                            label="ç›®æ ‡è¯­è¨€"
                        )
                        test_trans_btn = gr.Button("æµ‹è¯•ç¿»è¯‘")

                    with gr.Column():
                        test_trans_output = gr.Textbox(label="ç¿»è¯‘ç»“æœ", lines=10)

                test_trans_btn.click(
                    test_translation_ui,
                    inputs=[test_trans_text, test_trans_src, test_trans_tgt],
                    outputs=[test_trans_output]
                )

            with gr.Tab("ğŸ¤ æµ‹è¯• TTS"):
                gr.Markdown("""
                ### IndexTTS2 è¯­éŸ³åˆæˆæµ‹è¯•
                åŸºäºå£°éŸ³å‚è€ƒè¿›è¡Œè¯­éŸ³åˆæˆ
                """)

                with gr.Row():
                    with gr.Column(scale=1):
                        test_tts_text = gr.Textbox(
                            label="è¾“å…¥æ–‡æœ¬",
                            lines=3,
                            placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬å†…å®¹...",
                            value="ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªè¯­éŸ³åˆæˆæµ‹è¯•ã€‚"
                        )
                        test_tts_ref = gr.File(
                            label="å‚è€ƒéŸ³é¢‘",
                            file_types=[".wav", ".mp3"]
                        )

                        with gr.Accordion("âš™ï¸ å‚æ•°è®¾ç½®", open=False):
                            speed_slider = gr.Slider(
                                minimum=0.5, maximum=2.0, value=1.0,
                                label="è¯­é€Ÿ",
                                info="è°ƒæ•´åˆæˆè¯­éŸ³çš„é€Ÿåº¦"
                            )

                        test_tts_btn = gr.Button("ğŸµ æµ‹è¯•è¯­éŸ³åˆæˆ", variant="primary")

                    with gr.Column(scale=1):
                        test_tts_audio = gr.Audio(
                            label="åˆæˆéŸ³é¢‘",
                            type="filepath",
                            interactive=False
                        )
                        test_tts_status = gr.Textbox(
                            label="åˆæˆçŠ¶æ€",
                            lines=6,
                            max_lines=8
                        )

                test_tts_btn.click(
                    test_tts_simple,
                    inputs=[
                        test_tts_text,
                        test_tts_ref,
                        speed_slider
                    ],
                    outputs=[test_tts_audio, test_tts_status]
                )

        with gr.Tab("ğŸ“š æ¶æ„è¯´æ˜"):
            gr.Markdown("""
            ## ğŸ—ï¸ æ´‹è‘±æ¶æ„è®¾è®¡

            ### å±‚æ¬¡ç»“æ„
            ```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Infrastructure Layer (å¤–å±‚)        â”‚
            â”‚   - WebUI (æœ¬ç•Œé¢)                   â”‚
            â”‚   - æ¨¡å‹é€‚é…å™¨ (Whisper, Qwen, F5-TTS)â”‚
            â”‚   - æ–‡ä»¶ç³»ç»Ÿã€ç¼“å­˜                    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ å®ç°æ¥å£
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Application Layer (åº”ç”¨å±‚)         â”‚
            â”‚   - ç”¨ä¾‹ç¼–æ’ï¼ˆçº¯å‡½æ•°ï¼‰                â”‚
            â”‚   - ä¸šåŠ¡æµç¨‹å®šä¹‰                      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ ä½¿ç”¨
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Domain Layer (é¢†åŸŸæ ¸å¿ƒ)            â”‚
            â”‚   - å®ä½“ã€å€¼å¯¹è±¡ï¼ˆä¸å¯å˜ï¼‰            â”‚
            â”‚   - é¢†åŸŸæœåŠ¡ï¼ˆçº¯å‡½æ•°ï¼‰                â”‚
            â”‚   - æ¥å£å®šä¹‰ï¼ˆPortï¼‰                  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            ```

            ### æ ¸å¿ƒåŸåˆ™
            1. **ä¾èµ–å€’ç½®**: å¤–å±‚ä¾èµ–å†…å±‚ï¼Œå†…å±‚å®šä¹‰æ¥å£
            2. **çº¯å‡½æ•°**: æ ¸å¿ƒå±‚å’Œåº”ç”¨å±‚æ— å‰¯ä½œç”¨
            3. **ä¸å¯å˜æ€§**: é¢†åŸŸå¯¹è±¡ä½¿ç”¨ `@dataclass(frozen=True)`
            4. **å¯æµ‹è¯•**: é€šè¿‡æ¥å£æ³¨å…¥ï¼Œæ˜“äº mock æµ‹è¯•

            ### ä¼˜åŠ¿
            - âœ… ä¸šåŠ¡é€»è¾‘ä¸æŠ€æœ¯å®ç°è§£è€¦
            - âœ… è½»æ¾æ›¿æ¢åº•å±‚å®ç°ï¼ˆå¦‚æ›´æ¢ TTS å¼•æ“ï¼‰
            - âœ… é«˜å¯æµ‹è¯•æ€§
            - âœ… æ¸…æ™°çš„ä»£ç ç»„ç»‡
            """)

    return demo


# ============== å¯åŠ¨åº”ç”¨ ============== #

def main():
    """å¯åŠ¨ WebUI"""
    demo = build_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        inbrowser=True,
        share=False
    )


if __name__ == "__main__":
    main()
